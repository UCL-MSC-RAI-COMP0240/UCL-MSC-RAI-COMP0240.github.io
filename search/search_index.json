{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"COMP0240 Aerial Robotics Practical \u00b6 Welcome to the tutorials and practicals for the UCL Computer Science MSC Robotics and AI module COMP0240 Aerial Robotics 2025 Edition. This tutorial website follows along with the 10 week, 2nd term course intended for practical tutorials of around 2-3hrs a week. About \u00b6 The purpose of these practicals is to give you hands on experience in working with drones, drone system and drone autonomy in simulation and reality to the extent possible in the classroom. The practicals are split into - 3 Practicals - 2 Challenges Each of the practicals has a document on the left hand side of this page. The intention is that you follow the instructions to get the basics working, and then you explore the topic through doing the tasks. The practicals should one by one hopefully build up the skills, knowledge and intuition required to complete the challenges. The challenges serve as this courses assessed component. Due to the complexities of dealing with hardware, and to enable student exploration of the different challenges within robotics, these will be group challenges. The challenges revolve around the technical exploration in solving a particular problem and communicating your solution through a presentation. What We Will Cover \u00b6 During these practicals, we aim for students to learn and pick up on the following skills: Familiarising yourself with basic drone hardware and components Experience with using experimental micro-drone systems Programming single and multiple micro-drone systems Simulation to reality development pipelines Experience with industrial drone systems Solving real-world problems with drone systems Practical 1 \u00b6 Weeks 1 and 2 In practical 1, we start from introducing basical knowledge about linux system, ROS2 and the aerostack2 drone system management platform and then practice them through a simualted project. Installing ROS2 and Learning about aerostack2 and what capabilities it gives us Applying ROS2 to drone applications Learning simulation to real development pipeline Practical 2 \u00b6 Weeks 3 and 4 In practical 2, in pairs, you will be putting together and attempting to fly an educational micro-drone platfrom known as the crazyflie. You will need to assemble the micro-drone Work out how to fly the drone Autonomously fly the drone using the crazyflie API CW1: Structural Inspection Path Planning Challenge \u00b6 Weeks 4, 5 and 6 For the first challenge, you will be working individually primarily in simulation to solve a key part of the structural inspection path planning problem. Your task will be in investigating and implementing solutions to finding the most optimal way of visiting a series of locations representing inspection locations, while avoiding the obstacles in the environment. This mirrors real life autonomous inspection scenarios which are the majority of current day use cases with autonomous drones. The challenge will be assessments criteria and method will be released closer to the time Practical 3 (After reading week) \u00b6 CW2: Grand Challenge - Exploring Real-Life Drone Use Cases (Will show you soon) \u00b6 Contact \u00b6 Vijay Pawar: v.pawar AT ucl.ac.uk Mickey Li: mickey.li AT ucl.ac.uk Guohao Wang Narimslu Kemsaram","title":"COMP0240 Aerial Robotics Practical"},{"location":"#comp0240-aerial-robotics-practical","text":"Welcome to the tutorials and practicals for the UCL Computer Science MSC Robotics and AI module COMP0240 Aerial Robotics 2025 Edition. This tutorial website follows along with the 10 week, 2nd term course intended for practical tutorials of around 2-3hrs a week.","title":"COMP0240 Aerial Robotics Practical"},{"location":"#about","text":"The purpose of these practicals is to give you hands on experience in working with drones, drone system and drone autonomy in simulation and reality to the extent possible in the classroom. The practicals are split into - 3 Practicals - 2 Challenges Each of the practicals has a document on the left hand side of this page. The intention is that you follow the instructions to get the basics working, and then you explore the topic through doing the tasks. The practicals should one by one hopefully build up the skills, knowledge and intuition required to complete the challenges. The challenges serve as this courses assessed component. Due to the complexities of dealing with hardware, and to enable student exploration of the different challenges within robotics, these will be group challenges. The challenges revolve around the technical exploration in solving a particular problem and communicating your solution through a presentation.","title":"About"},{"location":"#what-we-will-cover","text":"During these practicals, we aim for students to learn and pick up on the following skills: Familiarising yourself with basic drone hardware and components Experience with using experimental micro-drone systems Programming single and multiple micro-drone systems Simulation to reality development pipelines Experience with industrial drone systems Solving real-world problems with drone systems","title":"What We Will Cover"},{"location":"#practical-1","text":"Weeks 1 and 2 In practical 1, we start from introducing basical knowledge about linux system, ROS2 and the aerostack2 drone system management platform and then practice them through a simualted project. Installing ROS2 and Learning about aerostack2 and what capabilities it gives us Applying ROS2 to drone applications Learning simulation to real development pipeline","title":"Practical 1"},{"location":"#practical-2","text":"Weeks 3 and 4 In practical 2, in pairs, you will be putting together and attempting to fly an educational micro-drone platfrom known as the crazyflie. You will need to assemble the micro-drone Work out how to fly the drone Autonomously fly the drone using the crazyflie API","title":"Practical 2"},{"location":"#cw1-structural-inspection-path-planning-challenge","text":"Weeks 4, 5 and 6 For the first challenge, you will be working individually primarily in simulation to solve a key part of the structural inspection path planning problem. Your task will be in investigating and implementing solutions to finding the most optimal way of visiting a series of locations representing inspection locations, while avoiding the obstacles in the environment. This mirrors real life autonomous inspection scenarios which are the majority of current day use cases with autonomous drones. The challenge will be assessments criteria and method will be released closer to the time","title":"CW1: Structural Inspection Path Planning Challenge"},{"location":"#practical-3-after-reading-week","text":"","title":"Practical 3 (After reading week)"},{"location":"#cw2-grand-challenge-exploring-real-life-drone-use-cases-will-show-you-soon","text":"","title":"CW2: Grand Challenge - Exploring Real-Life Drone Use Cases (Will show you soon)"},{"location":"#contact","text":"Vijay Pawar: v.pawar AT ucl.ac.uk Mickey Li: mickey.li AT ucl.ac.uk Guohao Wang Narimslu Kemsaram","title":"Contact"},{"location":"1a_introduction/","text":"Practical 1: ROS2 with Aerostack2 \u00b6 In this first practical, we will quickly be jumping into the greater code ecosystem for drone work. We start by introducing Aerostack2 which is a ROS2 based framework for the control and coordination of one or more drones. This practical also includes an introduction to ubuntu and ROS2 if you havent already. It will also take you through a ROS2 installation suggestions for various platforms. Linux Installation This article is for those who do not yet have a working Ubuntu 22.04 installation, OR those who are not as familiar with linux, and would like a refresher on how to navigate using linux. ROS2 Installation This articls is for those who have not installed ROS2 Humble on their machine OR those who have not used ROS2 before, or are unfamiliar with ROS2. Aerostack2 This article gives an overview of the aerostack2 framework for aerial robotics development which will be used throughout this course. Gazebo Aruco Mini-Challenge This article introduces an example application within ROS2 and Aerostack2 and gives a mini-challenge to the student to implement a solution within aerostack2 to solve the problem","title":"Introduction"},{"location":"1a_introduction/#practical-1-ros2-with-aerostack2","text":"In this first practical, we will quickly be jumping into the greater code ecosystem for drone work. We start by introducing Aerostack2 which is a ROS2 based framework for the control and coordination of one or more drones. This practical also includes an introduction to ubuntu and ROS2 if you havent already. It will also take you through a ROS2 installation suggestions for various platforms. Linux Installation This article is for those who do not yet have a working Ubuntu 22.04 installation, OR those who are not as familiar with linux, and would like a refresher on how to navigate using linux. ROS2 Installation This articls is for those who have not installed ROS2 Humble on their machine OR those who have not used ROS2 before, or are unfamiliar with ROS2. Aerostack2 This article gives an overview of the aerostack2 framework for aerial robotics development which will be used throughout this course. Gazebo Aruco Mini-Challenge This article introduces an example application within ROS2 and Aerostack2 and gives a mini-challenge to the student to implement a solution within aerostack2 to solve the problem","title":"Practical 1: ROS2 with Aerostack2"},{"location":"1b_intro_linux/","text":"Practical 1 Pre-Setup: Ubuntu Installation and Brief Intro to Linux \u00b6 Practical 1 Pre-Setup: Ubuntu Installation and Brief Intro to Linux Linux Installation A Brief Introduction to Linux What is Linux The Terminal Navigating the file system Working with files Installing Dependencies and Useful Programs Installing Git and VSCode sudo Installing Docker Tasks Linux Installation \u00b6 From this point on, this course will be primarily using the Ubuntu 22.04 Operating System. This is primarily because the majority of robotics and aerial robotics development activities in research and industry are developed for use on Linux-based systems (of which Ubuntu is one). In particular we will be primarily using ROS2 and ROS2 derived tools, which currently works best within Ubuntu 22.04. We\u2019ve explored alternatives like Windows and MacOS (including both Intel and Apple Silicon/M1/2 versions) in previous course iterations. Unfortunately, these platforms introduced significant challenges, including: Compatibility issues with ROS2 and its dependencies. Variability in performance and tool availability. Increased troubleshooting effort for both students and instructors. Ultimately reducing the quality of the course. Therefore this course would like to focus on providing the greatest course quality by focussing on Ubuntu. While containers (e.g., Docker) and virtual machines (VMs) can offer flexibility, they have limitations in this context: Docker : Known for portability, but graphical user interfaces (GUIs) for simulation tools like Gazebo and RViz can be challenging to configure, especially on Windows. Virtual Machines : Simulations in Gazebo are graphics-intensive, and VMs often lack the hardware acceleration needed for smooth operation. Additionally, VMs can have limited access to host machine IO interfaces (e.g., USB), making hardware integration difficult. That said, students are welcome to experiment with Docker or VMs, but these methods will receive minimal support from the course instructors. In order to enable as many of the students to work on native Ubuntu as possible, we suggest the following options: Dual Booting your Windows Install with Ubuntu 22.04 Install Ubuntu 22.04 alongside your existing Windows installation. This allows you to choose between operating systems at boot. Native ubuntu performance Fully supported by course instructors Will require at least 100Gb of unused disk space Windows only Borrowing and booting from an Ubuntu 22.04 installed USB-C NVME Drive Borrow or prepare a portable USB-C NVMe drive pre-installed with Ubuntu 22.04. This method allows you to run Ubuntu without altering your main system. Go to bios setting to reorder your booting sequence. Different brands of computers have different ways of getting into bios. For example, for dell computer, you could access the BIOS/UEFI settings by restarting your computer and pressing the F2 key repeatedly as soon as the Dell logo appears. Lift the borrowed drive up to the first, then save changes. Now your computer will boot from this automatically as long as it is plugged into your computer. Minimal changes to your primary system. Requires a USB-C port with fast read/write capabilities. Limited number of pre-setup Ubuntu 22.04 bootsticks Windows Primarily, but known to potentially work with Mac OSX too (pending testing). Repurpose an old laptop Spin up a virtual machine on a cloud provider Use a cloud provider like AWS, Google Cloud, or Azure to spin up a virtual machine running Ubuntu 22.04 Docker Docker may be possible for some of the situations Known issues with GUIs and Windows (which can be solved) Not as supported Virtual Machine For options 1 and 2, you will need to find a USB stick that is >4Gb and using a tool such as Rufus flash the Ubuntu 22.04 ISO file onto it. Once you have a flashed USB drive, you can insert that into the spare machine. On startup make sure to mash some combination of F2, F8 or F12 to go to the BIOS boot screen and select boot from USB. This will start up tp the Ubuntu installer on the USB drive where you can select what to do. Whether that is to wipe the machine, or in the advanced menu create a new partition for Ubuntu so you can dual boot (For Windows you will also need to shrink your primary partition). For more details see a guide such as this one . We have also created a number of pre-installed NVME drives which you may be able to borrow pending numbers A Brief Introduction to Linux \u00b6 Adapted from this digital ocean tutorial What is Linux \u00b6 Linux is a family of free and open-source operating systems based on the Linux kernel (core operating system). Operating systems based on Linux are known as Linux distributions or distros. Examples include Debian, Ubuntu, Fedora, CentOS, Gentoo, Arch Linux, and many others. The Linux kernel has been under active development since 1991, and has proven to be extremely versatile and adaptable. You can find computers that run Linux in a wide variety of contexts all over the world, from web servers to cell phones. Today, 90% of all cloud infrastructure and 74% of the world\u2019s smartphones are powered by Linux. However, newcomers to Linux may find it somewhat difficult to approach, as Linux filesystems have a different structure than those found on Windows or MacOS. Additionally, Linux-based operating systems depend heavily on working with the command line interface, while most personal computers rely on graphical interfaces. The Terminal \u00b6 The terms \u201cterminal,\u201d \u201cshell,\u201d and \u201ccommand line interface\u201d are often used interchangeably, but there are subtle differences between them: A terminal is an input and output environment that presents a text-only window running a shell. A shell is a program that exposes the computer\u2019s operating system to a user or program. In Linux systems, the shell presented in a terminal is a command line interpreter. The default shell in Ubuntu Linux is known as bash . A command line interface is a user interface (managed by a command line interpreter program) which processes commands to a computer program and outputs the results. When someone refers to one of these three terms in the context of Linux, they generally mean a terminal environment where you can run commands and see the results printed out to the terminal, such as this: There are two ways to open a terminal: Pressing the Win or Cmd key to open the program menu and typing terminal , then pressing Enter Pressing Ctrl + Alt + T This default terminal is known as the 'gnome-terminal'. Other terminals exist such as 'terminator' Becoming a Linux user requires you to be comfortable with using a terminal. Any administrative task, including file manipulation, package installation, and user management, can be accomplished through the terminal. The terminal is interactive: you specify commands to run (after the $ sign) and the terminal outputs the results of those commands. To execute any command, you type it into the prompt and press Enter . When using Linux and interacting with ROS2 you'll most often be doing so through a terminal shell. Although personal computers that run Linux often come with the kind of graphical desktop environment familiar to most computer users, it is often more efficient or practical to perform certain tasks through commands entered into the terminal. As of writing, a GUI (Graphical User Interface) has not been developed for Starling, and so almost all tasks have to be achieved through the terminal shell. A basic command to try out is echo , which will print things to the terminal. For example echo hello-world will print hello-world into the terminal. You can also use it to observe the value of Environment Variables which record and keep useful variables to the operation of the Operating System. For example, when you run a command in bash , bash will look for the command executable in the locations provided by the environment variable PATH . You can print the contents of this env-var using echo $PATH . The $ before the name of the variable tells bash that the following word represents an environment variable, and that it should be looked up. Note that for the default shell option of bash , there is often a way of specifying an environment (i.e. a set of variables, defaults etc) that your particular terminal shell interface is running. For bash, these defaults are defined inside of the .bashrc file found in your home directory. If we want defaults to be set, we add them in here - this becomes useful in many applications including ROS2. Navigating the file system \u00b6 Like Windows and Mac, the Linux filesystems are based on a directory tree. This means that you can create directories (which are functionally identical to folders found in other operating systems) inside other directories, and files can exist in any directory. The forward slash ( / ) is used to indicate the root directory in the filesystem hierarchy. When a user logs in to the shell, they are brought to their own user directory, stored within /home/<username> . This is referred to as the user\u2019s home directory. Often you may see the tilde ( ~ ) character when specifying a file location (e.g. ~/Documents/hello.txt = /home/<username>/Documents/hello.txt ). This is shorthand for the user's home directory and gets substituted in when used. To see what directory you are currently active in you can run the pwd command, which stands for \u201cprint working directory\u201d myuser@my-machine:~$ pwd /home/myuser To see a list of files and directories that exist in your current working directory, run the ls command: myuser@my-machine:~$ ls Desktop Documents Downloads Pictures Public Wallpapers You can get more details if you run ls -al command: myuser@my-machine:~$ ls -al drwxr-xr-x 2 myuser myuser 4096 Apr 30 2021 Desktop drwxrwxr-x 8 myuser myuser 4096 Oct 29 09:27 Documents drwxrwxr-x 8 myuser myuser 4096 Dec 10 14:41 Downloads drwxrwxr-x 8 myuser myuser 4096 May 23 10:43 Pictures drwxrwxr-x 8 myuser myuser 4096 Jan 19 2017 Public drwxrwxr-x 8 myuser myuser 4096 Oct 15 09:43 Wallpapers You can create one or more new directories within your current working directory with the mkdir command, which stands for \u201cmake directory\u201d. For example, to create two new directories named testdir1 and testdir2, you might run the first command. You can create nested directories by using the -p option: myuser@my-machine:~$ mkdir testdir1 testdir2 myuser@my-machine:~$ mkdir -p testdir3/testsubdir To navigate into one of these new directories, run the cd command (which stands for \u201cchange directory\u201d) and specify the directory\u2019s name: myuser@my-machine:~$ cd testdir1 myuser@my-machine:~/testdir1$ Note that you can navigate from anywhere to anywhere. cd only requires a valid filepath. Note also that . represents the current folder and .. represents the parent folder. Note also how is shows the current working directory in the shell as well. cd # This will bring you back to home directory cd testdir3/testsubdir # Brings you into testsubdir cd ../ # Brings you back out one level into testdir3 cd ../testdir1 # Brings you back out one level and back into testdir1 cd /home/<username>/testdir2 # Absolute reference to testdir2 cd ~/testdir2 # Absolute reference using tilde to testdir2 Working with files \u00b6 You cannot use cd to interact with files; cd stands for \u201cchange directory\u201d, and only allows you to navigate directories. You can, however, create, edit, and view the contents of files. One way to create a file is with the touch command. This creates an empty file in your current working directory. To create a new file called file.txt: touch file.txt If you decide to rename file.txt later on, you can do so with the mv command. mv stands for \u201cmove\u201d and it can move a file or directory from one place to another. By specifying the original file, file.txt, you can \u201cmove\u201d it to a new location in the current working directory, thereby renaming it. mv file.txt newfile.txt It is also possible to copy a file to a new location with the cp command. If we want to copy newfile.txt, you can make a copy of newfile.txt named newfile_copy.txt like this: cp newfile.txt newfile_copy.txt However, files are not of much use if they don\u2019t contain anything. To edit files, a file editor is necessary. There are many options for file editors, all created by professionals for daily use. Such editors include vim, emacs, nano, and pico. nano is a suitable option for beginners: it is relatively user-friendly and doesn\u2019t overload you with cryptic options or commands. nano file.txt This will open a space where you can start typing to edit the file. In nano specifically you can save your written text by pressing Ctrl + X , Y , and then Enter . This returns you to the shell with a newly saved file.txt . Now that file.txt has some text within it, you can view it using cat or less . The cat command prints the contents of a specified file to your system\u2019s output. Try running cat and pass the file.txt file you just edited as an argument: cat file.txt Using cat to view file contents can be unwieldy and difficult to read if the file is particularly long. As an alternative, you can use the less command which will allow you to paginate the output. Use less to view the contents of the file.txt file, like this: less file.txt This will also print the contents of file.txt, but one terminal page at a time beginning at the start of the file. You can use the spacebar to advance a page, or the arrow keys to go up and down one line at a time. Press Q to quit out of less . Finally, to delete the file.txt file, pass the name of the file as an argument to rm : rm file.txt rm -d directory rmidr directory rm -r directory # If the directory you are deleting is not empty NOTE : If your question has to do with a specific Linux command, the manual pages offer detailed and insightful documentation for nearly every command. To see the man page for any command, pass the command\u2019s name as an argument to the man command - man command . For instance, man rm displays the purpose of rm , how to use it, what options are available, examples of use, and more useful information. NOTE : If a command fails or is hanging or you just want to stop it, most of the time you can stop the running process by pressing Ctrl + C . This will send a Keyboard Interrupt message to the program and hopefully stop it. Installing Dependencies and Useful Programs \u00b6 Like windows and mac, individual programs can be manually downloaded (usually as a tar.gz file instead of exe ) and manually installed into your operating system (using dpkg ). However, the linux project offers a much more straight forward method through the apt (Advanced Packaging Tool) utility. apt is a free-software user interface that works with core libraries to handle the installation and removal of software on Debian operating systems like Ubuntu. (For other distributions you may come across equivalents like yum ). This is the primary method for installing software onto your system. To use apt , and more specifically apt-get which 'gets' programs for you, you must first run the update command to get the current list of all available software. Note that because sudo is used, you will most likely need to input your password. sudo will be explained below. sudo apt-get update Note that it will hang (stop responding) or fail if you are not connected to the internet. Installing Git and VSCode \u00b6 You can then install your programs using apt-get install . For Starling, you will need to use the git version control software to both download Starling and eventually build your own controllers. To install git , run the following: sudo apt-get install git We also recommend the use of Visual Studio Code as your development environment or text editor, but you are free to use whatever you want (atom, notepad++ etc etc). We heavily make use of it during development and recommend a number of extensions. VScode can be installed using the snap utility. snap is a slightly more modern successor to apt for more general programs. snap comes ready installed on your linux distrubtion. sudo snap install code --classic sudo \u00b6 Now in these commands, we have prefixed all of them with sudo . sudo these days usually stands for superuser do and allows a command to be run with the privileges of the superuser (aka the root user), if the user has been given permissions to do so. Any command which installs or modifies directories outside of the users home directory will often need superuser privileges to avoid non-superusers from changing things they shouldn't. As the above commands all seek to install programs to the system, they need superuser permissions to do so. Running without sudo will give you a permission error. Running a command with sudo will ask you for your own accounts password. Installing Docker \u00b6 For Linux systems, see the following install page . There are multiple ways of installation docker, but we recommend installing using the repository method: Update the apt repository and install requirements sudo apt-get update sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Add Docker's official GPG key: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg Add Docker's repository: echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Install Docker (and docker-compose!): sudo apt-get update sudo apt-get install docker-ce docker-ce-cli docker-compose containerd.io Test Docker installation: sudo docker run hello-world This will install Docker, accessible using sudo root privileges only. To use docker without sudo, run the following (there are security issues with this, but it doesn't matter for running Starling locally) Run the following sudo groupadd docker sudo usermod -aG docker $USER Log out and log in again to enforce changes Verify that it was successful (we will come back to this command): docker run hello-world That is Docker on Linux installed. See the original page for any further details. Tasks \u00b6 Install and Run Ubuntu 22.04 using a method described above Familiarise yourself with Linux and Ubuntu Install the programs and applications you might find useful Further Tasks: Read the following to get the best introduction! LinuxJourney","title":"Ubuntu Installation and Linux Re-Intro"},{"location":"1b_intro_linux/#practical-1-pre-setup-ubuntu-installation-and-brief-intro-to-linux","text":"Practical 1 Pre-Setup: Ubuntu Installation and Brief Intro to Linux Linux Installation A Brief Introduction to Linux What is Linux The Terminal Navigating the file system Working with files Installing Dependencies and Useful Programs Installing Git and VSCode sudo Installing Docker Tasks","title":"Practical 1 Pre-Setup: Ubuntu Installation and Brief Intro to Linux"},{"location":"1b_intro_linux/#linux-installation","text":"From this point on, this course will be primarily using the Ubuntu 22.04 Operating System. This is primarily because the majority of robotics and aerial robotics development activities in research and industry are developed for use on Linux-based systems (of which Ubuntu is one). In particular we will be primarily using ROS2 and ROS2 derived tools, which currently works best within Ubuntu 22.04. We\u2019ve explored alternatives like Windows and MacOS (including both Intel and Apple Silicon/M1/2 versions) in previous course iterations. Unfortunately, these platforms introduced significant challenges, including: Compatibility issues with ROS2 and its dependencies. Variability in performance and tool availability. Increased troubleshooting effort for both students and instructors. Ultimately reducing the quality of the course. Therefore this course would like to focus on providing the greatest course quality by focussing on Ubuntu. While containers (e.g., Docker) and virtual machines (VMs) can offer flexibility, they have limitations in this context: Docker : Known for portability, but graphical user interfaces (GUIs) for simulation tools like Gazebo and RViz can be challenging to configure, especially on Windows. Virtual Machines : Simulations in Gazebo are graphics-intensive, and VMs often lack the hardware acceleration needed for smooth operation. Additionally, VMs can have limited access to host machine IO interfaces (e.g., USB), making hardware integration difficult. That said, students are welcome to experiment with Docker or VMs, but these methods will receive minimal support from the course instructors. In order to enable as many of the students to work on native Ubuntu as possible, we suggest the following options: Dual Booting your Windows Install with Ubuntu 22.04 Install Ubuntu 22.04 alongside your existing Windows installation. This allows you to choose between operating systems at boot. Native ubuntu performance Fully supported by course instructors Will require at least 100Gb of unused disk space Windows only Borrowing and booting from an Ubuntu 22.04 installed USB-C NVME Drive Borrow or prepare a portable USB-C NVMe drive pre-installed with Ubuntu 22.04. This method allows you to run Ubuntu without altering your main system. Go to bios setting to reorder your booting sequence. Different brands of computers have different ways of getting into bios. For example, for dell computer, you could access the BIOS/UEFI settings by restarting your computer and pressing the F2 key repeatedly as soon as the Dell logo appears. Lift the borrowed drive up to the first, then save changes. Now your computer will boot from this automatically as long as it is plugged into your computer. Minimal changes to your primary system. Requires a USB-C port with fast read/write capabilities. Limited number of pre-setup Ubuntu 22.04 bootsticks Windows Primarily, but known to potentially work with Mac OSX too (pending testing). Repurpose an old laptop Spin up a virtual machine on a cloud provider Use a cloud provider like AWS, Google Cloud, or Azure to spin up a virtual machine running Ubuntu 22.04 Docker Docker may be possible for some of the situations Known issues with GUIs and Windows (which can be solved) Not as supported Virtual Machine For options 1 and 2, you will need to find a USB stick that is >4Gb and using a tool such as Rufus flash the Ubuntu 22.04 ISO file onto it. Once you have a flashed USB drive, you can insert that into the spare machine. On startup make sure to mash some combination of F2, F8 or F12 to go to the BIOS boot screen and select boot from USB. This will start up tp the Ubuntu installer on the USB drive where you can select what to do. Whether that is to wipe the machine, or in the advanced menu create a new partition for Ubuntu so you can dual boot (For Windows you will also need to shrink your primary partition). For more details see a guide such as this one . We have also created a number of pre-installed NVME drives which you may be able to borrow pending numbers","title":"Linux Installation"},{"location":"1b_intro_linux/#a-brief-introduction-to-linux","text":"Adapted from this digital ocean tutorial","title":"A Brief Introduction to Linux"},{"location":"1b_intro_linux/#what-is-linux","text":"Linux is a family of free and open-source operating systems based on the Linux kernel (core operating system). Operating systems based on Linux are known as Linux distributions or distros. Examples include Debian, Ubuntu, Fedora, CentOS, Gentoo, Arch Linux, and many others. The Linux kernel has been under active development since 1991, and has proven to be extremely versatile and adaptable. You can find computers that run Linux in a wide variety of contexts all over the world, from web servers to cell phones. Today, 90% of all cloud infrastructure and 74% of the world\u2019s smartphones are powered by Linux. However, newcomers to Linux may find it somewhat difficult to approach, as Linux filesystems have a different structure than those found on Windows or MacOS. Additionally, Linux-based operating systems depend heavily on working with the command line interface, while most personal computers rely on graphical interfaces.","title":"What is Linux"},{"location":"1b_intro_linux/#the-terminal","text":"The terms \u201cterminal,\u201d \u201cshell,\u201d and \u201ccommand line interface\u201d are often used interchangeably, but there are subtle differences between them: A terminal is an input and output environment that presents a text-only window running a shell. A shell is a program that exposes the computer\u2019s operating system to a user or program. In Linux systems, the shell presented in a terminal is a command line interpreter. The default shell in Ubuntu Linux is known as bash . A command line interface is a user interface (managed by a command line interpreter program) which processes commands to a computer program and outputs the results. When someone refers to one of these three terms in the context of Linux, they generally mean a terminal environment where you can run commands and see the results printed out to the terminal, such as this: There are two ways to open a terminal: Pressing the Win or Cmd key to open the program menu and typing terminal , then pressing Enter Pressing Ctrl + Alt + T This default terminal is known as the 'gnome-terminal'. Other terminals exist such as 'terminator' Becoming a Linux user requires you to be comfortable with using a terminal. Any administrative task, including file manipulation, package installation, and user management, can be accomplished through the terminal. The terminal is interactive: you specify commands to run (after the $ sign) and the terminal outputs the results of those commands. To execute any command, you type it into the prompt and press Enter . When using Linux and interacting with ROS2 you'll most often be doing so through a terminal shell. Although personal computers that run Linux often come with the kind of graphical desktop environment familiar to most computer users, it is often more efficient or practical to perform certain tasks through commands entered into the terminal. As of writing, a GUI (Graphical User Interface) has not been developed for Starling, and so almost all tasks have to be achieved through the terminal shell. A basic command to try out is echo , which will print things to the terminal. For example echo hello-world will print hello-world into the terminal. You can also use it to observe the value of Environment Variables which record and keep useful variables to the operation of the Operating System. For example, when you run a command in bash , bash will look for the command executable in the locations provided by the environment variable PATH . You can print the contents of this env-var using echo $PATH . The $ before the name of the variable tells bash that the following word represents an environment variable, and that it should be looked up. Note that for the default shell option of bash , there is often a way of specifying an environment (i.e. a set of variables, defaults etc) that your particular terminal shell interface is running. For bash, these defaults are defined inside of the .bashrc file found in your home directory. If we want defaults to be set, we add them in here - this becomes useful in many applications including ROS2.","title":"The Terminal"},{"location":"1b_intro_linux/#navigating-the-file-system","text":"Like Windows and Mac, the Linux filesystems are based on a directory tree. This means that you can create directories (which are functionally identical to folders found in other operating systems) inside other directories, and files can exist in any directory. The forward slash ( / ) is used to indicate the root directory in the filesystem hierarchy. When a user logs in to the shell, they are brought to their own user directory, stored within /home/<username> . This is referred to as the user\u2019s home directory. Often you may see the tilde ( ~ ) character when specifying a file location (e.g. ~/Documents/hello.txt = /home/<username>/Documents/hello.txt ). This is shorthand for the user's home directory and gets substituted in when used. To see what directory you are currently active in you can run the pwd command, which stands for \u201cprint working directory\u201d myuser@my-machine:~$ pwd /home/myuser To see a list of files and directories that exist in your current working directory, run the ls command: myuser@my-machine:~$ ls Desktop Documents Downloads Pictures Public Wallpapers You can get more details if you run ls -al command: myuser@my-machine:~$ ls -al drwxr-xr-x 2 myuser myuser 4096 Apr 30 2021 Desktop drwxrwxr-x 8 myuser myuser 4096 Oct 29 09:27 Documents drwxrwxr-x 8 myuser myuser 4096 Dec 10 14:41 Downloads drwxrwxr-x 8 myuser myuser 4096 May 23 10:43 Pictures drwxrwxr-x 8 myuser myuser 4096 Jan 19 2017 Public drwxrwxr-x 8 myuser myuser 4096 Oct 15 09:43 Wallpapers You can create one or more new directories within your current working directory with the mkdir command, which stands for \u201cmake directory\u201d. For example, to create two new directories named testdir1 and testdir2, you might run the first command. You can create nested directories by using the -p option: myuser@my-machine:~$ mkdir testdir1 testdir2 myuser@my-machine:~$ mkdir -p testdir3/testsubdir To navigate into one of these new directories, run the cd command (which stands for \u201cchange directory\u201d) and specify the directory\u2019s name: myuser@my-machine:~$ cd testdir1 myuser@my-machine:~/testdir1$ Note that you can navigate from anywhere to anywhere. cd only requires a valid filepath. Note also that . represents the current folder and .. represents the parent folder. Note also how is shows the current working directory in the shell as well. cd # This will bring you back to home directory cd testdir3/testsubdir # Brings you into testsubdir cd ../ # Brings you back out one level into testdir3 cd ../testdir1 # Brings you back out one level and back into testdir1 cd /home/<username>/testdir2 # Absolute reference to testdir2 cd ~/testdir2 # Absolute reference using tilde to testdir2","title":"Navigating the file system"},{"location":"1b_intro_linux/#working-with-files","text":"You cannot use cd to interact with files; cd stands for \u201cchange directory\u201d, and only allows you to navigate directories. You can, however, create, edit, and view the contents of files. One way to create a file is with the touch command. This creates an empty file in your current working directory. To create a new file called file.txt: touch file.txt If you decide to rename file.txt later on, you can do so with the mv command. mv stands for \u201cmove\u201d and it can move a file or directory from one place to another. By specifying the original file, file.txt, you can \u201cmove\u201d it to a new location in the current working directory, thereby renaming it. mv file.txt newfile.txt It is also possible to copy a file to a new location with the cp command. If we want to copy newfile.txt, you can make a copy of newfile.txt named newfile_copy.txt like this: cp newfile.txt newfile_copy.txt However, files are not of much use if they don\u2019t contain anything. To edit files, a file editor is necessary. There are many options for file editors, all created by professionals for daily use. Such editors include vim, emacs, nano, and pico. nano is a suitable option for beginners: it is relatively user-friendly and doesn\u2019t overload you with cryptic options or commands. nano file.txt This will open a space where you can start typing to edit the file. In nano specifically you can save your written text by pressing Ctrl + X , Y , and then Enter . This returns you to the shell with a newly saved file.txt . Now that file.txt has some text within it, you can view it using cat or less . The cat command prints the contents of a specified file to your system\u2019s output. Try running cat and pass the file.txt file you just edited as an argument: cat file.txt Using cat to view file contents can be unwieldy and difficult to read if the file is particularly long. As an alternative, you can use the less command which will allow you to paginate the output. Use less to view the contents of the file.txt file, like this: less file.txt This will also print the contents of file.txt, but one terminal page at a time beginning at the start of the file. You can use the spacebar to advance a page, or the arrow keys to go up and down one line at a time. Press Q to quit out of less . Finally, to delete the file.txt file, pass the name of the file as an argument to rm : rm file.txt rm -d directory rmidr directory rm -r directory # If the directory you are deleting is not empty NOTE : If your question has to do with a specific Linux command, the manual pages offer detailed and insightful documentation for nearly every command. To see the man page for any command, pass the command\u2019s name as an argument to the man command - man command . For instance, man rm displays the purpose of rm , how to use it, what options are available, examples of use, and more useful information. NOTE : If a command fails or is hanging or you just want to stop it, most of the time you can stop the running process by pressing Ctrl + C . This will send a Keyboard Interrupt message to the program and hopefully stop it.","title":"Working with files"},{"location":"1b_intro_linux/#installing-dependencies-and-useful-programs","text":"Like windows and mac, individual programs can be manually downloaded (usually as a tar.gz file instead of exe ) and manually installed into your operating system (using dpkg ). However, the linux project offers a much more straight forward method through the apt (Advanced Packaging Tool) utility. apt is a free-software user interface that works with core libraries to handle the installation and removal of software on Debian operating systems like Ubuntu. (For other distributions you may come across equivalents like yum ). This is the primary method for installing software onto your system. To use apt , and more specifically apt-get which 'gets' programs for you, you must first run the update command to get the current list of all available software. Note that because sudo is used, you will most likely need to input your password. sudo will be explained below. sudo apt-get update Note that it will hang (stop responding) or fail if you are not connected to the internet.","title":"Installing Dependencies and Useful Programs"},{"location":"1b_intro_linux/#installing-git-and-vscode","text":"You can then install your programs using apt-get install . For Starling, you will need to use the git version control software to both download Starling and eventually build your own controllers. To install git , run the following: sudo apt-get install git We also recommend the use of Visual Studio Code as your development environment or text editor, but you are free to use whatever you want (atom, notepad++ etc etc). We heavily make use of it during development and recommend a number of extensions. VScode can be installed using the snap utility. snap is a slightly more modern successor to apt for more general programs. snap comes ready installed on your linux distrubtion. sudo snap install code --classic","title":"Installing Git and VSCode"},{"location":"1b_intro_linux/#sudo","text":"Now in these commands, we have prefixed all of them with sudo . sudo these days usually stands for superuser do and allows a command to be run with the privileges of the superuser (aka the root user), if the user has been given permissions to do so. Any command which installs or modifies directories outside of the users home directory will often need superuser privileges to avoid non-superusers from changing things they shouldn't. As the above commands all seek to install programs to the system, they need superuser permissions to do so. Running without sudo will give you a permission error. Running a command with sudo will ask you for your own accounts password.","title":"sudo"},{"location":"1b_intro_linux/#installing-docker","text":"For Linux systems, see the following install page . There are multiple ways of installation docker, but we recommend installing using the repository method: Update the apt repository and install requirements sudo apt-get update sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Add Docker's official GPG key: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg Add Docker's repository: echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Install Docker (and docker-compose!): sudo apt-get update sudo apt-get install docker-ce docker-ce-cli docker-compose containerd.io Test Docker installation: sudo docker run hello-world This will install Docker, accessible using sudo root privileges only. To use docker without sudo, run the following (there are security issues with this, but it doesn't matter for running Starling locally) Run the following sudo groupadd docker sudo usermod -aG docker $USER Log out and log in again to enforce changes Verify that it was successful (we will come back to this command): docker run hello-world That is Docker on Linux installed. See the original page for any further details.","title":"Installing Docker"},{"location":"1b_intro_linux/#tasks","text":"Install and Run Ubuntu 22.04 using a method described above Familiarise yourself with Linux and Ubuntu Install the programs and applications you might find useful Further Tasks: Read the following to get the best introduction! LinuxJourney","title":"Tasks"},{"location":"1c_intro_ros2/","text":"Practical 1 Pre-Setup: ROS2 Installation, Intro to ROS2 \u00b6 Practical 1 Pre-Setup: ROS2 Installation, Intro to ROS2 Installation Installation ROS2 Humble Installing Ignition Gazebo Fortress A Brief Introduction to ROS Why does ROS exist? What is ROS ROS concepts through an example ROS2 vs ROS1 About Gazebo Tasks Installation \u00b6 Installation ROS2 Humble \u00b6 For this project, we assume that you are in Ubuntu 22.04 and therefore installing ROS2 Humble and Gazebo Fortress For ROS2 Installation Please see the following instructions: - https://docs.ros.org/en/humble/Installation/Ubuntu-Install-Debs.html You will need to install both the dekstop and build tools sudo apt install ros-humble-desktop sudo apt install ros-dev-tools In order to enable ROS2 tools, it is recommended that you add the startup script to your ~/.bashrc script nano ~/.bashrc The bashrc file should have been opened, then Scroll to bottom of bashrc and append the following to the end, save ( Ctrl+O ) and exit ( Ctrl+X ) source /opt/ros/humble/setup.bash This will ensure that ROS2 is auto-enabled for all terminals which you setup. Ensure your system uses UTF-8, install the required packages, and add the official ROS 2 repository. # Check and set the locale to UTF-8 locale sudo apt update && sudo apt install locales sudo locale-gen en_US en_US.UTF-8 sudo update-locale LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8 export LANG=en_US.UTF-8 locale # Verify settings # Install required packages sudo apt install software-properties-common sudo add-apt-repository universe sudo apt update sudo apt install curl -y # Download and add the ROS 2 GPG key sudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg # Add the ROS 2 repository to your sources list echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(. /etc/os-release && echo $UBUNTU_CODENAME) main\" | sudo tee /etc/apt/sources.list.d/ros2.list > /dev/null # Update and upgrade the system sudo apt update sudo apt upgrade Install ROS 2 and Configure Environment # Install ROS2 sudo apt install ros-humble-desktop sudo apt install ros-dev-tools (Only run the following once) # Auto Source in bashrc to have access to ros2 tools echo 'source /opt/ros/humble/setup.bash' >> $HOME/.bashrc # Since we are running on a large network with multiple other ROS users # Force ROS to only stay on the local machine. echo 'export ROS_LOCALHOST_ONLY=1' >> $HOME/.bashrc Verify your installation by running the following command: ros2 --version Note : In our tutorials please set ROS_LOCALHOST_ONLY=1 otherwise everybody's ROS traffic will interfere with everybody else in the class! Outside of class we would usually segregate ROS traffic only its own network to avoid interference. Installing Ignition Gazebo Fortress \u00b6 Gazebo is the simulation environment that is most often used with ROS2. There are many different versions of gazebo - for us we are using the new version designed for Ubuntu 22.04 known as Gazebo Fortress. The recommended compatible gazebo version for Ubuntu 22.04 and Humble is Fortress where installation instructions are here . But in short: sudo apt-get update sudo apt-get install lsb-release wget gnupg sudo wget https://packages.osrfoundation.org/gazebo.gpg -O /usr/share/keyrings/pkgs-osrf-archive-keyring.gpg echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/pkgs-osrf-archive-keyring.gpg] http://packages.osrfoundation.org/gazebo/ubuntu-stable $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/gazebo-stable.list > /dev/null sudo apt-get update sudo apt-get install ignition-fortress You can check succesful installation by using the ign cli command A Brief Introduction to ROS \u00b6 This section is adapted from this article ROS stands for the Robot Operating System, yet it isn't an actual operating system. It's a framework designed to expedite the development time of robot platforms. To understand what ROS is, we should understand why ROS exists in the first place. Why does ROS exist? \u00b6 In general, software developers avoid hardware like the plague. It's messy, doesn't have consistent behavior, and there's no ctrl-z in sight. Most beginner programmers think you have to have a deep knowledge of electronics and even mechanics to program robots. They think that the hardware and software are so tightly coupled, you have to know both in depth to build anything useful. Software developers became software developers for a reason, so they don't have to deal with hardware. For example, let's say you have to debug a faulty sensor. You first have to take out the sensor from the enclosure, test the sensor thoroughly with a multi meter and various test cases, document its behavior, then examine the hardware -level code to ensure that there were no bugs, and so on. That's a lot of interaction with the hardware that's not fun for someone who just wants to write some cool software. It's harder to attract good programmers if the programming is coupled deeply with hardware. This is where ROS comes into play. With ROS, you can completely abstract the hardware from software, and instead interact with an API that gives access to that data. You can forget about the hardware, and focus on developing the software that makes the robot do what you want. What is ROS \u00b6 ROS is essentially a framework that sits on top of an operating system which defines how particular ROS compatible programs communicate and share data with each other. Essentially ROS defines an interface between which compatible programs can communicate and interact with each other. Over the years that ROS has existed, many people have developed thousands of ROS compatible packages which can be used in a modular fashion. ROS concepts through an example \u00b6 To make it more concrete, imagine that on your drone you have a camera. There are also two processes which require, as inputs, that camera image. Say, a machine learning program, and a position estimation program. Traditionally, you would have to manually serialise (compress) and stream the image over a port which the other two programs could read from. But if the port changes or, say, the camera changes, lots of things have to be reconfigured. However, this sort of interaction can be made streamlined in ROS. Let us consider the programs we have as ROS nodes , i.e. a program which is responsible for one single modular purpose, with particular inputs or outputs: A camera image streaming node OUT: camera image A machine vision system for recognising objects IN: camera image OUT: list of recognised objects A simultaneous localisation and mapping system. IN: camera image OUT: vehicle position These outputs of a node define ROS topics , i.e. a single stream of one type of data. Each topic has a particular name which can be referred to. In our example, some of the topics might be: /drone/camera for the camera image /drone/recognised_objects for the machine vision system /drone/slam_position for the SLAM system Then, we see that there are two avenues of communication created from these node inputs and outputs. graph LR A[Camera] -->|out| node[drone/camera] node --in--> C[Machine Vision] node --in--> D[SLAM] style node fill:#f9f,stroke:#333,stroke-width:4px Now ROS follows a publisher/subscriber model of communication. What that means is that nodes publish data to topics as outputs. But that data is only sent across the network if a different nodes also subscribes to the same topic. So in our example we end up having A camera image streaming node OUT: publishing to /drone/camera A machine vision system for recognising objects IN: subscribed to /drone/camera OUT: publishing to /drone/recognised_objects A simultaneous localisation and mapping system. IN: subscribed to /drone/camera OUT: publishing to /drone/slam_position graph LR A[Camera] -->|out| node[drone/camera] node --in--> C[Vision] C -->|out| node1[drone/recognised_objects] node --in--> D[SLAM] D -->|out| node2[drone/slam_position] style node fill:#f9f,stroke:#333,stroke-width:4px style node1 fill:#f9f,stroke:#333,stroke-width:4px style node2 fill:#f9f,stroke:#333,stroke-width:4px Finally, the data that is sent is not just anything. The data or message is a specifically templated packet of data containing things specified for that paricular use case. In our example for /drone/slam_position topic, the message might be of type geometry_msgs/msg/Point.msg which is defined like so: # This contains the position of a point in free space float64 x float64 y float64 z In other words the message that the /drone/slam_position topic publishes must have a msg.x , msg.y and msg.z field, and the subscriber will only receivea message with those fields. There are a number of messages in the standard ROS library, but many libraries also define their own - as have we in some parts of Starling. This can be summarised in this diagram from the ROS tutorials demonstrates it very nicely: The bottom half of this shows how topics get sent from a publisher to a subscriber. Interestingly, if you put two topics together, you get some notion of two way communication. This is the basis of a service which can be seen in the top of the diagram. A service is made of a Request topic and a Response topic, but functions as a single communication type to the user. Similar to messages, a service has a defined request and response types (e.g. see std_srvs/srv/SetBool.srv ). A service request will often wait until a response is received before continuing. Then if you combine two services and a topic, you can imitate a request for something which takes time. This in ROS is known as an action . For example requesting a robot to move from one location to another. You first request the move, which you get a response as to whether its started. This is followed by constant feedback along a particular topic. Then ended with a task complete service response. In this a whole set of messages are defined. Note that everything happens asyncronously and in parallel, when a node subscribes or sends a requests, it doesn't know when the response will arrive. It only knows it will (hopefully) arrive at some point. When a packet is received the subscriber can then run a method - this method is usually known as a callback , but that will be covered in a later tutorial. Finally, each node is configured by a set of parameters which are broadcast to all other nodes. Parameters are often configuration values for particular methods in a node, and can sometimes be changed on startup (or dynamically through a service), to allow the node to provide adjustable functionality. For example the value of a timeout or frequency of a loop. So in summary, the key concepts and terminology are: Nodes Topics Publishers and Subscribers Messages Services Actions Parameters ROS2 vs ROS1 \u00b6 There are 2 versions of ROS: ROS1 and ROS2. ROS1, initially created in 2007 by Willow Garage, has become huge among the open source robotics community. However over the years they realised that there are a number of important features which are missing - and adding all of these would simply break ROS1. Also the most recent ROS1 distribution (ROS Noetic) is soon to reach the end of its supported life (EOL 2025) with no more ROS1 there after! (See this article for more details!) Therefore, to future proof the system, and to ensure all users get a well rounded experience that will hopefully translate to industry experience, Starling has been implemented in ROS2. Specifically, this tutorial uses the Humble Hawksbill (AKA Humble) Long Term Support (LTS) distribution throughout. There are some interesting changes between ROS1 and ROS2, but the core elements described above remain identical. For those interested, ROS2 follows a much more decentralised paradigm, and does not require a central ROSnode as it uses the distributed DDS communication protocol for its internal communication. All nodes therefore broadcast their own topics allowing for easy decentralised discovery - perfect for multi-robot applications. Note: Main thing to be aware of is if you are debugging and searching for ROS questions on the internet, be aware that there are many existing questions for ROS1 which will no longer apply for ROS2. About Gazebo \u00b6 Gazebo Fortress is a powerful and widely used open-source robotics simulation tool that allows developers and researchers to test, design, and validate their robots in virtual environments before deploying them in the real world. It provides a highly realistic simulation environment, complete with accurate physics, sensor emulation, and 3D visualization. Gazebo Fortress is the latest release in the Gazebo simulation suite, offering enhanced features such as improved physics engines, better integration with ROS2, and support for multi-robot systems. Students can use Gazebo to simulate everything from autonomous drones navigating complex environments to robotic arms performing precision tasks, all without the risk of damaging physical hardware. One of its standout features is the ability to create custom worlds and robots, making it a versatile tool for exploring robotics concepts and conducting experiments. Gazebo\u2019s tight integration with ROS2 also allows students to seamlessly test ROS2-based algorithms and systems within the simulator, bridging the gap between virtual testing and real-world implementation. Technically, Gazebo operates as a standalone simulation engine but shares key concepts with ROS2, including the use of the publish-subscribe architecture . In Gazebo, simulation components such as sensors, robots, and environmental factors communicate through topics, similar to ROS2 nodes. This allows for seamless integration between ROS2 and Gazebo, where ROS2 nodes can subscribe to Gazebo topics (e.g., sensor data) or publish commands (e.g., velocity or actuator inputs). The system we use in this course makes heavy use of Gazebo as the simulation platform for drone activities to enable development. It will not be likely that you will interact with Gazebo direcly, and instead through a ROS2 interface, however having some context is useful! Tasks \u00b6 Install ROS2 as instructed above Familiarise yourself with ROS2 (note the differences if you have previously used ROS1) Do some of the ROS2 tutorials here","title":"ROS2 Installation and ROS2 Re-Intro"},{"location":"1c_intro_ros2/#practical-1-pre-setup-ros2-installation-intro-to-ros2","text":"Practical 1 Pre-Setup: ROS2 Installation, Intro to ROS2 Installation Installation ROS2 Humble Installing Ignition Gazebo Fortress A Brief Introduction to ROS Why does ROS exist? What is ROS ROS concepts through an example ROS2 vs ROS1 About Gazebo Tasks","title":"Practical 1 Pre-Setup: ROS2 Installation, Intro to ROS2"},{"location":"1c_intro_ros2/#installation","text":"","title":"Installation"},{"location":"1c_intro_ros2/#installation-ros2-humble","text":"For this project, we assume that you are in Ubuntu 22.04 and therefore installing ROS2 Humble and Gazebo Fortress For ROS2 Installation Please see the following instructions: - https://docs.ros.org/en/humble/Installation/Ubuntu-Install-Debs.html You will need to install both the dekstop and build tools sudo apt install ros-humble-desktop sudo apt install ros-dev-tools In order to enable ROS2 tools, it is recommended that you add the startup script to your ~/.bashrc script nano ~/.bashrc The bashrc file should have been opened, then Scroll to bottom of bashrc and append the following to the end, save ( Ctrl+O ) and exit ( Ctrl+X ) source /opt/ros/humble/setup.bash This will ensure that ROS2 is auto-enabled for all terminals which you setup. Ensure your system uses UTF-8, install the required packages, and add the official ROS 2 repository. # Check and set the locale to UTF-8 locale sudo apt update && sudo apt install locales sudo locale-gen en_US en_US.UTF-8 sudo update-locale LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8 export LANG=en_US.UTF-8 locale # Verify settings # Install required packages sudo apt install software-properties-common sudo add-apt-repository universe sudo apt update sudo apt install curl -y # Download and add the ROS 2 GPG key sudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg # Add the ROS 2 repository to your sources list echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(. /etc/os-release && echo $UBUNTU_CODENAME) main\" | sudo tee /etc/apt/sources.list.d/ros2.list > /dev/null # Update and upgrade the system sudo apt update sudo apt upgrade Install ROS 2 and Configure Environment # Install ROS2 sudo apt install ros-humble-desktop sudo apt install ros-dev-tools (Only run the following once) # Auto Source in bashrc to have access to ros2 tools echo 'source /opt/ros/humble/setup.bash' >> $HOME/.bashrc # Since we are running on a large network with multiple other ROS users # Force ROS to only stay on the local machine. echo 'export ROS_LOCALHOST_ONLY=1' >> $HOME/.bashrc Verify your installation by running the following command: ros2 --version Note : In our tutorials please set ROS_LOCALHOST_ONLY=1 otherwise everybody's ROS traffic will interfere with everybody else in the class! Outside of class we would usually segregate ROS traffic only its own network to avoid interference.","title":"Installation ROS2 Humble"},{"location":"1c_intro_ros2/#installing-ignition-gazebo-fortress","text":"Gazebo is the simulation environment that is most often used with ROS2. There are many different versions of gazebo - for us we are using the new version designed for Ubuntu 22.04 known as Gazebo Fortress. The recommended compatible gazebo version for Ubuntu 22.04 and Humble is Fortress where installation instructions are here . But in short: sudo apt-get update sudo apt-get install lsb-release wget gnupg sudo wget https://packages.osrfoundation.org/gazebo.gpg -O /usr/share/keyrings/pkgs-osrf-archive-keyring.gpg echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/pkgs-osrf-archive-keyring.gpg] http://packages.osrfoundation.org/gazebo/ubuntu-stable $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/gazebo-stable.list > /dev/null sudo apt-get update sudo apt-get install ignition-fortress You can check succesful installation by using the ign cli command","title":"Installing Ignition Gazebo Fortress"},{"location":"1c_intro_ros2/#a-brief-introduction-to-ros","text":"This section is adapted from this article ROS stands for the Robot Operating System, yet it isn't an actual operating system. It's a framework designed to expedite the development time of robot platforms. To understand what ROS is, we should understand why ROS exists in the first place.","title":"A Brief Introduction to ROS"},{"location":"1c_intro_ros2/#why-does-ros-exist","text":"In general, software developers avoid hardware like the plague. It's messy, doesn't have consistent behavior, and there's no ctrl-z in sight. Most beginner programmers think you have to have a deep knowledge of electronics and even mechanics to program robots. They think that the hardware and software are so tightly coupled, you have to know both in depth to build anything useful. Software developers became software developers for a reason, so they don't have to deal with hardware. For example, let's say you have to debug a faulty sensor. You first have to take out the sensor from the enclosure, test the sensor thoroughly with a multi meter and various test cases, document its behavior, then examine the hardware -level code to ensure that there were no bugs, and so on. That's a lot of interaction with the hardware that's not fun for someone who just wants to write some cool software. It's harder to attract good programmers if the programming is coupled deeply with hardware. This is where ROS comes into play. With ROS, you can completely abstract the hardware from software, and instead interact with an API that gives access to that data. You can forget about the hardware, and focus on developing the software that makes the robot do what you want.","title":"Why does ROS exist?"},{"location":"1c_intro_ros2/#what-is-ros","text":"ROS is essentially a framework that sits on top of an operating system which defines how particular ROS compatible programs communicate and share data with each other. Essentially ROS defines an interface between which compatible programs can communicate and interact with each other. Over the years that ROS has existed, many people have developed thousands of ROS compatible packages which can be used in a modular fashion.","title":"What is ROS"},{"location":"1c_intro_ros2/#ros-concepts-through-an-example","text":"To make it more concrete, imagine that on your drone you have a camera. There are also two processes which require, as inputs, that camera image. Say, a machine learning program, and a position estimation program. Traditionally, you would have to manually serialise (compress) and stream the image over a port which the other two programs could read from. But if the port changes or, say, the camera changes, lots of things have to be reconfigured. However, this sort of interaction can be made streamlined in ROS. Let us consider the programs we have as ROS nodes , i.e. a program which is responsible for one single modular purpose, with particular inputs or outputs: A camera image streaming node OUT: camera image A machine vision system for recognising objects IN: camera image OUT: list of recognised objects A simultaneous localisation and mapping system. IN: camera image OUT: vehicle position These outputs of a node define ROS topics , i.e. a single stream of one type of data. Each topic has a particular name which can be referred to. In our example, some of the topics might be: /drone/camera for the camera image /drone/recognised_objects for the machine vision system /drone/slam_position for the SLAM system Then, we see that there are two avenues of communication created from these node inputs and outputs. graph LR A[Camera] -->|out| node[drone/camera] node --in--> C[Machine Vision] node --in--> D[SLAM] style node fill:#f9f,stroke:#333,stroke-width:4px Now ROS follows a publisher/subscriber model of communication. What that means is that nodes publish data to topics as outputs. But that data is only sent across the network if a different nodes also subscribes to the same topic. So in our example we end up having A camera image streaming node OUT: publishing to /drone/camera A machine vision system for recognising objects IN: subscribed to /drone/camera OUT: publishing to /drone/recognised_objects A simultaneous localisation and mapping system. IN: subscribed to /drone/camera OUT: publishing to /drone/slam_position graph LR A[Camera] -->|out| node[drone/camera] node --in--> C[Vision] C -->|out| node1[drone/recognised_objects] node --in--> D[SLAM] D -->|out| node2[drone/slam_position] style node fill:#f9f,stroke:#333,stroke-width:4px style node1 fill:#f9f,stroke:#333,stroke-width:4px style node2 fill:#f9f,stroke:#333,stroke-width:4px Finally, the data that is sent is not just anything. The data or message is a specifically templated packet of data containing things specified for that paricular use case. In our example for /drone/slam_position topic, the message might be of type geometry_msgs/msg/Point.msg which is defined like so: # This contains the position of a point in free space float64 x float64 y float64 z In other words the message that the /drone/slam_position topic publishes must have a msg.x , msg.y and msg.z field, and the subscriber will only receivea message with those fields. There are a number of messages in the standard ROS library, but many libraries also define their own - as have we in some parts of Starling. This can be summarised in this diagram from the ROS tutorials demonstrates it very nicely: The bottom half of this shows how topics get sent from a publisher to a subscriber. Interestingly, if you put two topics together, you get some notion of two way communication. This is the basis of a service which can be seen in the top of the diagram. A service is made of a Request topic and a Response topic, but functions as a single communication type to the user. Similar to messages, a service has a defined request and response types (e.g. see std_srvs/srv/SetBool.srv ). A service request will often wait until a response is received before continuing. Then if you combine two services and a topic, you can imitate a request for something which takes time. This in ROS is known as an action . For example requesting a robot to move from one location to another. You first request the move, which you get a response as to whether its started. This is followed by constant feedback along a particular topic. Then ended with a task complete service response. In this a whole set of messages are defined. Note that everything happens asyncronously and in parallel, when a node subscribes or sends a requests, it doesn't know when the response will arrive. It only knows it will (hopefully) arrive at some point. When a packet is received the subscriber can then run a method - this method is usually known as a callback , but that will be covered in a later tutorial. Finally, each node is configured by a set of parameters which are broadcast to all other nodes. Parameters are often configuration values for particular methods in a node, and can sometimes be changed on startup (or dynamically through a service), to allow the node to provide adjustable functionality. For example the value of a timeout or frequency of a loop. So in summary, the key concepts and terminology are: Nodes Topics Publishers and Subscribers Messages Services Actions Parameters","title":"ROS concepts through an example"},{"location":"1c_intro_ros2/#ros2-vs-ros1","text":"There are 2 versions of ROS: ROS1 and ROS2. ROS1, initially created in 2007 by Willow Garage, has become huge among the open source robotics community. However over the years they realised that there are a number of important features which are missing - and adding all of these would simply break ROS1. Also the most recent ROS1 distribution (ROS Noetic) is soon to reach the end of its supported life (EOL 2025) with no more ROS1 there after! (See this article for more details!) Therefore, to future proof the system, and to ensure all users get a well rounded experience that will hopefully translate to industry experience, Starling has been implemented in ROS2. Specifically, this tutorial uses the Humble Hawksbill (AKA Humble) Long Term Support (LTS) distribution throughout. There are some interesting changes between ROS1 and ROS2, but the core elements described above remain identical. For those interested, ROS2 follows a much more decentralised paradigm, and does not require a central ROSnode as it uses the distributed DDS communication protocol for its internal communication. All nodes therefore broadcast their own topics allowing for easy decentralised discovery - perfect for multi-robot applications. Note: Main thing to be aware of is if you are debugging and searching for ROS questions on the internet, be aware that there are many existing questions for ROS1 which will no longer apply for ROS2.","title":"ROS2 vs ROS1"},{"location":"1c_intro_ros2/#about-gazebo","text":"Gazebo Fortress is a powerful and widely used open-source robotics simulation tool that allows developers and researchers to test, design, and validate their robots in virtual environments before deploying them in the real world. It provides a highly realistic simulation environment, complete with accurate physics, sensor emulation, and 3D visualization. Gazebo Fortress is the latest release in the Gazebo simulation suite, offering enhanced features such as improved physics engines, better integration with ROS2, and support for multi-robot systems. Students can use Gazebo to simulate everything from autonomous drones navigating complex environments to robotic arms performing precision tasks, all without the risk of damaging physical hardware. One of its standout features is the ability to create custom worlds and robots, making it a versatile tool for exploring robotics concepts and conducting experiments. Gazebo\u2019s tight integration with ROS2 also allows students to seamlessly test ROS2-based algorithms and systems within the simulator, bridging the gap between virtual testing and real-world implementation. Technically, Gazebo operates as a standalone simulation engine but shares key concepts with ROS2, including the use of the publish-subscribe architecture . In Gazebo, simulation components such as sensors, robots, and environmental factors communicate through topics, similar to ROS2 nodes. This allows for seamless integration between ROS2 and Gazebo, where ROS2 nodes can subscribe to Gazebo topics (e.g., sensor data) or publish commands (e.g., velocity or actuator inputs). The system we use in this course makes heavy use of Gazebo as the simulation platform for drone activities to enable development. It will not be likely that you will interact with Gazebo direcly, and instead through a ROS2 interface, however having some context is useful!","title":"About Gazebo"},{"location":"1c_intro_ros2/#tasks","text":"Install ROS2 as instructed above Familiarise yourself with ROS2 (note the differences if you have previously used ROS1) Do some of the ROS2 tutorials here","title":"Tasks"},{"location":"1c_practical_crazyflie/","text":"Practical 2: First time flight \u00b6 Practical 2: First time flight Before you fly Flying Autonomous Flight Before you fly \u00b6 At this point, the crazyflie will have no external sensors and will rely on its internal odometry to keep itself stable. BEFORE YOU FLY PLEASE READ THE FOLLOWING WARNING If the loco board or flow board is installed, the drone will be in position hold mode where it can hold still itself. If they are not the drone will be in manual flight mode and will have no ability to help you fly or stabilise it will drift . The joysticks will be mapped directly to roll/pitch/yaw and thrust. The joystcks control might be quite sensitive so use small movements until you get the hang of it You must be as smooth as you can with your joystick control - try not to twitch in the opposite direction when something happens - this results in a hard to recover drone! Familiarise yourself with the location of the emergency stop if you feel uncomfortable press it at any time (the crazyflies can handle heavy landings) To understand the coordinate system and roll pitch and yaw see the following link: - https://www.bitcraze.io/documentation/system/platform/cf2-coordinate-system/ When you are ready to fly, notify an instructor and place the crazyflie in the middle of your flying space. Flying \u00b6 The instructor will first check all is good with your drone and you have a good setup before you fly. Note: A batery only has around 5 minutes of flying time - keep an eye on it and swap it out if necessary. Just like being introduced to any new drone, you will first try to get a feel for how the drone will react to your control. Here is a list of actions you can go through to check and learn how your drone flies. Arming only test - In this test you will simply arm, wait a second, and then disarm he drone - you will not move the sticks. When you arm the drone, the motors will start spinning at minimum RPM, but not takeoff or otherwise move. Disarming the drone will then stop the motors. Does the drone arm and disarm again straight after without needing a restart Arming ESTOP test - In this test you will familiarise yourself with, and ensure that the emergency stop button works. Arm the drone and then press the estop. The motors should stop and the GUI should show emergency stop has been pressed. Floor thrust test - In this test you will start to familiarise yourself with the thrust control of the controller and response from the drone. You will want to gently increase the thrust, and you should start to see the drone looking like it wants to get off the ground. Try and get a feel for its response! Also worth noting if the drone appears to pivot to one side instead of thrusting equally from all four motors - you may need to compensate a little using the yaw and pitch. Gentle Takeoff and Landing - In this test you will attempt to have the drone takeoff and gently land in a controlled fashion. Building upon the previous, you should gently increase thrust until the drone leaves the ground. When it leaves the ground, try and hold the right thrust level if you can, otherwise reduce the thrust and try and land softly on the ground. Important A lot of beginner flyers will jerk the stick when it takes off - either plunging it into the ceiling or floor. Try to avoid this by holding your nerve and staying in control. If you feel out of control hit the ESTOP Landing Tip - Thust up a little bit just before you hit the floor, this will reduce the impact on the drone. Hovering - Once you are happy and in control with takeoff and landing, you should try and hold the hover for longer, correcting for horizontal drift using the roll/pitch correction. If you feel out of control hit the ESTOP Controlled Flight (no yaw)- Hopefully at this point you should be feeling a lot more in control of the drone (but you probably need a bit of practice!). Now try and slowly fly some shapes (square, circle etc) with the drone always facing away from you (no yaw). Controlled Flight - Now try flying the same shapes, but this time using yaw instead of rolling, i.e. always facing in the direction you want to move. It is recommended you try this by first yawing on the spot then moving forward. When you get more confident, you will be able to yaw while moving - this is the closest you'll get to FPV flying in this course! If working in a pair, don't forget to swap over! You may have noticed in the GUI and on the control scheme, there is a button labelled Assist Mode this will perform an automatic takeoff and activate hover mode as long as its pressed down. Read the Flow deck tutorial for more information. Dont worry if you don't get through all of these maneouvers, these take practice to learn and master! The goal here is for you to have some hands on experience on what a flying vehicle is like and have some experience with the physical system before we start playing with autonomy. If you want a challenge, remove the optical flow board / loco positioning board and try flying in manual mode WARNING you will have no automatic positioning or hovering - you will have to manually control this yourself. Autonomous Flight \u00b6 Having hopefully flown the drone manually, you should now be beginning to form some intuitions as to how a drone flies, its features, things to watch out for and so on. Now we move onto starting to consider a key part of this course - which is autonomy. Autonomy is a bit different for drones over other robots you may have used in the past. Unlike ground vehicles, we have to consider the effects of a 3rd dimension, and need some form of cascaded control to stay in the air. Also whilst the inverse kinematics calculations for arms are not necessary for drones - features such as smooth trajectory generation and following, and mission planning are neccesary. Thankfully, crazyflie offers a simple starting point with a python api for autonomous flight. This will get you back up to speed with programming and testing robots before we delve into more complex autonomous systems which you would see flavours of in the real world. Remember at the end of the day, all of these different frameworks are just tools - we hope that through learning to use these tools, you also absorb and learn more generalisable features of drone autonomy which you could apply to the many other open source and proprietary systems which exist out there! For crazyflie autonomy, try out the following links which show a simple getting started! Flow Deck tutorial Multi-Ranger Deck tutorial Loco Deck tutorial Example Scripts Link Your goal here is to have the drone fly in some shapes autonomosly As a bonus, see if you can start to work out how to use the LED Deck! You will likely needed to look up a little of the documentation for the api but I'm sure you can mostly use the examples to work out the best way to program the drones to do what you need. Don't worry about making things too complicated, as we will be quickly moving on to ROS2 for drone control!","title":"Practical 2: First time flight"},{"location":"1c_practical_crazyflie/#practical-2-first-time-flight","text":"Practical 2: First time flight Before you fly Flying Autonomous Flight","title":"Practical 2: First time flight"},{"location":"1c_practical_crazyflie/#before-you-fly","text":"At this point, the crazyflie will have no external sensors and will rely on its internal odometry to keep itself stable. BEFORE YOU FLY PLEASE READ THE FOLLOWING WARNING If the loco board or flow board is installed, the drone will be in position hold mode where it can hold still itself. If they are not the drone will be in manual flight mode and will have no ability to help you fly or stabilise it will drift . The joysticks will be mapped directly to roll/pitch/yaw and thrust. The joystcks control might be quite sensitive so use small movements until you get the hang of it You must be as smooth as you can with your joystick control - try not to twitch in the opposite direction when something happens - this results in a hard to recover drone! Familiarise yourself with the location of the emergency stop if you feel uncomfortable press it at any time (the crazyflies can handle heavy landings) To understand the coordinate system and roll pitch and yaw see the following link: - https://www.bitcraze.io/documentation/system/platform/cf2-coordinate-system/ When you are ready to fly, notify an instructor and place the crazyflie in the middle of your flying space.","title":"Before you fly"},{"location":"1c_practical_crazyflie/#flying","text":"The instructor will first check all is good with your drone and you have a good setup before you fly. Note: A batery only has around 5 minutes of flying time - keep an eye on it and swap it out if necessary. Just like being introduced to any new drone, you will first try to get a feel for how the drone will react to your control. Here is a list of actions you can go through to check and learn how your drone flies. Arming only test - In this test you will simply arm, wait a second, and then disarm he drone - you will not move the sticks. When you arm the drone, the motors will start spinning at minimum RPM, but not takeoff or otherwise move. Disarming the drone will then stop the motors. Does the drone arm and disarm again straight after without needing a restart Arming ESTOP test - In this test you will familiarise yourself with, and ensure that the emergency stop button works. Arm the drone and then press the estop. The motors should stop and the GUI should show emergency stop has been pressed. Floor thrust test - In this test you will start to familiarise yourself with the thrust control of the controller and response from the drone. You will want to gently increase the thrust, and you should start to see the drone looking like it wants to get off the ground. Try and get a feel for its response! Also worth noting if the drone appears to pivot to one side instead of thrusting equally from all four motors - you may need to compensate a little using the yaw and pitch. Gentle Takeoff and Landing - In this test you will attempt to have the drone takeoff and gently land in a controlled fashion. Building upon the previous, you should gently increase thrust until the drone leaves the ground. When it leaves the ground, try and hold the right thrust level if you can, otherwise reduce the thrust and try and land softly on the ground. Important A lot of beginner flyers will jerk the stick when it takes off - either plunging it into the ceiling or floor. Try to avoid this by holding your nerve and staying in control. If you feel out of control hit the ESTOP Landing Tip - Thust up a little bit just before you hit the floor, this will reduce the impact on the drone. Hovering - Once you are happy and in control with takeoff and landing, you should try and hold the hover for longer, correcting for horizontal drift using the roll/pitch correction. If you feel out of control hit the ESTOP Controlled Flight (no yaw)- Hopefully at this point you should be feeling a lot more in control of the drone (but you probably need a bit of practice!). Now try and slowly fly some shapes (square, circle etc) with the drone always facing away from you (no yaw). Controlled Flight - Now try flying the same shapes, but this time using yaw instead of rolling, i.e. always facing in the direction you want to move. It is recommended you try this by first yawing on the spot then moving forward. When you get more confident, you will be able to yaw while moving - this is the closest you'll get to FPV flying in this course! If working in a pair, don't forget to swap over! You may have noticed in the GUI and on the control scheme, there is a button labelled Assist Mode this will perform an automatic takeoff and activate hover mode as long as its pressed down. Read the Flow deck tutorial for more information. Dont worry if you don't get through all of these maneouvers, these take practice to learn and master! The goal here is for you to have some hands on experience on what a flying vehicle is like and have some experience with the physical system before we start playing with autonomy. If you want a challenge, remove the optical flow board / loco positioning board and try flying in manual mode WARNING you will have no automatic positioning or hovering - you will have to manually control this yourself.","title":"Flying"},{"location":"1c_practical_crazyflie/#autonomous-flight","text":"Having hopefully flown the drone manually, you should now be beginning to form some intuitions as to how a drone flies, its features, things to watch out for and so on. Now we move onto starting to consider a key part of this course - which is autonomy. Autonomy is a bit different for drones over other robots you may have used in the past. Unlike ground vehicles, we have to consider the effects of a 3rd dimension, and need some form of cascaded control to stay in the air. Also whilst the inverse kinematics calculations for arms are not necessary for drones - features such as smooth trajectory generation and following, and mission planning are neccesary. Thankfully, crazyflie offers a simple starting point with a python api for autonomous flight. This will get you back up to speed with programming and testing robots before we delve into more complex autonomous systems which you would see flavours of in the real world. Remember at the end of the day, all of these different frameworks are just tools - we hope that through learning to use these tools, you also absorb and learn more generalisable features of drone autonomy which you could apply to the many other open source and proprietary systems which exist out there! For crazyflie autonomy, try out the following links which show a simple getting started! Flow Deck tutorial Multi-Ranger Deck tutorial Loco Deck tutorial Example Scripts Link Your goal here is to have the drone fly in some shapes autonomosly As a bonus, see if you can start to work out how to use the LED Deck! You will likely needed to look up a little of the documentation for the api but I'm sure you can mostly use the examples to work out the best way to program the drones to do what you need. Don't worry about making things too complicated, as we will be quickly moving on to ROS2 for drone control!","title":"Autonomous Flight"},{"location":"1d_aerostack2/","text":"Practical 1: Aerostack2 Framework For Aerial Robotic Systems \u00b6 Practical 1: Aerostack2 Framework For Aerial Robotic Systems Robotic Frameworks For Aerial Control Aerostack2 Overview Using Aerostack2 Core Aerostack Libraries Project Repository Tasks Robotic Frameworks For Aerial Control \u00b6 As robotic and autonomous systems proliferate into the wider world, there is a need to address the difficulties of system development and deployment at scale. There is evidence that industry is directly facing these challenges through the use of cloud computing, continuous integration and similar systems inspired from very successful and agile software development processes. This is made clear through offerings such as Amazon's AWS Robomaker , Google's cloud robotics platforms and so on. However, there is a great lack of such systems in most academic settings. The result's oriented attitude of many labs often leads to each researcher building a bespoke solution in order to evaluate, validate or prove their goals. These bespoke solutions are often inflexible, not extensible, difficult to understand and, importantly, reuse, with any level of confidence. This becomes especially difficult when coupled with hardware, such as UAVs, where many operational details have been implicitly assumed or ignored for favour of getting the experiment running as quick as possible. In addition these solutions are often poorly structured and maintained with little to no documentation meaning that it is difficult for researchers to build upon these systems. This is an exceptionally large hurdle to researchers who do not have strong software backgrounds, but wish to perform real world experiments which could improve the quality of research outputs. This is not to say that it is impossible for a research system to be developed into a reusable platform. There are many examples of research systems being ubiquitous within a group or being released outside the lab. For instance, the Robotarium at Georgia Tech , the Multi-Robot Systems Group at the Czech Technical University with their experimental system , and the PX4 autopilot which began it's life as a collaboration between a number of labs at ETH Zurich. But what we see is that it takes a concerted effort and many years of coincidental work which provide incremental improvements to the system each time. Previously I had attempted to develop a system which solved this problem called starling which had a number of flaws. Therefore as an example system for you to learn, we have chosen the aerostack2 as our aerial robotic systems development platform. This will enable the following: Supports single or multiple drones with low (control) or high level (path planning) experimentation. Supports the transition between simulation to indoor flight to outdoor flight. Provides a simple and easy to use interface for researchers to implement experimentation without knowledge of hardware specifics. We hope that through using aerostack2, you develop an overview of the components required in an aerial system, and gain experiences which can then be applied to whatever systems you work with in the future. Aerostack2 \u00b6 Link To Documentation Link To Github UCL fork of aerostack2: Github Overview \u00b6 Aerostack2 (AS2) is an open source software framework that helps developers design and build the control architecture of aerial robotic systems, integrating multiple heterogeneous computational solutions (e.g., computer vision algorithms, motion controllers, self-localization and mapping methods, motion planning algorithms, etc.), built for ROS 2 Humble and ROS 2 Galactic. Aerostack2 is useful for building autonomous aerial systems in complex and dynamic environments and it is also a useful research tool for aerial robotics to test new algorithms and architectures. It was created to be available for communities of researchers and developers and it is currently an active open-source project with periodic software releases. Aerostack2 is versatile for building different system configurations with various degrees of autonomy. It\u2019s most important features are: From teleoperation to autonomous flight. Aerostack2 can be used for teleoperation flights (with manual control) but it can also be used for building autonomous robot systems to perform aerial missions without operator assistance. Single robots or multi-robot systems. Aerostack2 can be used to fly a swarm of heterogeneous drones to perform multi-aerial-robot missions. It has been validated to operate with severals drones simultaneously, both in indoor and outdoor environments. Flexible for different applications. Aerostack2 can be used by system designers to develop their own systems in a wide range of applications. Aerostack2 provides languages and graphical tools to configure specific aerial missions. Hardware independent. Aerostack2 runs on conventional laptops and it has also run on onboard computers like Nvidia Jetson NX. Aerostack2 has been used in different aerial platforms, including, but not limited to: DJI platforms (Matrice 210RTKv2, Matrice 300, Ryze Tello), Pixhawk autopilots and Crazyflie drones. The framework can operate in simulation and in a real environment in a similar way, what simplifies the Sim2Real development. Complete modularity, allowing elements to be changed or interchanged without affecting the rest of the system. Plugin-based architecture allows to use different implementations for every tasks. Project-oriented, allowing to install and use only the necessary packages and configurations for the application to be developed Using Aerostack2 \u00b6 In practice, a Aerostack ROS2 working project comprises of two parts: The Core Aerostack Libraries This provides the core ros2 nodes, funcionalities and interfaces and will be described in more detail below. It is simply another ROS2 library like any other. An Aerostack2 Project Repository (Application) This is a project folder which interacts and manages a specific application you want to develop with aerostack2. This contains a number of scripts, as well as the python api interface scripts for easy implementation of applications. Core Aerostack Libraries \u00b6 Aerostack2 provides existing implementations of, and methods of extending key parts of a flight control stack. This includes: Support of different vehicle platforms - both real drone platforms such as a crazyflie or PX4 and also gazebo for simulation Robotic Functions - essential software components and algorithms essential for aerial robotics functionality such as State Estimation (External Source, Ground Truth, Motion Capture) Motion Control (PID, Differential Flatness) Emergency Handling Behaviours - Individual built-in components that implement specific high level robotic functionalities such as the following. This allows a mission plan to be expressed as a sequence of activated behaviours in series or in parallel. A key aspect of behaviours is that each individually has state defining whether a behaviour can be activated, if it is still performing that behaviour, deactivating the behaviour and so on. Motion Control (e.g. Takeing Off, Landing, Go_to ) Perception (e.g. Detect Marker, Record Video, Control Gimbal) Planning (e.g. Trajectory Generation) Mission Control - Highest level user facing components for designing and implementing missions of behaviours. Behaviour Trees Python API Mission Monitoring (Rviz) See this page for more details The core libraries can be installed for apt-get if no changes are necessary. However in this course, we will be building from scratch as it allows us to make quick fixes versions within our repository ( UCL AS2 REPO ). To install aerostack, you create a new ros2 workspace and clone our aerostack into the src folder, download depenedencies, and build as usual See these instructions We will be diving further into a specific example in the next article. Project Repository \u00b6 The mission control elements of aerostack2 combined with ROS2 allow a high degree of separation between the core libraries and any application specific code. This is great because it means that the core of aerostack is application agnostic, allowing it to be applied to various different specific applications. The specific application then comes in the form of its own repository which is in charge of specifying which nodes are to be run, and in what configuration for a particular use case. A python script can then be used to run the core mission logic, invoking behaviours and telling the drone what to do and when. AS2 developers have built up a recommended pattern of usage for invoking and using aerostack2 functionality. This primarily revolves around the use of the linux command line, bash scripts and a tool called tmux . Tmux is a 'terminal multiplexer' which allows the operation of multiple terminals simultaneously from within the same window. Combined with the tmuxinator tool which can define configurations of tmux windows, tmux serves to run all of the many ROS2 nodes. This makes things easier as we would ordinarly need to manual start, or replaces a standard ROS2 launch file whilst making monitoring easier in these larger complex systems. A launch bash script, often called launch_as2.bash is then used to spin up all of the ROS2 nodes representing a single drone by managing the various environment variables and runtime options, and then invoking tmux. A second bash script is used to represent the ground station launch_ground_station.bash which launches central processes such as visualisation, motion capture, rosbag recording or other external data sources. Finally a mission can be defined in python using AS2's python API and the ROS2 API. Then either from within the drone or ground station tmux, this mission can be started. Lets take a look at the project gazebo example project for working with gazebo - all of the simulation based projects are based on this repository. Project Gazebo All projects in aerostack2 are structured in the same way. The project is divided into the following directories: tmuxinator : Contains the tmuxinator launch file, which is used to launch all aerostack2 nodes. aerostack2.yaml : Tmuxinator launch file for each drone. The list of nodes to be launched is defined here. ground_station.yaml : Tmuxinator launch file for the ground station. The list of nodes to be launched is defined here. config : Contains the configuration files for the launchers of the nodes in the drones. config_ground_station : Contains the configuration files for the launchers of the nodes in the ground station. launch_as2.bash : Script to launch nodes defined in tmuxinator/aerostack2.yaml . launch_ground_station.bash : Script to launch nodes defined in tmuxinator/ground_station.yaml . mission_*.py : Differents python mission files that can be executed. stop.bash : Script to stop all nodes launched by launch_as2.bash and launch_ground_station.bash . rosbag/record_rosbag.bash : Script to record a rosbag. Can be modified to record only the topics that are needed. trees* : Contains the behavior trees that can be executed. They can be selected in the aerostack2.yaml file. utils : Contains utils scripts for launchers. Both python and bash scripts have a help message that can be displayed by running the script with the -h option. For example, ./launch_as2.bash -h will display the help message for the launch_as2.bash script. Note: Whilst this is the main template, the different projects will have variants depending on their needs and therefore might look and contain slightly different things. We will go into more details of usage instructions in the next article, but here is the link to how to run this example: Link Tasks \u00b6 Install Aerostack2 as instructed above Read the Aerostack2 documentation to understand the architecture and different components. What is the purpose of aerostack? What are the key components that it implements? Why are we using it here? Follow the instructions and install project gazebo What are the purposes of each of the files and folders Run Project Gazebo with the teleoperation panel Takeoff the Simulated drone and fly in around in some shapes Try switching to velocity mode - whats the difference? Run Project Gazebo with the example mission script What does the script do Run Project Gazebo with your own modified mission script","title":"Aerostack2 Introduction"},{"location":"1d_aerostack2/#practical-1-aerostack2-framework-for-aerial-robotic-systems","text":"Practical 1: Aerostack2 Framework For Aerial Robotic Systems Robotic Frameworks For Aerial Control Aerostack2 Overview Using Aerostack2 Core Aerostack Libraries Project Repository Tasks","title":"Practical 1: Aerostack2 Framework For Aerial Robotic Systems"},{"location":"1d_aerostack2/#robotic-frameworks-for-aerial-control","text":"As robotic and autonomous systems proliferate into the wider world, there is a need to address the difficulties of system development and deployment at scale. There is evidence that industry is directly facing these challenges through the use of cloud computing, continuous integration and similar systems inspired from very successful and agile software development processes. This is made clear through offerings such as Amazon's AWS Robomaker , Google's cloud robotics platforms and so on. However, there is a great lack of such systems in most academic settings. The result's oriented attitude of many labs often leads to each researcher building a bespoke solution in order to evaluate, validate or prove their goals. These bespoke solutions are often inflexible, not extensible, difficult to understand and, importantly, reuse, with any level of confidence. This becomes especially difficult when coupled with hardware, such as UAVs, where many operational details have been implicitly assumed or ignored for favour of getting the experiment running as quick as possible. In addition these solutions are often poorly structured and maintained with little to no documentation meaning that it is difficult for researchers to build upon these systems. This is an exceptionally large hurdle to researchers who do not have strong software backgrounds, but wish to perform real world experiments which could improve the quality of research outputs. This is not to say that it is impossible for a research system to be developed into a reusable platform. There are many examples of research systems being ubiquitous within a group or being released outside the lab. For instance, the Robotarium at Georgia Tech , the Multi-Robot Systems Group at the Czech Technical University with their experimental system , and the PX4 autopilot which began it's life as a collaboration between a number of labs at ETH Zurich. But what we see is that it takes a concerted effort and many years of coincidental work which provide incremental improvements to the system each time. Previously I had attempted to develop a system which solved this problem called starling which had a number of flaws. Therefore as an example system for you to learn, we have chosen the aerostack2 as our aerial robotic systems development platform. This will enable the following: Supports single or multiple drones with low (control) or high level (path planning) experimentation. Supports the transition between simulation to indoor flight to outdoor flight. Provides a simple and easy to use interface for researchers to implement experimentation without knowledge of hardware specifics. We hope that through using aerostack2, you develop an overview of the components required in an aerial system, and gain experiences which can then be applied to whatever systems you work with in the future.","title":"Robotic Frameworks For Aerial Control"},{"location":"1d_aerostack2/#aerostack2","text":"Link To Documentation Link To Github UCL fork of aerostack2: Github","title":"Aerostack2"},{"location":"1d_aerostack2/#overview","text":"Aerostack2 (AS2) is an open source software framework that helps developers design and build the control architecture of aerial robotic systems, integrating multiple heterogeneous computational solutions (e.g., computer vision algorithms, motion controllers, self-localization and mapping methods, motion planning algorithms, etc.), built for ROS 2 Humble and ROS 2 Galactic. Aerostack2 is useful for building autonomous aerial systems in complex and dynamic environments and it is also a useful research tool for aerial robotics to test new algorithms and architectures. It was created to be available for communities of researchers and developers and it is currently an active open-source project with periodic software releases. Aerostack2 is versatile for building different system configurations with various degrees of autonomy. It\u2019s most important features are: From teleoperation to autonomous flight. Aerostack2 can be used for teleoperation flights (with manual control) but it can also be used for building autonomous robot systems to perform aerial missions without operator assistance. Single robots or multi-robot systems. Aerostack2 can be used to fly a swarm of heterogeneous drones to perform multi-aerial-robot missions. It has been validated to operate with severals drones simultaneously, both in indoor and outdoor environments. Flexible for different applications. Aerostack2 can be used by system designers to develop their own systems in a wide range of applications. Aerostack2 provides languages and graphical tools to configure specific aerial missions. Hardware independent. Aerostack2 runs on conventional laptops and it has also run on onboard computers like Nvidia Jetson NX. Aerostack2 has been used in different aerial platforms, including, but not limited to: DJI platforms (Matrice 210RTKv2, Matrice 300, Ryze Tello), Pixhawk autopilots and Crazyflie drones. The framework can operate in simulation and in a real environment in a similar way, what simplifies the Sim2Real development. Complete modularity, allowing elements to be changed or interchanged without affecting the rest of the system. Plugin-based architecture allows to use different implementations for every tasks. Project-oriented, allowing to install and use only the necessary packages and configurations for the application to be developed","title":"Overview"},{"location":"1d_aerostack2/#using-aerostack2","text":"In practice, a Aerostack ROS2 working project comprises of two parts: The Core Aerostack Libraries This provides the core ros2 nodes, funcionalities and interfaces and will be described in more detail below. It is simply another ROS2 library like any other. An Aerostack2 Project Repository (Application) This is a project folder which interacts and manages a specific application you want to develop with aerostack2. This contains a number of scripts, as well as the python api interface scripts for easy implementation of applications.","title":"Using Aerostack2"},{"location":"1d_aerostack2/#core-aerostack-libraries","text":"Aerostack2 provides existing implementations of, and methods of extending key parts of a flight control stack. This includes: Support of different vehicle platforms - both real drone platforms such as a crazyflie or PX4 and also gazebo for simulation Robotic Functions - essential software components and algorithms essential for aerial robotics functionality such as State Estimation (External Source, Ground Truth, Motion Capture) Motion Control (PID, Differential Flatness) Emergency Handling Behaviours - Individual built-in components that implement specific high level robotic functionalities such as the following. This allows a mission plan to be expressed as a sequence of activated behaviours in series or in parallel. A key aspect of behaviours is that each individually has state defining whether a behaviour can be activated, if it is still performing that behaviour, deactivating the behaviour and so on. Motion Control (e.g. Takeing Off, Landing, Go_to ) Perception (e.g. Detect Marker, Record Video, Control Gimbal) Planning (e.g. Trajectory Generation) Mission Control - Highest level user facing components for designing and implementing missions of behaviours. Behaviour Trees Python API Mission Monitoring (Rviz) See this page for more details The core libraries can be installed for apt-get if no changes are necessary. However in this course, we will be building from scratch as it allows us to make quick fixes versions within our repository ( UCL AS2 REPO ). To install aerostack, you create a new ros2 workspace and clone our aerostack into the src folder, download depenedencies, and build as usual See these instructions We will be diving further into a specific example in the next article.","title":"Core Aerostack Libraries"},{"location":"1d_aerostack2/#project-repository","text":"The mission control elements of aerostack2 combined with ROS2 allow a high degree of separation between the core libraries and any application specific code. This is great because it means that the core of aerostack is application agnostic, allowing it to be applied to various different specific applications. The specific application then comes in the form of its own repository which is in charge of specifying which nodes are to be run, and in what configuration for a particular use case. A python script can then be used to run the core mission logic, invoking behaviours and telling the drone what to do and when. AS2 developers have built up a recommended pattern of usage for invoking and using aerostack2 functionality. This primarily revolves around the use of the linux command line, bash scripts and a tool called tmux . Tmux is a 'terminal multiplexer' which allows the operation of multiple terminals simultaneously from within the same window. Combined with the tmuxinator tool which can define configurations of tmux windows, tmux serves to run all of the many ROS2 nodes. This makes things easier as we would ordinarly need to manual start, or replaces a standard ROS2 launch file whilst making monitoring easier in these larger complex systems. A launch bash script, often called launch_as2.bash is then used to spin up all of the ROS2 nodes representing a single drone by managing the various environment variables and runtime options, and then invoking tmux. A second bash script is used to represent the ground station launch_ground_station.bash which launches central processes such as visualisation, motion capture, rosbag recording or other external data sources. Finally a mission can be defined in python using AS2's python API and the ROS2 API. Then either from within the drone or ground station tmux, this mission can be started. Lets take a look at the project gazebo example project for working with gazebo - all of the simulation based projects are based on this repository. Project Gazebo All projects in aerostack2 are structured in the same way. The project is divided into the following directories: tmuxinator : Contains the tmuxinator launch file, which is used to launch all aerostack2 nodes. aerostack2.yaml : Tmuxinator launch file for each drone. The list of nodes to be launched is defined here. ground_station.yaml : Tmuxinator launch file for the ground station. The list of nodes to be launched is defined here. config : Contains the configuration files for the launchers of the nodes in the drones. config_ground_station : Contains the configuration files for the launchers of the nodes in the ground station. launch_as2.bash : Script to launch nodes defined in tmuxinator/aerostack2.yaml . launch_ground_station.bash : Script to launch nodes defined in tmuxinator/ground_station.yaml . mission_*.py : Differents python mission files that can be executed. stop.bash : Script to stop all nodes launched by launch_as2.bash and launch_ground_station.bash . rosbag/record_rosbag.bash : Script to record a rosbag. Can be modified to record only the topics that are needed. trees* : Contains the behavior trees that can be executed. They can be selected in the aerostack2.yaml file. utils : Contains utils scripts for launchers. Both python and bash scripts have a help message that can be displayed by running the script with the -h option. For example, ./launch_as2.bash -h will display the help message for the launch_as2.bash script. Note: Whilst this is the main template, the different projects will have variants depending on their needs and therefore might look and contain slightly different things. We will go into more details of usage instructions in the next article, but here is the link to how to run this example: Link","title":"Project Repository"},{"location":"1d_aerostack2/#tasks","text":"Install Aerostack2 as instructed above Read the Aerostack2 documentation to understand the architecture and different components. What is the purpose of aerostack? What are the key components that it implements? Why are we using it here? Follow the instructions and install project gazebo What are the purposes of each of the files and folders Run Project Gazebo with the teleoperation panel Takeoff the Simulated drone and fly in around in some shapes Try switching to velocity mode - whats the difference? Run Project Gazebo with the example mission script What does the script do Run Project Gazebo with your own modified mission script","title":"Tasks"},{"location":"2_practical_gazebo_aruco/","text":"Practical 2: ROS2 with Aerostack2 \u00b6 Practical 2: ROS2 with Aerostack2 Mini-Challenge Aerostack2 Installation Local Installation Setup and build Aerostack2 (we use version 1.1.2) Setup this project Docker Running the simulator What does the simulator run A quick aside on Tmux Controlling with teleoperation Running the example autonomous mission Stopping the simulator Dissecting the controller Modifying the controller Tasks NOTE: If you run into any issues, create an issue on the github repository: project_gazebo_aruco Mini-Challenge \u00b6 In this simulation mini-challenge, we have setup a small project in which there are a number of aruco codes in a line in front of the drone. There are only two types of aruco code arranged such that for n codes in a line, the first n-1 will be of type 1, and the nth will be type 2. Your task is to write an algorithm which can detect aruco markers from the drones camera, and fly forward until you reach the type 2 code, and land on it. In order to do this, you must be able to first have installed aerostack2 and the project code. You will then need to have understood how the mission script works and how to subscribe and process the drone stream. Finally you will need to work out how to control the drone from the drone stream. Aerostack2 Installation \u00b6 No matter how you run the linux system, you can try each of the following two options (Local or Docker). Local Installation \u00b6 Setup and build Aerostack2 (we use version 1.1.2) \u00b6 In your home directory (could be anywhere else but all the paths below are for your home directory) mkdir -p ~/project_gazebo_ws/src cd ~/project_gazebo_ws/src git clone https://github.com/UCL-MSC-RAI-COMP0240/aerostack2.git Make sure you clone our UCL-MSC-RAI-COMP0240 fork of aerostack2 - a fork is a fancy name for a copy of the original repository but still remains connected - such that if the original developers ever update their version, we can decide to also pull in the new updates. But it remains separate such that we can also add our own updates for you. This will create a ros2 workspace and place the aerostack2 repository in it. You can see that aerostack2 itself is made up of many different ros nodes and packages. You will need to install the dependencies by running the following: sudo apt install git python3-rosdep python3-pip python3-colcon-common-extensions tmux tmuxinator -y pip3 install pysimplegui-4-foss And then going back into the root workspace to install the remaining ROS2 dependencies automatically. cd ~/project_gazebo_ws sudo rosdep init rosdep update rosdep install -y -r -q --from-paths src --ignore-src Then, enable the handy aerostack2 cli (only run this once) echo 'export AEROSTACK2_PATH=$HOME/project_gazebo_ws/src/aerostack2' >> $HOME/.bashrc echo 'source $AEROSTACK2_PATH/as2_cli/setup_env.bash' >> $HOME/.bashrc source ~/.bashrc This will enable you to build the project from any folder using It will take a couple of minutes as it builds all of aerostack2! as2 build Now as2 should be installed. Finally add the following to your bashrc echo 'source $HOME/project_gazebo_ws/install/setup.bash' >> $HOME/.bashrc source ~/.bashrc Setup this project \u00b6 Get this project locally mkdir -p ~/project_gazebo_ws/src cd ~/project_gazebo_ws/src git clone https://github.com/UCL-MSC-RAI-COMP0240/project_gazebo_aruco.git Run the example using cd ~/project_gazebo_ws/src/project_gazebo_aruco ./launch_as2.bash -s -t Docker \u00b6 If you have locally installed this project already, you can elect not to do this section. Otherwise you could give it a go if you usually run a different version of ubuntu. Ensure Docker or Docker Desktop is installed on your machine First your will need to clone this project somewhere (doesn't need to be in a ros2 workspace) git clone https://github.com/UCL-MSC-RAI-COMP0240/project_gazebo_aruco.git To build and/or run the container run the script This container is based on Ubuntu 22.04, ROS2 Humble and Ignition Gazebo Fortress ./docker/docker_start.bash After building for a while, this will drop you inside the docker container. The container will have live mounted this project into /ros2/project_gazebo_aruco so that any changes made to this repository outside of the container will be reflected inside. Inside the container, navigate to that repository and run the example. cd /ros2/project_gazebo_aruco ./launch_as2.bash -s -t Note that you can utilise a GPU if you install the nvidia-container-toolkit . Pass the -nvidia argument to docker_start.bash Running the simulator \u00b6 In your selected environment, start the aerostack2 simulation environment cd ~/project_gazebo_ws/src/project_gazebo_aruco # or docker: cd /ros2/project_gazebo_aruco ./launch_as2.bash -s -t -v The launch_as2.bash file runs a bash script which controls what elements are also started up. The -s indicates that the system should be launched in simulation mode. The -t opens up the teleoperation remote control window to control the drone. The -v opens up a visualisation software called rviz2. You can have a look at -h for all of the different options, some won't be relevant for this task. What does the simulator run \u00b6 The aerostack simulator runs a number of different modules for ensuring successful flight this includes: Platform Interface State Estimation Motion Controllers Behaviour Controllers (Takeoff, Land, Go-To) You can see the status of these other modules if you select the terminal and press Ctrl + B and then press 0 ... 5 to change windows. In a single window you can switch between panes by pressing Ctrl + B and then a direction arrow. A quick aside on Tmux \u00b6 This fancy terminal environment you find yourself in is known as tmux . Tmux is a way for to allow terminal users to run and view multiple programs at the same time - in the case for us, multiple ROS2 nodes doing different things. Here they are organised in windows and panes. I would recommend reading through a guide such as the following, and just trying it out! https://hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/ The useful feature for us is that you can define configuration files for running a bunch of different programs - especially useful for complex systems with multiple components to spin up, but also observe (observation being one of the main issues with ros launch files). Controlling with teleoperation \u00b6 Using the -t option will open up the teleoperation panel, allowing you to attempt to manually fly the robot in simulation. With the teleoperation panel clicked on and selected, you can press the T key to takeoff. The arrow keys will then control the direction of flight of the drone in position control mode (you tell the drone where specifically to go). Notice how the drone flies around in the simulation and in rviz. Also see how the image within rviz changes! The image is from a bottom mounted camera on the simulated drone, mirroring the setup you will be building with the real drone. Play around a fly the drone around! Running the example autonomous mission \u00b6 In the original terminal (Use Ctrl + B + 5 ), we can run any scripts we want. The mission_*.py scripts are some examples of these autonomous missions. To run one of these examples, in the terminal type: python3 mission_camera.py This example mission gives you a camera stream from the drone, takes off and arms the drone and flies it around using a few different methods. Key points is the use of ROS2 to subscribe to the camera topic published by the drone model. Whenever an image is received, it will run the img_callback function. Stopping the simulator \u00b6 In order to stop the simulator cleanly, in any terminal run the ./stop.bash script. ./stop.bash This will stop all containers and relevant programs to the simulator in a clean manner. Sometimes the simulated drone will go into an unrecoverable state - you may need to resart the simulator. Dissecting the controller \u00b6 Here is the example controller. It is documented! #!/bin/python3 \"\"\" CAMERA SAMPLE MISSION This file is an example mission which reads from the aerostack drone camera and prints it to screen It also flies around using position and velocity control camera topic \"\"\" # Imports import time import rclpy import argparse from as2_python_api.drone_interface import DroneInterface from rclpy.qos import qos_profile_sensor_data from sensor_msgs.msg import Image, CameraInfo from cv_bridge import CvBridge import cv2 ######## Drone Control Class ################### class DroneMotionRef(DroneInterface): \"\"\"Drone Interface This is the aerostack2 drone interface for connecting to simulated and real drones. It runs as a ROS2 Node which interacts with the currently available ROS2 topics. It defines the variables that represent a single drone, i.e. - Platform Information - Vehicle Pose and Twist (angular velocity) - Functions to control the hardware of the drone (arm, disarm, change mode, estop) It also contains some modules for flying the drone, this includes: - Takeoff, Landing (self.takeoff, self.land) - GoTo position control (self.go_to) [https://github.com/aerostack2/aerostack2/blob/main/as2_python_api/as2_python_api/modules/go_to_module.py] - FollowPath module (self.follow_path) [https://github.com/aerostack2/aerostack2/blob/main/as2_python_api/as2_python_api/modules/follow_path_module.py] Other module exist which could be used to. Their interfaces and functions can be referenced most easily in the code. Some Documentation is here: https://aerostack2.github.io/_09_development/_api_documentation/temp_ws/src/as2_python_api/docs/source/as2_python_api.html The Source Code is here: https://github.com/aerostack2/aerostack2/tree/main/as2_python_api Drone Interface Base.py: https://github.com/aerostack2/aerostack2/blob/main/as2_python_api/as2_python_api/drone_interface_base.py Drone Interface.py: https://github.com/aerostack2/aerostack2/blob/main/as2_python_api/as2_python_api/drone_interface.py \"\"\" def __init__(self, name, verbose=False, use_sim_time=False): super().__init__(name, verbose, use_sim_time) # ROS2 create a subscription to the raw image of the sensors. # This details the ros message type (Image), the name of the topic # And the function that should be called when a message is received on this topic self.create_subscription(Image, \"sensor_measurements/hd_camera/image_raw\", self.img_callback, qos_profile_sensor_data) # CV Bridge is a set of functions to convert to and from ROS images to Opencv images self.br = CvBridge() def img_callback(self, data): \"\"\"Image Callback Function The image message is defined here: https://github.com/ros2/common_interfaces/blob/rolling/sensor_msgs/msg/Image.msg Args: data (sensor_msgs.msg.Image): The received image message \"\"\" self.get_logger().info('Receiving video frame', once=True) # Log Once # Convert the image message to a Opencv image frame current_frame = self.br.imgmsg_to_cv2(data) # Show the frame in a window cv2.imshow(\"camera\", current_frame) cv2.waitKey(1) # Wait a millisecond def run_test(self): \"\"\" Run the mission \"\"\" # Set the drone to offboard mode. This prepares the drone to receive # commands from outside of the flight controller. self.offboard() self.get_logger().info(\"Offboard Mode\") # Arming the drone powers up the motors to prepare for flight self.arm() self.get_logger().info(\"Armed!\") # Takeoff to 1 meter self.get_logger().info(\"Taking Off!\") res = self.takeoff(height=1.0, speed=0.5) if res: self.get_logger().info(\"Take off complete\") else: self.get_logger().info(\"Take off Failed, exiting\") return # Wait a little bit time.sleep(1.0) # Position Control fly around a bit speed = 1.5 self.go_to.go_to_point([1, 0, 1.0], speed=speed) self.get_logger().info(\"Point 1\") self.go_to.go_to_point([2, 0, 2.0], speed=speed) self.get_logger().info(\"Point 2\") self.go_to.go_to_point([3, 0, 3.0], speed=speed) self.get_logger().info(\"Point 3\") self.go_to.go_to(3.0, -1.0, 2.5, speed=speed) self.get_logger().info(\"Point 4\") self.go_to.go_to_point_with_yaw([4, 1, 3.0], angle=45.0, speed=speed) self.get_logger().info(\"Point 5\") self.go_to.go_to_point_with_yaw([3, -2, 2.0], angle=-45.0, speed=speed) self.get_logger().info(\"Point 6\") self.go_to.go_to_point_with_yaw([0, 0, 1.0], angle=0.0, speed=speed) self.get_logger().info(\"Point 7\") self.land() ############# Running the mission and Entrypoint ################################# if __name__ == '__main__': parser = argparse.ArgumentParser( description=\"Starts camera mission\") parser.add_argument('-s', '--simulated', action='store_true', default=False) parser.add_argument('-n', '--drone_name', default=\"cf0\") args = parser.parse_args() if args.simulated: print(\"Mission running in simulation mode\") else: print(\"Mission running in real mode\") # Starts ROS2 Node in a script rclpy.init() # Create the drone object. Connects to the real/simulated drone and runs tests uav = DroneMotionRef(args.drone_name, verbose=True) # Runs the UAV TEST function uav.run_test() # Shuts down the UAV uav.shutdown() # Stop ROS2 Node rclpy.shutdown() print(\"Clean exit\") exit(0) Modifying the controller \u00b6 Now try and modify this controller by making it go to different places at different speeds. This would be a good time to try an automate any computer vision or detection algorithms inside this python script. Finally, remind yourself of the goal of this mini-challenge, and try to implement a solution! In this simulation mini-challenge, we have setup a small project in which there are a number of aruco codes in a line in front of the drone. There are only two types of aruco code arranged such that for n codes in a line, the first n-1 will be of type 1, and the nth will be type 2. Your task is to write an algorithm which can detect aruco markers from the drones camera, and fly forward until you reach the type 2 code, and land on it. Note: Pay attention to the grid sizes of the ArUco markers and the maximum ID number that the detection algorithm can handle. Check the ID numbers of the simulated markers (found in the model file in the src folder) and determine your ArUco dictionary setup strategy accordingly. Here is a quick tutorial on ArUco markers on OpenCV: https://docs.opencv.org/4.x/d5/dae/tutorial_aruco_detection.html. Note: you likely do not need to restart the entire simulation every time you test your script (unless you crash out or put the drone/system into an unrecoverable state). Simply make your change and re-run the script! (Although you may want a script to send the drone back to the starting pose!) Tasks \u00b6 Install project gazebo aruco as instructed Whats the differences between this project and project gazebo? Run Project Gazebo with the teleoperation panel and rviz2 for the camera feed Where is the camera feed coming from? What is the purpose of rviz2 and what does it show you? Run Project Gazebo with the example mission camera script How does the script get the camera data? Implement and test and aruco detection method using OpenCV What version of aruco marker are we using? With the aruco detection, implement a controller which can solve the mini-challenge Further Tasks: The generated world is defined in sim_config/world.json , with models defined in the models directory. Have a play around with the world What other tasks could you do with your aruco detection module?","title":"Gazebo Aruco Mini-Challenge"},{"location":"2_practical_gazebo_aruco/#practical-2-ros2-with-aerostack2","text":"Practical 2: ROS2 with Aerostack2 Mini-Challenge Aerostack2 Installation Local Installation Setup and build Aerostack2 (we use version 1.1.2) Setup this project Docker Running the simulator What does the simulator run A quick aside on Tmux Controlling with teleoperation Running the example autonomous mission Stopping the simulator Dissecting the controller Modifying the controller Tasks NOTE: If you run into any issues, create an issue on the github repository: project_gazebo_aruco","title":"Practical 2: ROS2 with Aerostack2"},{"location":"2_practical_gazebo_aruco/#mini-challenge","text":"In this simulation mini-challenge, we have setup a small project in which there are a number of aruco codes in a line in front of the drone. There are only two types of aruco code arranged such that for n codes in a line, the first n-1 will be of type 1, and the nth will be type 2. Your task is to write an algorithm which can detect aruco markers from the drones camera, and fly forward until you reach the type 2 code, and land on it. In order to do this, you must be able to first have installed aerostack2 and the project code. You will then need to have understood how the mission script works and how to subscribe and process the drone stream. Finally you will need to work out how to control the drone from the drone stream.","title":"Mini-Challenge"},{"location":"2_practical_gazebo_aruco/#aerostack2-installation","text":"No matter how you run the linux system, you can try each of the following two options (Local or Docker).","title":"Aerostack2 Installation"},{"location":"2_practical_gazebo_aruco/#local-installation","text":"","title":"Local Installation"},{"location":"2_practical_gazebo_aruco/#setup-and-build-aerostack2-we-use-version-112","text":"In your home directory (could be anywhere else but all the paths below are for your home directory) mkdir -p ~/project_gazebo_ws/src cd ~/project_gazebo_ws/src git clone https://github.com/UCL-MSC-RAI-COMP0240/aerostack2.git Make sure you clone our UCL-MSC-RAI-COMP0240 fork of aerostack2 - a fork is a fancy name for a copy of the original repository but still remains connected - such that if the original developers ever update their version, we can decide to also pull in the new updates. But it remains separate such that we can also add our own updates for you. This will create a ros2 workspace and place the aerostack2 repository in it. You can see that aerostack2 itself is made up of many different ros nodes and packages. You will need to install the dependencies by running the following: sudo apt install git python3-rosdep python3-pip python3-colcon-common-extensions tmux tmuxinator -y pip3 install pysimplegui-4-foss And then going back into the root workspace to install the remaining ROS2 dependencies automatically. cd ~/project_gazebo_ws sudo rosdep init rosdep update rosdep install -y -r -q --from-paths src --ignore-src Then, enable the handy aerostack2 cli (only run this once) echo 'export AEROSTACK2_PATH=$HOME/project_gazebo_ws/src/aerostack2' >> $HOME/.bashrc echo 'source $AEROSTACK2_PATH/as2_cli/setup_env.bash' >> $HOME/.bashrc source ~/.bashrc This will enable you to build the project from any folder using It will take a couple of minutes as it builds all of aerostack2! as2 build Now as2 should be installed. Finally add the following to your bashrc echo 'source $HOME/project_gazebo_ws/install/setup.bash' >> $HOME/.bashrc source ~/.bashrc","title":"Setup and build Aerostack2 (we use version 1.1.2)"},{"location":"2_practical_gazebo_aruco/#setup-this-project","text":"Get this project locally mkdir -p ~/project_gazebo_ws/src cd ~/project_gazebo_ws/src git clone https://github.com/UCL-MSC-RAI-COMP0240/project_gazebo_aruco.git Run the example using cd ~/project_gazebo_ws/src/project_gazebo_aruco ./launch_as2.bash -s -t","title":"Setup this project"},{"location":"2_practical_gazebo_aruco/#docker","text":"If you have locally installed this project already, you can elect not to do this section. Otherwise you could give it a go if you usually run a different version of ubuntu. Ensure Docker or Docker Desktop is installed on your machine First your will need to clone this project somewhere (doesn't need to be in a ros2 workspace) git clone https://github.com/UCL-MSC-RAI-COMP0240/project_gazebo_aruco.git To build and/or run the container run the script This container is based on Ubuntu 22.04, ROS2 Humble and Ignition Gazebo Fortress ./docker/docker_start.bash After building for a while, this will drop you inside the docker container. The container will have live mounted this project into /ros2/project_gazebo_aruco so that any changes made to this repository outside of the container will be reflected inside. Inside the container, navigate to that repository and run the example. cd /ros2/project_gazebo_aruco ./launch_as2.bash -s -t Note that you can utilise a GPU if you install the nvidia-container-toolkit . Pass the -nvidia argument to docker_start.bash","title":"Docker"},{"location":"2_practical_gazebo_aruco/#running-the-simulator","text":"In your selected environment, start the aerostack2 simulation environment cd ~/project_gazebo_ws/src/project_gazebo_aruco # or docker: cd /ros2/project_gazebo_aruco ./launch_as2.bash -s -t -v The launch_as2.bash file runs a bash script which controls what elements are also started up. The -s indicates that the system should be launched in simulation mode. The -t opens up the teleoperation remote control window to control the drone. The -v opens up a visualisation software called rviz2. You can have a look at -h for all of the different options, some won't be relevant for this task.","title":"Running the simulator"},{"location":"2_practical_gazebo_aruco/#what-does-the-simulator-run","text":"The aerostack simulator runs a number of different modules for ensuring successful flight this includes: Platform Interface State Estimation Motion Controllers Behaviour Controllers (Takeoff, Land, Go-To) You can see the status of these other modules if you select the terminal and press Ctrl + B and then press 0 ... 5 to change windows. In a single window you can switch between panes by pressing Ctrl + B and then a direction arrow.","title":"What does the simulator run"},{"location":"2_practical_gazebo_aruco/#a-quick-aside-on-tmux","text":"This fancy terminal environment you find yourself in is known as tmux . Tmux is a way for to allow terminal users to run and view multiple programs at the same time - in the case for us, multiple ROS2 nodes doing different things. Here they are organised in windows and panes. I would recommend reading through a guide such as the following, and just trying it out! https://hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/ The useful feature for us is that you can define configuration files for running a bunch of different programs - especially useful for complex systems with multiple components to spin up, but also observe (observation being one of the main issues with ros launch files).","title":"A quick aside on Tmux"},{"location":"2_practical_gazebo_aruco/#controlling-with-teleoperation","text":"Using the -t option will open up the teleoperation panel, allowing you to attempt to manually fly the robot in simulation. With the teleoperation panel clicked on and selected, you can press the T key to takeoff. The arrow keys will then control the direction of flight of the drone in position control mode (you tell the drone where specifically to go). Notice how the drone flies around in the simulation and in rviz. Also see how the image within rviz changes! The image is from a bottom mounted camera on the simulated drone, mirroring the setup you will be building with the real drone. Play around a fly the drone around!","title":"Controlling with teleoperation"},{"location":"2_practical_gazebo_aruco/#running-the-example-autonomous-mission","text":"In the original terminal (Use Ctrl + B + 5 ), we can run any scripts we want. The mission_*.py scripts are some examples of these autonomous missions. To run one of these examples, in the terminal type: python3 mission_camera.py This example mission gives you a camera stream from the drone, takes off and arms the drone and flies it around using a few different methods. Key points is the use of ROS2 to subscribe to the camera topic published by the drone model. Whenever an image is received, it will run the img_callback function.","title":"Running the example autonomous mission"},{"location":"2_practical_gazebo_aruco/#stopping-the-simulator","text":"In order to stop the simulator cleanly, in any terminal run the ./stop.bash script. ./stop.bash This will stop all containers and relevant programs to the simulator in a clean manner. Sometimes the simulated drone will go into an unrecoverable state - you may need to resart the simulator.","title":"Stopping the simulator"},{"location":"2_practical_gazebo_aruco/#dissecting-the-controller","text":"Here is the example controller. It is documented! #!/bin/python3 \"\"\" CAMERA SAMPLE MISSION This file is an example mission which reads from the aerostack drone camera and prints it to screen It also flies around using position and velocity control camera topic \"\"\" # Imports import time import rclpy import argparse from as2_python_api.drone_interface import DroneInterface from rclpy.qos import qos_profile_sensor_data from sensor_msgs.msg import Image, CameraInfo from cv_bridge import CvBridge import cv2 ######## Drone Control Class ################### class DroneMotionRef(DroneInterface): \"\"\"Drone Interface This is the aerostack2 drone interface for connecting to simulated and real drones. It runs as a ROS2 Node which interacts with the currently available ROS2 topics. It defines the variables that represent a single drone, i.e. - Platform Information - Vehicle Pose and Twist (angular velocity) - Functions to control the hardware of the drone (arm, disarm, change mode, estop) It also contains some modules for flying the drone, this includes: - Takeoff, Landing (self.takeoff, self.land) - GoTo position control (self.go_to) [https://github.com/aerostack2/aerostack2/blob/main/as2_python_api/as2_python_api/modules/go_to_module.py] - FollowPath module (self.follow_path) [https://github.com/aerostack2/aerostack2/blob/main/as2_python_api/as2_python_api/modules/follow_path_module.py] Other module exist which could be used to. Their interfaces and functions can be referenced most easily in the code. Some Documentation is here: https://aerostack2.github.io/_09_development/_api_documentation/temp_ws/src/as2_python_api/docs/source/as2_python_api.html The Source Code is here: https://github.com/aerostack2/aerostack2/tree/main/as2_python_api Drone Interface Base.py: https://github.com/aerostack2/aerostack2/blob/main/as2_python_api/as2_python_api/drone_interface_base.py Drone Interface.py: https://github.com/aerostack2/aerostack2/blob/main/as2_python_api/as2_python_api/drone_interface.py \"\"\" def __init__(self, name, verbose=False, use_sim_time=False): super().__init__(name, verbose, use_sim_time) # ROS2 create a subscription to the raw image of the sensors. # This details the ros message type (Image), the name of the topic # And the function that should be called when a message is received on this topic self.create_subscription(Image, \"sensor_measurements/hd_camera/image_raw\", self.img_callback, qos_profile_sensor_data) # CV Bridge is a set of functions to convert to and from ROS images to Opencv images self.br = CvBridge() def img_callback(self, data): \"\"\"Image Callback Function The image message is defined here: https://github.com/ros2/common_interfaces/blob/rolling/sensor_msgs/msg/Image.msg Args: data (sensor_msgs.msg.Image): The received image message \"\"\" self.get_logger().info('Receiving video frame', once=True) # Log Once # Convert the image message to a Opencv image frame current_frame = self.br.imgmsg_to_cv2(data) # Show the frame in a window cv2.imshow(\"camera\", current_frame) cv2.waitKey(1) # Wait a millisecond def run_test(self): \"\"\" Run the mission \"\"\" # Set the drone to offboard mode. This prepares the drone to receive # commands from outside of the flight controller. self.offboard() self.get_logger().info(\"Offboard Mode\") # Arming the drone powers up the motors to prepare for flight self.arm() self.get_logger().info(\"Armed!\") # Takeoff to 1 meter self.get_logger().info(\"Taking Off!\") res = self.takeoff(height=1.0, speed=0.5) if res: self.get_logger().info(\"Take off complete\") else: self.get_logger().info(\"Take off Failed, exiting\") return # Wait a little bit time.sleep(1.0) # Position Control fly around a bit speed = 1.5 self.go_to.go_to_point([1, 0, 1.0], speed=speed) self.get_logger().info(\"Point 1\") self.go_to.go_to_point([2, 0, 2.0], speed=speed) self.get_logger().info(\"Point 2\") self.go_to.go_to_point([3, 0, 3.0], speed=speed) self.get_logger().info(\"Point 3\") self.go_to.go_to(3.0, -1.0, 2.5, speed=speed) self.get_logger().info(\"Point 4\") self.go_to.go_to_point_with_yaw([4, 1, 3.0], angle=45.0, speed=speed) self.get_logger().info(\"Point 5\") self.go_to.go_to_point_with_yaw([3, -2, 2.0], angle=-45.0, speed=speed) self.get_logger().info(\"Point 6\") self.go_to.go_to_point_with_yaw([0, 0, 1.0], angle=0.0, speed=speed) self.get_logger().info(\"Point 7\") self.land() ############# Running the mission and Entrypoint ################################# if __name__ == '__main__': parser = argparse.ArgumentParser( description=\"Starts camera mission\") parser.add_argument('-s', '--simulated', action='store_true', default=False) parser.add_argument('-n', '--drone_name', default=\"cf0\") args = parser.parse_args() if args.simulated: print(\"Mission running in simulation mode\") else: print(\"Mission running in real mode\") # Starts ROS2 Node in a script rclpy.init() # Create the drone object. Connects to the real/simulated drone and runs tests uav = DroneMotionRef(args.drone_name, verbose=True) # Runs the UAV TEST function uav.run_test() # Shuts down the UAV uav.shutdown() # Stop ROS2 Node rclpy.shutdown() print(\"Clean exit\") exit(0)","title":"Dissecting the controller"},{"location":"2_practical_gazebo_aruco/#modifying-the-controller","text":"Now try and modify this controller by making it go to different places at different speeds. This would be a good time to try an automate any computer vision or detection algorithms inside this python script. Finally, remind yourself of the goal of this mini-challenge, and try to implement a solution! In this simulation mini-challenge, we have setup a small project in which there are a number of aruco codes in a line in front of the drone. There are only two types of aruco code arranged such that for n codes in a line, the first n-1 will be of type 1, and the nth will be type 2. Your task is to write an algorithm which can detect aruco markers from the drones camera, and fly forward until you reach the type 2 code, and land on it. Note: Pay attention to the grid sizes of the ArUco markers and the maximum ID number that the detection algorithm can handle. Check the ID numbers of the simulated markers (found in the model file in the src folder) and determine your ArUco dictionary setup strategy accordingly. Here is a quick tutorial on ArUco markers on OpenCV: https://docs.opencv.org/4.x/d5/dae/tutorial_aruco_detection.html. Note: you likely do not need to restart the entire simulation every time you test your script (unless you crash out or put the drone/system into an unrecoverable state). Simply make your change and re-run the script! (Although you may want a script to send the drone back to the starting pose!)","title":"Modifying the controller"},{"location":"2_practical_gazebo_aruco/#tasks","text":"Install project gazebo aruco as instructed Whats the differences between this project and project gazebo? Run Project Gazebo with the teleoperation panel and rviz2 for the camera feed Where is the camera feed coming from? What is the purpose of rviz2 and what does it show you? Run Project Gazebo with the example mission camera script How does the script get the camera data? Implement and test and aruco detection method using OpenCV What version of aruco marker are we using? With the aruco detection, implement a controller which can solve the mini-challenge Further Tasks: The generated world is defined in sim_config/world.json , with models defined in the models directory. Have a play around with the world What other tasks could you do with your aruco detection module?","title":"Tasks"},{"location":"3_challenge_cw1/","text":"Coursework Challenge 1: Structural Inspection Path Planning \u00b6 NOTE: If you run into any issues, create an issue on the github repository: challenge_mission_planning Challenge \u00b6 You have the challenge of solving the problem of finding the most optimal way of visiting a series of locations and taking pictures of markers, while avoiding the obstacles in the environment. This mirrors real life autonomous inspection scenarios which are the majority of current day use cases with autonomous drones. Structural Inspection Path Planning \u00b6 The structural inspection path planning problem is often broken down into three different problems 3D Viewpoint generation, in which a 2.5D or CAD model of the structure is used to determine the optimal set of camera positions which need to be visited in order to generate enough data for a reconstruction. [This Challenge] Coverage Path Planning / Tour Optimisation & Trajectory Generation, then tries to find the optimal ordering of viewpoints to visit, potentially including the optimal trajectory itself. This often attempts to minimise time, travel distance, energy or other factors. Trajectory Following & Control is then the lower level controller which attempts to follow the calculated optimal coverage path as close as possible in order to capture the images in a location as close as specified. For further information on structural inspection path planning, see this article Your Challenge \u00b6 In this challenge we focus on the second step of finding the optimal route around a set of viewpoints. You will be given a scenario which specifies: The drone starting location The position and orientation (pose) of each of the viewpoints which you are being requested to visit The location of cuboid shaped obstacles to avoid A set of scenarios have been given within the scenarios folder for you to use for development and testing. Your challenge will be in implementing and comparing a set of methods for controlling a drone to fly around the viewpoints, verifying that all of the specified aruco targets have been seen and visited, whilst avoiding the obstacles. You will need to consider the following: Optimal routing and finding a solution to the path planning problem Motion planning and how to route from one viewpoint to another to avoid collisions while preserving energy Logging to ensure that your solution has collected \"data\" and visited every target to proove completion to a potential client. Calculation of relevant metrics (time, distance, speed, etc) for comparison of various methods. Interesting things to look up to get you started \u00b6 Travelling Salesman Problem Combinatorial Optimisation Complete routing graph of all the connectivity of the viewpoints Consider the value of the weight on each edge on the routing graph Dubins Paths Visualise your paths, maps and routes Python Libraries: scipy numpy python_tsp networkx matplotlib Challenge Environment \u00b6 Just as with the previous gazebo aruco mini-challenge, this will also be within aerostack2 and ros2 and will hopefully build upon the scripts and knowledge you have gained through doing that task. However, we have created a new repository specifically for this application. The repository is here: challenge_mission_planning Installation \u00b6 For this project, we assume that you are already familiar with the installation process from the gazebo aruco mini-challenge. Follow the README.md in the challenge repository for installation instructions Running the environment \u00b6 The README.md goes into full details of each command, but essentially you will need two terminals, both in the root of the repository: In terminal 1 run the following to launch the drone simulation of scenario1, and you will see gazebo popup. ./launch_as2.bash -s scenarios/scenario1.yaml In this tmux, I have enabled the mouse so you should be able to scroll and such (still havent figured out copy paste though...) And in a second terminal, you can run the ground station, which will spin up the visualisation software rviz2 as well as provide a prompt for you to run your mission (Though you can run your python mission from everywhere, provided you source /mission_planning_ws/install/setup.bash first) ./launch_ground_station.bash # or if you want to play around with teleoperating the drone ./launch_ground_station.bash -t Remember : If you want to close the simulation down, just run the stop.bash script from the repository root in any terminal or tmux window. Building your solution \u00b6 We have provided a very basic sample solution in mission_scenario.py which simply reads the scenario file, iterates through the viewpoints and visits them one by one. # From the root of this repository python3 mission_scenario.py -s scenarios/scenario1.yaml The same camera control script from the previous mini-challenge is also available at mission_camera.py Your challenge will be to build upon this script in any way you see fit to provide an efficient solution to the problem. You have free reign to build your own libraries, use existing python libraries and create solutions which can provably solve your problem. In order to validate and verify your solution, you will need to add logging and metrics to enable comparison and proof of the effiency of your proposed solution. Finally, imagine that you are selling this as a product to a customer, you will need to provide some sort of guarantee that the task - visiting and photographing all targets - has been completed. We have provided 3 sample scenarios in the scenarios directory with which you can test your proposed solutions on: scenario1.yaml: A low number mix of targets and obstacles scenario2.yaml: A large number of targets only scenario3.yaml: A small number of targets but many obstacles scenario4.yaml: A large number of targets and many obstacles! Note: The utils generate_scenario.py script can generate more scenarios for you if you wish. Recommended Tasks \u00b6 Install this repository and run the example code What is this mission file doing? Does it work for other scenarios? Write down on a piece of paper, the various parts that you might need to implement What functionalities does you system need What data structures do you need to enable those functionalities Implement some sort of internal map representation to enable planning What sort of map should this be? What functionalities does a map enable? Find or implement one simple path planning system e.g. choose either A*, Dijkstra, Dubins or RRT to implement. What will the results of the path planning be used for? Which routes do we need to apply the path planning for? You only need to implement one path planning system of your choice. Find a library which will help solve the travelling salesman problem (TSP) Why do we need to solve the TSP? What data representation does the TSP need? What are the weights going to be for solving the TSP Using the aruco detection from the previous mini-challenge, implement a method of checking which points you have visited, and whether the task is complete? Implement timing and the calculation of metrics What metrics are best for comparing solutions to this problem? What methods could you use to improve my solution? How could you make it faster? What are the drawbacks of your solution? Are there any edge cases? Coursework Submission \u00b6 The deadline for this coursework is 17.00 on 7th March 2025. Submissions will uploaded into Moodle using the relevant Coursework 1 submission links. The objective of this coursework is to complete the technical elements of the challenge and then write up a report describing the performance of your approach to solving the TSP problem. You will need to submit 3 items: - Coursework 1 report (1250 words): This report should be structured to clearly explain the problem, methodology, implementation, results and conclusions. - 4min - Video: Demonstration of optimal path-planning solution for autonomous drone-based inspection, showcasing waypoint opitmisation, collision free motion planning and performance analysis through a simulated Gazebo environment. - Code submission Assessment Breakdown and Structure: \u00b6 Below are set of expectations that you need to address when making your submission. 1) Report (60% of total grade): Please use the following section headings to structure your report. Problem Statement and Objectives (10%) Define, in your own words, the structural inspection path planning challenge, outlining the need for an efficient drone navigation strategy to visit all viewpoints while avoiding obstacles, and specifying the key objectives. Methodology (40%) Detail the approach used to solve the path-planning problem, including the selection and implementation of algorithms for waypoint sequencing (TSP solver), motion planning (e.g., A*, Dijkstra, RRT and Dubins paths), and obstacle avoidance, while also explaining how these methods were integrated into the Gazebo simulation environment to execute the drone\u2019s mission Results and Discussion (40%) Present key performance metrics, such as total mission time, flight distance, and mission success rate, comparing different planning approaches where applicable, while also analysing the effectiveness, limitations, and potential improvements of the implemented solution. This should be done for each (or as many as your can achieve) of the 4 test scenarios (scenario1.yaml, scenario2.yaml, scenario3.yaml and scenario4.yaml). Conclusions (10%) Summarise the key findings of the project, highlighting the effectiveness of the implemented path-planning approach, its impact on optimizing drone-based structural inspections, and potential future enhancements to improve efficiency, adaptability, or real-world applicability. Please use an Arial typeface with font size 11. Use a standard document layout with 2cm margins all around. For reports that have a word count 10% over the 1250 word limit a penalty maybe applied. Items not included in the word count are: tables, figures, appendix, references and code. 2) Video Demonstration (40% of total grade) - Description of work and clarity (20%) Provide a concise and well-structured narration, clearly describing the problem, the implemented approach, and how the solution addresses the path-planning challenge, ensuring that viewers can easily understand the methodology and its significance. Successful runs (60%) Demonstrate the drone autonomously navigating the environment, visiting all required viewpoints, avoiding obstacles, and verifying mission completion by capturing ArUco marker images, providing clear visual proof of the solution\u2019s effectiveness. Basic Performance Analysis (20%) Briefly present key metrics such as total mission time, flight distance, and mission success rate, offering a quick evaluation of the system\u2019s efficiency while avoiding deep technical justifications to maintain a clear presentation.","title":"Structural Inspection Path Planning Challenge"},{"location":"3_challenge_cw1/#coursework-challenge-1-structural-inspection-path-planning","text":"NOTE: If you run into any issues, create an issue on the github repository: challenge_mission_planning","title":"Coursework Challenge 1: Structural Inspection Path Planning"},{"location":"3_challenge_cw1/#challenge","text":"You have the challenge of solving the problem of finding the most optimal way of visiting a series of locations and taking pictures of markers, while avoiding the obstacles in the environment. This mirrors real life autonomous inspection scenarios which are the majority of current day use cases with autonomous drones.","title":"Challenge"},{"location":"3_challenge_cw1/#structural-inspection-path-planning","text":"The structural inspection path planning problem is often broken down into three different problems 3D Viewpoint generation, in which a 2.5D or CAD model of the structure is used to determine the optimal set of camera positions which need to be visited in order to generate enough data for a reconstruction. [This Challenge] Coverage Path Planning / Tour Optimisation & Trajectory Generation, then tries to find the optimal ordering of viewpoints to visit, potentially including the optimal trajectory itself. This often attempts to minimise time, travel distance, energy or other factors. Trajectory Following & Control is then the lower level controller which attempts to follow the calculated optimal coverage path as close as possible in order to capture the images in a location as close as specified. For further information on structural inspection path planning, see this article","title":"Structural Inspection Path Planning"},{"location":"3_challenge_cw1/#your-challenge","text":"In this challenge we focus on the second step of finding the optimal route around a set of viewpoints. You will be given a scenario which specifies: The drone starting location The position and orientation (pose) of each of the viewpoints which you are being requested to visit The location of cuboid shaped obstacles to avoid A set of scenarios have been given within the scenarios folder for you to use for development and testing. Your challenge will be in implementing and comparing a set of methods for controlling a drone to fly around the viewpoints, verifying that all of the specified aruco targets have been seen and visited, whilst avoiding the obstacles. You will need to consider the following: Optimal routing and finding a solution to the path planning problem Motion planning and how to route from one viewpoint to another to avoid collisions while preserving energy Logging to ensure that your solution has collected \"data\" and visited every target to proove completion to a potential client. Calculation of relevant metrics (time, distance, speed, etc) for comparison of various methods.","title":"Your Challenge"},{"location":"3_challenge_cw1/#interesting-things-to-look-up-to-get-you-started","text":"Travelling Salesman Problem Combinatorial Optimisation Complete routing graph of all the connectivity of the viewpoints Consider the value of the weight on each edge on the routing graph Dubins Paths Visualise your paths, maps and routes Python Libraries: scipy numpy python_tsp networkx matplotlib","title":"Interesting things to look up to get you started"},{"location":"3_challenge_cw1/#challenge-environment","text":"Just as with the previous gazebo aruco mini-challenge, this will also be within aerostack2 and ros2 and will hopefully build upon the scripts and knowledge you have gained through doing that task. However, we have created a new repository specifically for this application. The repository is here: challenge_mission_planning","title":"Challenge Environment"},{"location":"3_challenge_cw1/#installation","text":"For this project, we assume that you are already familiar with the installation process from the gazebo aruco mini-challenge. Follow the README.md in the challenge repository for installation instructions","title":"Installation"},{"location":"3_challenge_cw1/#running-the-environment","text":"The README.md goes into full details of each command, but essentially you will need two terminals, both in the root of the repository: In terminal 1 run the following to launch the drone simulation of scenario1, and you will see gazebo popup. ./launch_as2.bash -s scenarios/scenario1.yaml In this tmux, I have enabled the mouse so you should be able to scroll and such (still havent figured out copy paste though...) And in a second terminal, you can run the ground station, which will spin up the visualisation software rviz2 as well as provide a prompt for you to run your mission (Though you can run your python mission from everywhere, provided you source /mission_planning_ws/install/setup.bash first) ./launch_ground_station.bash # or if you want to play around with teleoperating the drone ./launch_ground_station.bash -t Remember : If you want to close the simulation down, just run the stop.bash script from the repository root in any terminal or tmux window.","title":"Running the environment"},{"location":"3_challenge_cw1/#building-your-solution","text":"We have provided a very basic sample solution in mission_scenario.py which simply reads the scenario file, iterates through the viewpoints and visits them one by one. # From the root of this repository python3 mission_scenario.py -s scenarios/scenario1.yaml The same camera control script from the previous mini-challenge is also available at mission_camera.py Your challenge will be to build upon this script in any way you see fit to provide an efficient solution to the problem. You have free reign to build your own libraries, use existing python libraries and create solutions which can provably solve your problem. In order to validate and verify your solution, you will need to add logging and metrics to enable comparison and proof of the effiency of your proposed solution. Finally, imagine that you are selling this as a product to a customer, you will need to provide some sort of guarantee that the task - visiting and photographing all targets - has been completed. We have provided 3 sample scenarios in the scenarios directory with which you can test your proposed solutions on: scenario1.yaml: A low number mix of targets and obstacles scenario2.yaml: A large number of targets only scenario3.yaml: A small number of targets but many obstacles scenario4.yaml: A large number of targets and many obstacles! Note: The utils generate_scenario.py script can generate more scenarios for you if you wish.","title":"Building your solution"},{"location":"3_challenge_cw1/#recommended-tasks","text":"Install this repository and run the example code What is this mission file doing? Does it work for other scenarios? Write down on a piece of paper, the various parts that you might need to implement What functionalities does you system need What data structures do you need to enable those functionalities Implement some sort of internal map representation to enable planning What sort of map should this be? What functionalities does a map enable? Find or implement one simple path planning system e.g. choose either A*, Dijkstra, Dubins or RRT to implement. What will the results of the path planning be used for? Which routes do we need to apply the path planning for? You only need to implement one path planning system of your choice. Find a library which will help solve the travelling salesman problem (TSP) Why do we need to solve the TSP? What data representation does the TSP need? What are the weights going to be for solving the TSP Using the aruco detection from the previous mini-challenge, implement a method of checking which points you have visited, and whether the task is complete? Implement timing and the calculation of metrics What metrics are best for comparing solutions to this problem? What methods could you use to improve my solution? How could you make it faster? What are the drawbacks of your solution? Are there any edge cases?","title":"Recommended Tasks"},{"location":"3_challenge_cw1/#coursework-submission","text":"The deadline for this coursework is 17.00 on 7th March 2025. Submissions will uploaded into Moodle using the relevant Coursework 1 submission links. The objective of this coursework is to complete the technical elements of the challenge and then write up a report describing the performance of your approach to solving the TSP problem. You will need to submit 3 items: - Coursework 1 report (1250 words): This report should be structured to clearly explain the problem, methodology, implementation, results and conclusions. - 4min - Video: Demonstration of optimal path-planning solution for autonomous drone-based inspection, showcasing waypoint opitmisation, collision free motion planning and performance analysis through a simulated Gazebo environment. - Code submission","title":"Coursework Submission"},{"location":"3_challenge_cw1/#assessment-breakdown-and-structure","text":"Below are set of expectations that you need to address when making your submission. 1) Report (60% of total grade): Please use the following section headings to structure your report. Problem Statement and Objectives (10%) Define, in your own words, the structural inspection path planning challenge, outlining the need for an efficient drone navigation strategy to visit all viewpoints while avoiding obstacles, and specifying the key objectives. Methodology (40%) Detail the approach used to solve the path-planning problem, including the selection and implementation of algorithms for waypoint sequencing (TSP solver), motion planning (e.g., A*, Dijkstra, RRT and Dubins paths), and obstacle avoidance, while also explaining how these methods were integrated into the Gazebo simulation environment to execute the drone\u2019s mission Results and Discussion (40%) Present key performance metrics, such as total mission time, flight distance, and mission success rate, comparing different planning approaches where applicable, while also analysing the effectiveness, limitations, and potential improvements of the implemented solution. This should be done for each (or as many as your can achieve) of the 4 test scenarios (scenario1.yaml, scenario2.yaml, scenario3.yaml and scenario4.yaml). Conclusions (10%) Summarise the key findings of the project, highlighting the effectiveness of the implemented path-planning approach, its impact on optimizing drone-based structural inspections, and potential future enhancements to improve efficiency, adaptability, or real-world applicability. Please use an Arial typeface with font size 11. Use a standard document layout with 2cm margins all around. For reports that have a word count 10% over the 1250 word limit a penalty maybe applied. Items not included in the word count are: tables, figures, appendix, references and code. 2) Video Demonstration (40% of total grade) - Description of work and clarity (20%) Provide a concise and well-structured narration, clearly describing the problem, the implemented approach, and how the solution addresses the path-planning challenge, ensuring that viewers can easily understand the methodology and its significance. Successful runs (60%) Demonstrate the drone autonomously navigating the environment, visiting all required viewpoints, avoiding obstacles, and verifying mission completion by capturing ArUco marker images, providing clear visual proof of the solution\u2019s effectiveness. Basic Performance Analysis (20%) Briefly present key metrics such as total mission time, flight distance, and mission success rate, offering a quick evaluation of the system\u2019s efficiency while avoiding deep technical justifications to maintain a clear presentation.","title":"Assessment Breakdown and Structure:"},{"location":"3a_practical_crazyflie/","text":"Practical 3 & 4: Building and flying Crazyflies \u00b6 In this third and fourth practical we are jumping in and getting hands on with flying crazyflies - manually and hopefully if we're lucky autonomously doing simple things. Practical 3 & 4: Building and flying Crazyflies Crazyflies What are Crazyflies Why are we using them Safety Personal Safety Battery Safety Practical Tasks Preliminary Tasks Main Tasks Crazyflies \u00b6 What are Crazyflies \u00b6 The Crazyflie 2.1+. is a versatile open source micro-drone flying development platform that only weighs 27g and fits in the palm of your hand. Checkout their website to learn more about them - https://www.bitcraze.io/ You will see that they are used for R&D activities all over as its a perfect platform for learning, researching and experimenting with all things drones and drone swarms in a lab setting. Why are we using them \u00b6 Inspired by aerial robotics courses from other institutions, we utilise the crazyflie as an initial teaching platform due to: Ease of use Size & Safety Programmability Large Community Through using them, you will learn the basics of constructing a drone platform and how the various physical components fit together. This includes the drone itself but also how you might communicate with a drone through the radio. The crazyflie comes with a nice cross-platform user interface for connecting to, configuring and monitoring a drone. This interface allows you to connect a game pad for manual flight of the crazyflie, but also there is an andriod and IOS app for manual flying of the drone over bluetooth. Bitcraze and various research groups have released a lot of open source, freely available code for programming the drone in python and c++, with many examples. This allows you to more easily implement and experiment with different methods of autonomy for the drone. Finally due to the small size, there is very little chance of injury ( PROVIDING YOU ARE WEARING YOUR SAFETY GLASSES ) when flying, even if you do collide with something or someone. This makes it ideal for use during a tutorial with many people in a classroom setting. As you will see later, flying with anything larger will require flying within our dedicated flying arena. Safety \u00b6 Personal Safety \u00b6 WEAR YOUR SAFETY GLASSES AT ALL TIMES DURING THIS PRACTICAL It is recommended to wear long sleves and trousers When the drone is flying, it should always be watched Battery Safety \u00b6 The Drones use small Lithium-Polymer batteries and as such we must be careful DO NOT DROP, PIERCE, HAMMER OR OTHERWISE DAMAGE THE BATTERIES IF A BATTERY LOOKS DAMAGED DO NOT USE IT AND TELL A STAFF MEMBER IF A BATTERY STARTS SMOKING OR IS ON FIRE TELL THE STAFF AND CALMLY MOVE AWAY FROM IT We will have a metal sand bucket and long tongs and the battery will go in there DO NOT ATTEMPT TO PUT OUT THE FIRE YOURSELF Practical Tasks \u00b6 The following are tasks for you to hopefully solve and achieve throughout this session. Consider these questions when you do the next few tutorials. Preliminary Tasks \u00b6 Get and put on your safety glasses Sign out a single crazyflie and associated parts Clear out a designated 2x2m ish flying space Main Tasks \u00b6 Build the crazyflie What are the different sensors and when should each one be used? Setup the software and crazy radio and connect your crazyflie Can you see the live sensor feed? Can you perform manual flight with your phone or game controller WATCH OUT - SMALL INPUTS NEEDED OTHERWISE IT WILL HIT THE CEILING / OTHER PEOPLE How does the crazyflie respond to your controls - is it sluggish? fast to react? How stable is the drone? Do you think this is a good way to control a drone? Using the crazyflie python library, write a simple script which (1) takesoff the drone, (2) flies to another point (3) lands What are the commands needed to achieve takeoff? How accurate is the drones takeoff height? How well does it fly to the other point, can you characterise it? How good is the landing? How repeatable is the flight? Now make the drone do something interesting e.g. Fly a more interesting pattern? Perform an experiment to find the accuracy of the system?","title":"Introduction"},{"location":"3a_practical_crazyflie/#practical-3-4-building-and-flying-crazyflies","text":"In this third and fourth practical we are jumping in and getting hands on with flying crazyflies - manually and hopefully if we're lucky autonomously doing simple things. Practical 3 & 4: Building and flying Crazyflies Crazyflies What are Crazyflies Why are we using them Safety Personal Safety Battery Safety Practical Tasks Preliminary Tasks Main Tasks","title":"Practical 3 &amp; 4: Building and flying Crazyflies"},{"location":"3a_practical_crazyflie/#crazyflies","text":"","title":"Crazyflies"},{"location":"3a_practical_crazyflie/#what-are-crazyflies","text":"The Crazyflie 2.1+. is a versatile open source micro-drone flying development platform that only weighs 27g and fits in the palm of your hand. Checkout their website to learn more about them - https://www.bitcraze.io/ You will see that they are used for R&D activities all over as its a perfect platform for learning, researching and experimenting with all things drones and drone swarms in a lab setting.","title":"What are Crazyflies"},{"location":"3a_practical_crazyflie/#why-are-we-using-them","text":"Inspired by aerial robotics courses from other institutions, we utilise the crazyflie as an initial teaching platform due to: Ease of use Size & Safety Programmability Large Community Through using them, you will learn the basics of constructing a drone platform and how the various physical components fit together. This includes the drone itself but also how you might communicate with a drone through the radio. The crazyflie comes with a nice cross-platform user interface for connecting to, configuring and monitoring a drone. This interface allows you to connect a game pad for manual flight of the crazyflie, but also there is an andriod and IOS app for manual flying of the drone over bluetooth. Bitcraze and various research groups have released a lot of open source, freely available code for programming the drone in python and c++, with many examples. This allows you to more easily implement and experiment with different methods of autonomy for the drone. Finally due to the small size, there is very little chance of injury ( PROVIDING YOU ARE WEARING YOUR SAFETY GLASSES ) when flying, even if you do collide with something or someone. This makes it ideal for use during a tutorial with many people in a classroom setting. As you will see later, flying with anything larger will require flying within our dedicated flying arena.","title":"Why are we using them"},{"location":"3a_practical_crazyflie/#safety","text":"","title":"Safety"},{"location":"3a_practical_crazyflie/#personal-safety","text":"WEAR YOUR SAFETY GLASSES AT ALL TIMES DURING THIS PRACTICAL It is recommended to wear long sleves and trousers When the drone is flying, it should always be watched","title":"Personal Safety"},{"location":"3a_practical_crazyflie/#battery-safety","text":"The Drones use small Lithium-Polymer batteries and as such we must be careful DO NOT DROP, PIERCE, HAMMER OR OTHERWISE DAMAGE THE BATTERIES IF A BATTERY LOOKS DAMAGED DO NOT USE IT AND TELL A STAFF MEMBER IF A BATTERY STARTS SMOKING OR IS ON FIRE TELL THE STAFF AND CALMLY MOVE AWAY FROM IT We will have a metal sand bucket and long tongs and the battery will go in there DO NOT ATTEMPT TO PUT OUT THE FIRE YOURSELF","title":"Battery Safety"},{"location":"3a_practical_crazyflie/#practical-tasks","text":"The following are tasks for you to hopefully solve and achieve throughout this session. Consider these questions when you do the next few tutorials.","title":"Practical Tasks"},{"location":"3a_practical_crazyflie/#preliminary-tasks","text":"Get and put on your safety glasses Sign out a single crazyflie and associated parts Clear out a designated 2x2m ish flying space","title":"Preliminary Tasks"},{"location":"3a_practical_crazyflie/#main-tasks","text":"Build the crazyflie What are the different sensors and when should each one be used? Setup the software and crazy radio and connect your crazyflie Can you see the live sensor feed? Can you perform manual flight with your phone or game controller WATCH OUT - SMALL INPUTS NEEDED OTHERWISE IT WILL HIT THE CEILING / OTHER PEOPLE How does the crazyflie respond to your controls - is it sluggish? fast to react? How stable is the drone? Do you think this is a good way to control a drone? Using the crazyflie python library, write a simple script which (1) takesoff the drone, (2) flies to another point (3) lands What are the commands needed to achieve takeoff? How accurate is the drones takeoff height? How well does it fly to the other point, can you characterise it? How good is the landing? How repeatable is the flight? Now make the drone do something interesting e.g. Fly a more interesting pattern? Perform an experiment to find the accuracy of the system?","title":"Main Tasks"},{"location":"3b_practical_crazyflie/","text":"Practical 3: Crazyflie Build and Installation \u00b6 Practical 3: Crazyflie Build and Installation Build Crazyflie Build Connect External Sensors Next Steps Build \u00b6 The first step is to unbox your crazyflie kit and build your crazyflie! To take you through the unboxing and first time setup, let us follow Bitcraze's own introductory tutorial which will hopefully get you to the state of flying! Come back when you have finished bench testing and are ready to fly Crazyflie Build \u00b6 Notes: Be gentle with the parts - the crazyflie is hardy but the connectors might not be! We have small tools you can borrow if needed. Do not fly yet! https://www.bitcraze.io/documentation/tutorials/getting-started-with-crazyflie-2-x/ To understand more about the GUI and to connect up a controller see: https://www.bitcraze.io/documentation/repository/crazyflie-clients-python/master/userguides/userguide_client/ Connect External Sensors \u00b6 Not included in the tutorials above are the rangefinder and flow v2 sensor decks. The crazyflie has a very limited ability to stably fly without external sensing of some descring. As it is right now, the drone will likely drift away very quickly and will be difficult to control. To solve this, we will be using the rangefinder and Flow v2 sensor decks from crazyflie for external sensing. The flow v2 sensor deck uses a camera on the bottom to perform optical flow to enable the drone to sense horizontal motion. It also has a downwards facing rangefinder for vertical stabilisation. The rangfinder deck then contains a number of ranging units for the other 5 directions. If the crazyflie detects the prescence of these decks, it will automatically use them to stabilse. More information on their operation is in the next tutorial. Rangefinder: Flow V2 Sensor: To see pictures and instructions for the mounting see the following: https://www.bitcraze.io/documentation/tutorials/getting-started-with-stem-ranging-bundle/ Double check that your boards are on the correct way around (notice the arrows) Next Steps \u00b6 Congratulations you have hopefully now built a working crazyflie which is ready to fly and hover in place. NOTE: you must notify an instructor to check your crazyflie before you should fly Go onto the next tutorial for instructions for manual and autonomous flying.","title":"Building Crazyflies"},{"location":"3b_practical_crazyflie/#practical-3-crazyflie-build-and-installation","text":"Practical 3: Crazyflie Build and Installation Build Crazyflie Build Connect External Sensors Next Steps","title":"Practical 3: Crazyflie Build and Installation"},{"location":"3b_practical_crazyflie/#build","text":"The first step is to unbox your crazyflie kit and build your crazyflie! To take you through the unboxing and first time setup, let us follow Bitcraze's own introductory tutorial which will hopefully get you to the state of flying! Come back when you have finished bench testing and are ready to fly","title":"Build"},{"location":"3b_practical_crazyflie/#crazyflie-build","text":"Notes: Be gentle with the parts - the crazyflie is hardy but the connectors might not be! We have small tools you can borrow if needed. Do not fly yet! https://www.bitcraze.io/documentation/tutorials/getting-started-with-crazyflie-2-x/ To understand more about the GUI and to connect up a controller see: https://www.bitcraze.io/documentation/repository/crazyflie-clients-python/master/userguides/userguide_client/","title":"Crazyflie Build"},{"location":"3b_practical_crazyflie/#connect-external-sensors","text":"Not included in the tutorials above are the rangefinder and flow v2 sensor decks. The crazyflie has a very limited ability to stably fly without external sensing of some descring. As it is right now, the drone will likely drift away very quickly and will be difficult to control. To solve this, we will be using the rangefinder and Flow v2 sensor decks from crazyflie for external sensing. The flow v2 sensor deck uses a camera on the bottom to perform optical flow to enable the drone to sense horizontal motion. It also has a downwards facing rangefinder for vertical stabilisation. The rangfinder deck then contains a number of ranging units for the other 5 directions. If the crazyflie detects the prescence of these decks, it will automatically use them to stabilse. More information on their operation is in the next tutorial. Rangefinder: Flow V2 Sensor: To see pictures and instructions for the mounting see the following: https://www.bitcraze.io/documentation/tutorials/getting-started-with-stem-ranging-bundle/ Double check that your boards are on the correct way around (notice the arrows)","title":"Connect External Sensors"},{"location":"3b_practical_crazyflie/#next-steps","text":"Congratulations you have hopefully now built a working crazyflie which is ready to fly and hover in place. NOTE: you must notify an instructor to check your crazyflie before you should fly Go onto the next tutorial for instructions for manual and autonomous flying.","title":"Next Steps"},{"location":"4_practical_crazyflie/","text":"Practical 4: Flying crazyflies \u00b6 Practical 4: Flying crazyflies Manual flight Before you fly Crazyradio 2.0 Preparation and Configuration Guide 1. Prepare & Flash the Crazyradio 2.0 Firmware 2. Install USB Driver Using Zadig for Windows 3. Set Up cfclient in Your Miniforge Environment 4. Connect and Verify 5. Changing the Radio Frequency Channel Flying Assisted manual flight Autonomous Flight PLEASE READ THIS ENTIRE TUTORIAL IN DETAIL BEFORE YOU START Manual flight \u00b6 Before you fly \u00b6 If you have followed the tutorial, you should have a built crazyflie 2.1 which you can connect to your laptop via a crazyradio or your mobile phone via bluetooth! We recommend using laptop-based method to fly first, as so far the stablization of crazyflie will only rely on your operation which makes very difficult to control through unprecise mobile phone interface. You could try fly through phones later. To mannually fly crazyflies through laptops, you need a game controller. Either playstation or xBox could work. Crazyradio 2.0 Preparation and Configuration Guide \u00b6 This guide explains how to prepare and flash the Crazyradio 2.0 firmware, install the necessary USB driver using Zadig (for Windows), set up cfclient in your Miniforge environment, connect and verify your Crazyflie, and change the radio frequency channel. 1. Prepare & Flash the Crazyradio 2.0 Firmware \u00b6 Reference: Getting Started with Crazyradio 2.0 Assemble the hardware: Screw the provided antenna onto the Crazyradio 2.0 dongle. Enter bootloader mode: Press and hold the onboard button while plugging the dongle into your USB port. You should see a pulsating red LED indicating bootloader mode. Download and flash firmware: Visit the Crazyradio2 Firmware Releases (v1.1) page. Get the latest compatible firmware file (a .uf2 file, e.g. named like crazyradio2-CRPA-emulation-[version].uf2 ) from Bitcraze\u2019s release page or documentation. Open your file explorer, locate the drive (usually named \u201cCrazyradio2\u201d), and drag & drop the .uf2 file onto it. Once flashed (the drive disappears), unplug and reinsert the dongle. A brief white LED flash confirms the new firmware is running. 2. Install USB Driver Using Zadig for Windows \u00b6 Download and run Zadig: Get the Zadig tool from its official website. Open Zadig and enable List All Devices from the Options menu. Install the driver: In the device dropdown, select your Crazyradio (often shown as \u201cCrazyradio PA\u201d or similar). Choose a libusb driver (e.g. libusb-win32 or libusbK ) and click Install (or Replace Driver ). This step is essential for Windows to allow proper USB communication with the Crazyradio 2.0. The Zadig usb driver installation step is not necessary for Linux or macOS, instead follow the link below. https://www.bitcraze.io/documentation/repository/crazyflie-lib-python/master/installation/usb_permissions/ These steps enable you to use the USB Radio and Crazyflie 2 over USB without requiring root access. You\u2019ll be able to work with Crazyflie 2 and Crazyradio without needing administrative privileges, eliminating the need to run commands with sudo during development and testing. 3. Set Up cfclient in Your Miniforge Environment \u00b6 Reference: cfclient Installation Instructions Install cfclient (or the full crazyflie-clients-python package): For the stable release where you plan to develop or customize, clone the repository and install in editable mode: git clone https://github.com/bitcraze/crazyflie-clients-python cd crazyflie-clients-python pip install -e . Launch the GUI Run the following command to to open the cfclient interface. cfclient 4. Connect and Verify \u00b6 Power on your Crazyflie: Make sure your Crazyflie is switched on. Scan in cfclient: Click the Scan button in the GUI. The Crazyradio (now emulating a Crazyradio PA) should detect your Crazyflie. Troubleshoot if needed: If the Crazyradio isn\u2019t listed, recheck the Zadig driver installation and ensure the dongle is correctly flashed and reinserted. Note: Do not fly at this stage before changing the Crazyradio radio frequency channel. 5. Changing the Radio Frequency Channel \u00b6 Reference: cfclient Userguide Prepare Your Setup: Ensure that the Crazyradio 2.0 dongle is correctly flashed and that the proper Zadig driver is installed on your Windows 11 machine (as described above). Launch cfclient from your Miniforge terminal. Initial Connection: Connect to your Crazyflie using the default connection URI. For example: radio://0/80/250k Open the Configuration Dialog: In cfclient, click on the Connect menu and select Configure 2.x (or Configure 2.0 if that\u2019s what your firmware uses). This opens the configuration dialog for radio-related parameters. Set the New Channel: Locate the Radio channel parameter in the dialog. Enter the desired channel value (the Crazyflie supports channels from 0 up to 82, where the channel number sets the frequency offset from 2400\u202fMHz; for instance, channel 15 corresponds to 2415\u202fMHz). Write and Save the Settings: Click the Write button to save the new channel value permanently to the Crazyflie\u2019s EEPROM. Restart and Reconnect: Disconnect the Crazyflie, then power-cycle it so that it boots using the new radio channel setting. Update the connection URI in cfclient to reflect the new channel. For example, if you set the channel to 50, use a URI like: radio://0/50/2M \u2013 Click Scan again and connect; the Crazyflie should now be visible on the new channel. Channel Selection Note: The Crazyflie\u2019s channel system works by adding the channel number to a base frequency of 2400\u202fMHz. For example: - Channel 20 corresponds to 2400\u202fMHz + 20\u202fMHz = 2420\u202fMHz . - Channel 70 corresponds to 2400\u202fMHz + 70\u202fMHz = 2470\u202fMHz . Important: - When operating in the UK, the 2.4\u202fGHz band is legally restricted to approximately 2400\u202fMHz\u20132483.5\u202fMHz. Therefore, channels should be chosen so that the resulting frequency does not exceed this range. - Always maintain at least 2\u202fMHz spacing between channels to minimize interference, and coordinate with your peers to ensure no two drones are set to the same channel. WARNING At this point, you must ensure that the crazyflie has external sensing otherwise the drone will be in manual flight mode and will have no ability to help you fly or stabilise it will drift . The joysticks will be mapped directly to roll/pitch/yaw and thrust. The joysticks control might be quite sensitive so use small movements until you get the hang of it You must be as smooth as you can with your joystick control - try not to twitch in the opposite direction when something happens - this results in a hard to recover drone! Familiarise yourself with the location of the emergency stop if you feel uncomfortable press it at any time (the crazyflies can handle heavy landings) To understand the coordinate system and roll pitch and yaw see the following link: - https://www.bitcraze.io/documentation/system/platform/cf2-coordinate-system/ When you are ready to fly, notify an instructor and place the crazyflie in the middle of your flying space. Flying \u00b6 The instructor will first check all is good with your drone and you have a good setup before you fly. Note: A batery only has around 5 minutes of flying time - keep an eye on it and swap it out if necessary. Just like being introduced to any new drone, you will first try to get a feel for how the drone will react to your control. Here is a list of actions you can go through to check and learn how your drone flies. Arming only test - In this test you will simply arm, wait a second, and then disarm he drone - you will not move the sticks. When you arm the drone, the motors will start spinning at minimum RPM, but not takeoff or otherwise move. Disarming the drone will then stop the motors. Does the drone arm and disarm again straight after without needing a restart Arming ESTOP test - In this test you will familiarise yourself with, and ensure that the emergency stop button works. Arm the drone and then press the estop. The motors should stop and the GUI should show emergency stop has been pressed. Floor thrust test - In this test you will start to familiarise yourself with the thrust control of the controller and response from the drone. You will want to gently increase the thrust, and you should start to see the drone looking like it wants to get off the ground. Try and get a feel for its response! Also worth noting if the drone appears to pivot to one side instead of thrusting equally from all four motors - you may need to compensate a little using the yaw and pitch. Gentle Takeoff and Landing - In this test you will attempt to have the drone takeoff and gently land in a controlled fashion. Building upon the previous, you should gently increase thrust until the drone leaves the ground. When it leaves the ground, try and hold the right thrust level if you can, otherwise reduce the thrust and try and land softly on the ground. Important A lot of beginner flyers will jerk the stick when it takes off - either plunging it into the ceiling or floor. Try to avoid this by holding your nerve and staying in control. If you feel out of control hit the ESTOP Landing Tip - Thust up a little bit just before you hit the floor, this will reduce the impact on the drone. Hovering - Once you are happy and in control with takeoff and landing, you should try and hold the hover for longer, correcting for horizontal drift using the roll/pitch correction. If you feel out of control hit the ESTOP Controlled Flight (no yaw) - Hopefully at this point you should be feeling a lot more in control of the drone (but you probably need a bit of practice!). Now try and slowly fly some shapes (square, circle etc) with the drone always facing away from you (no yaw). Controlled Flight - Now try flying the same shapes, but this time using yaw instead of rolling, i.e. always facing in the direction you want to move. It is recommended you try this by first yawing on the spot then moving forward. When you get more confident, you will be able to yaw while moving - this is the closest you'll get to FPV flying in this course! If working in a pair, don't forget to swap over! Assisted manual flight \u00b6 At this point, you may have noticed that the drone doesn't have any external sensing capability. You may also have noticed in the GUI and on the control scheme, there is a button labelled Assist Mode this will perform an automatic takeoff and activate hover mode as long as its pressed down. Unlike UGV,Drones need a method of external sensing on top of internal IMU and gyroscope in order to hover in place and not drift, especially if autonomously flying. What you need to do now is to check which sensor can help you to achieve assisted mannual flight. For commercial and custom drones there are a variety of sensors available, as will be shown in lectures, and this includes sensors like: Cameras (Monocular, Stereo, Depth) LIDAR (Single Beam, Wedge and 360) Laser Rangefinders Optical Flow Cameras External Position Tracking (Motion Capture, UWB, etc) For use with the crazyflies there are several sensor add-on boards available which we may have for the tutorial. Loco Positioning System (** we are not using this today) Hopefully we will have access to the loco positioning system which uses ultra wide band signals from multiple anchors placed around the room. This enables a tagging board on the drone to work out the drones position to centimeter level accuracy You will have been given a Loco Tag add-on board, this will need to be placed on top of the GPIO stack - it replaces the battery holder! Ensure that it is facing in the correct direction! With this the drone will be able to hover and know its location in the real world Read more about how it works here: https://www.bitcraze.io/documentation/system/positioning/loco-positioning-system/ About General Positioning Systems and crazyflies: https://www.bitcraze.io/documentation/system/positioning/ For the anchors We will likely need to use the Time Difference of Arrival 3 (TDOA3 Mode) to enable all the drones to fly with lots more anchors - consider why this might be. Flow Deck V2 This is a sensor which can be attached to the bottom of the crazyflie and provides hardware optical flow and height information If you don't know what Optical Flow is, it is a method of estimating horizontal motion from a camera image See this opencv tutorial for more information: https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html It also has a Time of Flight sensor for measuring its height above ground level Together these allow the drone to stabily hover in one location, and estimate its location relative to its take-off location! Place the sensor on the bottom of the GPIO stack on the underside of the drone Ensure that it is facing in the correct direction! Read about the usage here: https://www.bitcraze.io/documentation/tutorials/getting-started-with-flow-deck/ Multi-Ranger Deck This board has 5 distance time of flight sensors on it allowing the simple perception of the area around the drone. This perception gives the drone the capability to react to its surroundings and perform simple obstacle avoidance, detection, wall following and other tasks. It also allows to start working on environment-aware problems like Simultaneous Localization And Mapping (SLAM) algorithms. LED Ring Deck (** we are not using these today) While not a sensor, its an attachment that can go on the bottom of the deck! These will hopefully be used for the light-show challenge Note that they go on the bottom and do not work at the same time as the flow deck. As a result, will need the loco positioning system to work properly. There are a number of other sensors and other details too Checkout this link for a full list of expansion decks See that link for tips on mounting the decks if confused Autonomous Flight \u00b6 Having hopefully flown the drone manually, you should now be beginning to form some intuitions as to how a drone flies, its features, things to watch out for and so on. Now we move onto starting to consider a key part of this course - which is autonomy. Autonomy is a bit different for drones over other robots you may have used in the past. Unlike ground vehicles, we have to consider the effects of a 3rd dimension, and need some form of cascaded control to stay in the air. Also whilst the inverse kinematics calculations for arms are not necessary for drones - features such as smooth trajectory generation and following, and mission planning are neccesary. Thankfully, crazyflie offers a simple starting point with a python api for autonomous flight. This will get you back up to speed with programming and testing robots before we delve into more complex autonomous systems which you would see flavours of in the real world. Remember at the end of the day, all of these different frameworks are just tools - we hope that through learning to use these tools, you also absorb and learn more generalisable features of drone autonomy which you could apply to the many other open source and proprietary systems which exist out there! For crazyflie autonomy, try out the following links which show a simple getting started! Flow Deck tutorial Multi-Ranger Deck tutorial Loco Deck tutorial Example Scripts Link Your goal here is to have the drone fly in some shapes autonomosly As a bonus, see if you can start to work out how to use the LED Deck! You will likely needed to look up a little of the documentation for the api but I'm sure you can mostly use the examples to work out the best way to program the drones to do what you need. Don't worry about making things too complicated, as we will be quickly moving on to ROS2 for drone control!","title":"Flying Crazyflies"},{"location":"4_practical_crazyflie/#practical-4-flying-crazyflies","text":"Practical 4: Flying crazyflies Manual flight Before you fly Crazyradio 2.0 Preparation and Configuration Guide 1. Prepare & Flash the Crazyradio 2.0 Firmware 2. Install USB Driver Using Zadig for Windows 3. Set Up cfclient in Your Miniforge Environment 4. Connect and Verify 5. Changing the Radio Frequency Channel Flying Assisted manual flight Autonomous Flight PLEASE READ THIS ENTIRE TUTORIAL IN DETAIL BEFORE YOU START","title":"Practical 4: Flying crazyflies"},{"location":"4_practical_crazyflie/#manual-flight","text":"","title":"Manual flight"},{"location":"4_practical_crazyflie/#before-you-fly","text":"If you have followed the tutorial, you should have a built crazyflie 2.1 which you can connect to your laptop via a crazyradio or your mobile phone via bluetooth! We recommend using laptop-based method to fly first, as so far the stablization of crazyflie will only rely on your operation which makes very difficult to control through unprecise mobile phone interface. You could try fly through phones later. To mannually fly crazyflies through laptops, you need a game controller. Either playstation or xBox could work.","title":"Before you fly"},{"location":"4_practical_crazyflie/#crazyradio-20-preparation-and-configuration-guide","text":"This guide explains how to prepare and flash the Crazyradio 2.0 firmware, install the necessary USB driver using Zadig (for Windows), set up cfclient in your Miniforge environment, connect and verify your Crazyflie, and change the radio frequency channel.","title":"Crazyradio 2.0 Preparation and Configuration Guide"},{"location":"4_practical_crazyflie/#1-prepare-flash-the-crazyradio-20-firmware","text":"Reference: Getting Started with Crazyradio 2.0 Assemble the hardware: Screw the provided antenna onto the Crazyradio 2.0 dongle. Enter bootloader mode: Press and hold the onboard button while plugging the dongle into your USB port. You should see a pulsating red LED indicating bootloader mode. Download and flash firmware: Visit the Crazyradio2 Firmware Releases (v1.1) page. Get the latest compatible firmware file (a .uf2 file, e.g. named like crazyradio2-CRPA-emulation-[version].uf2 ) from Bitcraze\u2019s release page or documentation. Open your file explorer, locate the drive (usually named \u201cCrazyradio2\u201d), and drag & drop the .uf2 file onto it. Once flashed (the drive disappears), unplug and reinsert the dongle. A brief white LED flash confirms the new firmware is running.","title":"1. Prepare &amp; Flash the Crazyradio 2.0 Firmware"},{"location":"4_practical_crazyflie/#2-install-usb-driver-using-zadig-for-windows","text":"Download and run Zadig: Get the Zadig tool from its official website. Open Zadig and enable List All Devices from the Options menu. Install the driver: In the device dropdown, select your Crazyradio (often shown as \u201cCrazyradio PA\u201d or similar). Choose a libusb driver (e.g. libusb-win32 or libusbK ) and click Install (or Replace Driver ). This step is essential for Windows to allow proper USB communication with the Crazyradio 2.0. The Zadig usb driver installation step is not necessary for Linux or macOS, instead follow the link below. https://www.bitcraze.io/documentation/repository/crazyflie-lib-python/master/installation/usb_permissions/ These steps enable you to use the USB Radio and Crazyflie 2 over USB without requiring root access. You\u2019ll be able to work with Crazyflie 2 and Crazyradio without needing administrative privileges, eliminating the need to run commands with sudo during development and testing.","title":"2. Install USB Driver Using Zadig for Windows"},{"location":"4_practical_crazyflie/#3-set-up-cfclient-in-your-miniforge-environment","text":"Reference: cfclient Installation Instructions Install cfclient (or the full crazyflie-clients-python package): For the stable release where you plan to develop or customize, clone the repository and install in editable mode: git clone https://github.com/bitcraze/crazyflie-clients-python cd crazyflie-clients-python pip install -e . Launch the GUI Run the following command to to open the cfclient interface. cfclient","title":"3. Set Up cfclient in Your Miniforge Environment"},{"location":"4_practical_crazyflie/#4-connect-and-verify","text":"Power on your Crazyflie: Make sure your Crazyflie is switched on. Scan in cfclient: Click the Scan button in the GUI. The Crazyradio (now emulating a Crazyradio PA) should detect your Crazyflie. Troubleshoot if needed: If the Crazyradio isn\u2019t listed, recheck the Zadig driver installation and ensure the dongle is correctly flashed and reinserted. Note: Do not fly at this stage before changing the Crazyradio radio frequency channel.","title":"4. Connect and Verify"},{"location":"4_practical_crazyflie/#5-changing-the-radio-frequency-channel","text":"Reference: cfclient Userguide Prepare Your Setup: Ensure that the Crazyradio 2.0 dongle is correctly flashed and that the proper Zadig driver is installed on your Windows 11 machine (as described above). Launch cfclient from your Miniforge terminal. Initial Connection: Connect to your Crazyflie using the default connection URI. For example: radio://0/80/250k Open the Configuration Dialog: In cfclient, click on the Connect menu and select Configure 2.x (or Configure 2.0 if that\u2019s what your firmware uses). This opens the configuration dialog for radio-related parameters. Set the New Channel: Locate the Radio channel parameter in the dialog. Enter the desired channel value (the Crazyflie supports channels from 0 up to 82, where the channel number sets the frequency offset from 2400\u202fMHz; for instance, channel 15 corresponds to 2415\u202fMHz). Write and Save the Settings: Click the Write button to save the new channel value permanently to the Crazyflie\u2019s EEPROM. Restart and Reconnect: Disconnect the Crazyflie, then power-cycle it so that it boots using the new radio channel setting. Update the connection URI in cfclient to reflect the new channel. For example, if you set the channel to 50, use a URI like: radio://0/50/2M \u2013 Click Scan again and connect; the Crazyflie should now be visible on the new channel. Channel Selection Note: The Crazyflie\u2019s channel system works by adding the channel number to a base frequency of 2400\u202fMHz. For example: - Channel 20 corresponds to 2400\u202fMHz + 20\u202fMHz = 2420\u202fMHz . - Channel 70 corresponds to 2400\u202fMHz + 70\u202fMHz = 2470\u202fMHz . Important: - When operating in the UK, the 2.4\u202fGHz band is legally restricted to approximately 2400\u202fMHz\u20132483.5\u202fMHz. Therefore, channels should be chosen so that the resulting frequency does not exceed this range. - Always maintain at least 2\u202fMHz spacing between channels to minimize interference, and coordinate with your peers to ensure no two drones are set to the same channel. WARNING At this point, you must ensure that the crazyflie has external sensing otherwise the drone will be in manual flight mode and will have no ability to help you fly or stabilise it will drift . The joysticks will be mapped directly to roll/pitch/yaw and thrust. The joysticks control might be quite sensitive so use small movements until you get the hang of it You must be as smooth as you can with your joystick control - try not to twitch in the opposite direction when something happens - this results in a hard to recover drone! Familiarise yourself with the location of the emergency stop if you feel uncomfortable press it at any time (the crazyflies can handle heavy landings) To understand the coordinate system and roll pitch and yaw see the following link: - https://www.bitcraze.io/documentation/system/platform/cf2-coordinate-system/ When you are ready to fly, notify an instructor and place the crazyflie in the middle of your flying space.","title":"5. Changing the Radio Frequency Channel"},{"location":"4_practical_crazyflie/#flying","text":"The instructor will first check all is good with your drone and you have a good setup before you fly. Note: A batery only has around 5 minutes of flying time - keep an eye on it and swap it out if necessary. Just like being introduced to any new drone, you will first try to get a feel for how the drone will react to your control. Here is a list of actions you can go through to check and learn how your drone flies. Arming only test - In this test you will simply arm, wait a second, and then disarm he drone - you will not move the sticks. When you arm the drone, the motors will start spinning at minimum RPM, but not takeoff or otherwise move. Disarming the drone will then stop the motors. Does the drone arm and disarm again straight after without needing a restart Arming ESTOP test - In this test you will familiarise yourself with, and ensure that the emergency stop button works. Arm the drone and then press the estop. The motors should stop and the GUI should show emergency stop has been pressed. Floor thrust test - In this test you will start to familiarise yourself with the thrust control of the controller and response from the drone. You will want to gently increase the thrust, and you should start to see the drone looking like it wants to get off the ground. Try and get a feel for its response! Also worth noting if the drone appears to pivot to one side instead of thrusting equally from all four motors - you may need to compensate a little using the yaw and pitch. Gentle Takeoff and Landing - In this test you will attempt to have the drone takeoff and gently land in a controlled fashion. Building upon the previous, you should gently increase thrust until the drone leaves the ground. When it leaves the ground, try and hold the right thrust level if you can, otherwise reduce the thrust and try and land softly on the ground. Important A lot of beginner flyers will jerk the stick when it takes off - either plunging it into the ceiling or floor. Try to avoid this by holding your nerve and staying in control. If you feel out of control hit the ESTOP Landing Tip - Thust up a little bit just before you hit the floor, this will reduce the impact on the drone. Hovering - Once you are happy and in control with takeoff and landing, you should try and hold the hover for longer, correcting for horizontal drift using the roll/pitch correction. If you feel out of control hit the ESTOP Controlled Flight (no yaw) - Hopefully at this point you should be feeling a lot more in control of the drone (but you probably need a bit of practice!). Now try and slowly fly some shapes (square, circle etc) with the drone always facing away from you (no yaw). Controlled Flight - Now try flying the same shapes, but this time using yaw instead of rolling, i.e. always facing in the direction you want to move. It is recommended you try this by first yawing on the spot then moving forward. When you get more confident, you will be able to yaw while moving - this is the closest you'll get to FPV flying in this course! If working in a pair, don't forget to swap over!","title":"Flying"},{"location":"4_practical_crazyflie/#assisted-manual-flight","text":"At this point, you may have noticed that the drone doesn't have any external sensing capability. You may also have noticed in the GUI and on the control scheme, there is a button labelled Assist Mode this will perform an automatic takeoff and activate hover mode as long as its pressed down. Unlike UGV,Drones need a method of external sensing on top of internal IMU and gyroscope in order to hover in place and not drift, especially if autonomously flying. What you need to do now is to check which sensor can help you to achieve assisted mannual flight. For commercial and custom drones there are a variety of sensors available, as will be shown in lectures, and this includes sensors like: Cameras (Monocular, Stereo, Depth) LIDAR (Single Beam, Wedge and 360) Laser Rangefinders Optical Flow Cameras External Position Tracking (Motion Capture, UWB, etc) For use with the crazyflies there are several sensor add-on boards available which we may have for the tutorial. Loco Positioning System (** we are not using this today) Hopefully we will have access to the loco positioning system which uses ultra wide band signals from multiple anchors placed around the room. This enables a tagging board on the drone to work out the drones position to centimeter level accuracy You will have been given a Loco Tag add-on board, this will need to be placed on top of the GPIO stack - it replaces the battery holder! Ensure that it is facing in the correct direction! With this the drone will be able to hover and know its location in the real world Read more about how it works here: https://www.bitcraze.io/documentation/system/positioning/loco-positioning-system/ About General Positioning Systems and crazyflies: https://www.bitcraze.io/documentation/system/positioning/ For the anchors We will likely need to use the Time Difference of Arrival 3 (TDOA3 Mode) to enable all the drones to fly with lots more anchors - consider why this might be. Flow Deck V2 This is a sensor which can be attached to the bottom of the crazyflie and provides hardware optical flow and height information If you don't know what Optical Flow is, it is a method of estimating horizontal motion from a camera image See this opencv tutorial for more information: https://docs.opencv.org/3.4/d4/dee/tutorial_optical_flow.html It also has a Time of Flight sensor for measuring its height above ground level Together these allow the drone to stabily hover in one location, and estimate its location relative to its take-off location! Place the sensor on the bottom of the GPIO stack on the underside of the drone Ensure that it is facing in the correct direction! Read about the usage here: https://www.bitcraze.io/documentation/tutorials/getting-started-with-flow-deck/ Multi-Ranger Deck This board has 5 distance time of flight sensors on it allowing the simple perception of the area around the drone. This perception gives the drone the capability to react to its surroundings and perform simple obstacle avoidance, detection, wall following and other tasks. It also allows to start working on environment-aware problems like Simultaneous Localization And Mapping (SLAM) algorithms. LED Ring Deck (** we are not using these today) While not a sensor, its an attachment that can go on the bottom of the deck! These will hopefully be used for the light-show challenge Note that they go on the bottom and do not work at the same time as the flow deck. As a result, will need the loco positioning system to work properly. There are a number of other sensors and other details too Checkout this link for a full list of expansion decks See that link for tips on mounting the decks if confused","title":"Assisted manual flight"},{"location":"4_practical_crazyflie/#autonomous-flight","text":"Having hopefully flown the drone manually, you should now be beginning to form some intuitions as to how a drone flies, its features, things to watch out for and so on. Now we move onto starting to consider a key part of this course - which is autonomy. Autonomy is a bit different for drones over other robots you may have used in the past. Unlike ground vehicles, we have to consider the effects of a 3rd dimension, and need some form of cascaded control to stay in the air. Also whilst the inverse kinematics calculations for arms are not necessary for drones - features such as smooth trajectory generation and following, and mission planning are neccesary. Thankfully, crazyflie offers a simple starting point with a python api for autonomous flight. This will get you back up to speed with programming and testing robots before we delve into more complex autonomous systems which you would see flavours of in the real world. Remember at the end of the day, all of these different frameworks are just tools - we hope that through learning to use these tools, you also absorb and learn more generalisable features of drone autonomy which you could apply to the many other open source and proprietary systems which exist out there! For crazyflie autonomy, try out the following links which show a simple getting started! Flow Deck tutorial Multi-Ranger Deck tutorial Loco Deck tutorial Example Scripts Link Your goal here is to have the drone fly in some shapes autonomosly As a bonus, see if you can start to work out how to use the LED Deck! You will likely needed to look up a little of the documentation for the api but I'm sure you can mostly use the examples to work out the best way to program the drones to do what you need. Don't worry about making things too complicated, as we will be quickly moving on to ROS2 for drone control!","title":"Autonomous Flight"},{"location":"5_practical_crazyflie_aerostack2/","text":"Practical 5: Crazyflie with aerostack2 \u00b6 In this fifth practical, we will reimplement manual flight and scripted autonomous flight of crazyflies via aerostack2. Practical 5: Crazyflie with aerostack2 Clone and build aerostack2 package: as2_platform_crazyflie Clone project_crazyflie into your aerostack2 workspace Change radio channel in config file Teleoperate and autonomously fly crazyflie via aerostack2 Clone and build aerostack2 package: as2_platform_crazyflie \u00b6 First step is to clone and build another aerostack2 package called as2_platform_crazyflie. Please follow the instructions here before aerostack2 common interface. https://aerostack2.github.io/_03_aerial_platforms/_crazyflie/index.html NOTE: Use source installation and clone the package in the src folder under your aerostack2 workspace. **NOTE: The documentation may use the \"git@..\" and you may have issues with ssh. Go to the github repository and use the \"https://...\" version! Clone project_crazyflie into your aerostack2 workspace \u00b6 Then, you need to clone the project_crazyflie into your aerostack2 workspace. Follow the instructions here. https://github.com/aerostack2/project_crazyflie?tab=readme-ov-file Change radio channel in config file \u00b6 To be able to connect to your crazyflie, you need to change the setting of radio channel in the package's config file. This can be done by accessing the config/config.yaml: configuration file for the Crazyflie drones. NOTE: Make sure the radio channel set in the config file matches the channel you plan to use in the cfclient GUI, as done in the previous lab. Inconsistent channels will prevent the drone from connecting. Please read through instructions here. https://aerostack2.github.io/_02_examples/crazyflie/project_crazyflie/ Teleoperate and autonomously fly crazyflie via aerostack2 \u00b6 Now, you should be able to lanuch the project, connect to, teleoperate, and autonomously fly your crazyflie. Follow instructions here. https://github.com/aerostack2/project_crazyflie?tab=readme-ov-file\u200b\u200b NOTE: 1.Before launch any bash file, disconnect your crazyflie with your cfclient which may occupy the radio channel. 2. After each flight, you have to manually restart your crazyflie. NOTE: 2. After each flight, you have to manually restart your crazyflie.If you find the Crazyflie remains unresponsive after a flight or changing configs, physically unplug the battery for a few seconds, then reconnect it. NOTE: 3. Always test your Crazyflie in an open, safe area free of obstacles, and with safety googles on, especially when first trying autonomous flight. Keep emergency stop in mind if something goes wrong.","title":"Crazyflie with aerostack2"},{"location":"5_practical_crazyflie_aerostack2/#practical-5-crazyflie-with-aerostack2","text":"In this fifth practical, we will reimplement manual flight and scripted autonomous flight of crazyflies via aerostack2. Practical 5: Crazyflie with aerostack2 Clone and build aerostack2 package: as2_platform_crazyflie Clone project_crazyflie into your aerostack2 workspace Change radio channel in config file Teleoperate and autonomously fly crazyflie via aerostack2","title":"Practical 5: Crazyflie with aerostack2"},{"location":"5_practical_crazyflie_aerostack2/#clone-and-build-aerostack2-package-as2_platform_crazyflie","text":"First step is to clone and build another aerostack2 package called as2_platform_crazyflie. Please follow the instructions here before aerostack2 common interface. https://aerostack2.github.io/_03_aerial_platforms/_crazyflie/index.html NOTE: Use source installation and clone the package in the src folder under your aerostack2 workspace. **NOTE: The documentation may use the \"git@..\" and you may have issues with ssh. Go to the github repository and use the \"https://...\" version!","title":"Clone and build aerostack2 package: as2_platform_crazyflie"},{"location":"5_practical_crazyflie_aerostack2/#clone-project_crazyflie-into-your-aerostack2-workspace","text":"Then, you need to clone the project_crazyflie into your aerostack2 workspace. Follow the instructions here. https://github.com/aerostack2/project_crazyflie?tab=readme-ov-file","title":"Clone project_crazyflie into your aerostack2 workspace"},{"location":"5_practical_crazyflie_aerostack2/#change-radio-channel-in-config-file","text":"To be able to connect to your crazyflie, you need to change the setting of radio channel in the package's config file. This can be done by accessing the config/config.yaml: configuration file for the Crazyflie drones. NOTE: Make sure the radio channel set in the config file matches the channel you plan to use in the cfclient GUI, as done in the previous lab. Inconsistent channels will prevent the drone from connecting. Please read through instructions here. https://aerostack2.github.io/_02_examples/crazyflie/project_crazyflie/","title":"Change radio channel in config file"},{"location":"5_practical_crazyflie_aerostack2/#teleoperate-and-autonomously-fly-crazyflie-via-aerostack2","text":"Now, you should be able to lanuch the project, connect to, teleoperate, and autonomously fly your crazyflie. Follow instructions here. https://github.com/aerostack2/project_crazyflie?tab=readme-ov-file\u200b\u200b NOTE: 1.Before launch any bash file, disconnect your crazyflie with your cfclient which may occupy the radio channel. 2. After each flight, you have to manually restart your crazyflie. NOTE: 2. After each flight, you have to manually restart your crazyflie.If you find the Crazyflie remains unresponsive after a flight or changing configs, physically unplug the battery for a few seconds, then reconnect it. NOTE: 3. Always test your Crazyflie in an open, safe area free of obstacles, and with safety googles on, especially when first trying autonomous flight. Keep emergency stop in mind if something goes wrong.","title":"Teleoperate and autonomously fly crazyflie via aerostack2"},{"location":"6_practical_pixhawk_PX4/","text":"Practical 6: Pixhawk and PX4, Commercial and R&D Autonomous Drone Systems \u00b6 Practical 6: Pixhawk and PX4, Commercial and R&D Autonomous Drone Systems Overview What is Pixhawk? Common Pixhawk Versions: How to use a Pixhawk Flight Control Firmware PX4 Software Stack Autopilot communication Ground Stations Setting Up Pixhawk with PX4 and QGroundControl Setup Overview Install QGroundControl Power up and connect the Pixhawk Flashing the Pixhawk Sensor Calibration Continue Setup Next step Using PX4 in HITL (Hardware-In-The-Loop) Simulation Installing Dependencies Ubuntu JMavSim Install (Ubuntu Tested) Steps to Run PX4 HITL: Starting Pixhawk Configuring your Pixhawk/PX4 installation for HITL Running JMAVSim Tasks Summary Overview \u00b6 Pixhawk and PX4 are widely used in autonomous drone development, providing an open-source hardware and software ecosystem for UAV control. In this tutorial, we will introduce: Pixhawk : The hardware autopilot platform PX4 : The open-source flight control firmware How to set up and use PX4 with Pixhawk PX4 Hardware In The Loop (HITL) Testing Creating Missions with QGC This will help you transition from Crazyflies and Aerostack2 to more advanced UAV platforms. What is Pixhawk? \u00b6 Pixhawk is a family of open-source flight controllers developed for UAVs and robotics applications. It provides: High-performance autopilot capabilities Multiple sensor support (GPS, IMUs, barometers, etc.) Compatibility with PX4 and ArduPilot firmware Connectivity options like UART, I2C, CAN, MAVLink and DDS Common Pixhawk Versions: \u00b6 Pixhawk 4 : Standard for academic and research use Pixhawk 6X : More advanced processing capabilities Pixhawk 6C : Compact version with strong processing power Pixhawk Mini : Compact version for small UAVs How to use a Pixhawk \u00b6 This is the Pixhawk 6C you will be playing around with, see all of the ports for various inputs This following wiring diagram is from an older Pixhawk model, but should show the key elements of connecting up and powering a Pixhawk Flight Control Firmware \u00b6 There is no universal controller design of converting from user inputs to motor thrust. In the same way, there are numerous other functionalities that an autopilot can cover. These can range from running control loops for gimbals, cameras and other actuation, to high level mission following and safety features. These functionalities are bundled into specific autopilot firmware which each offer a slightly different set of features, as well as differing user interfaces each with their advantages and drawbacks. The two current most common autopilot firmware's in use in research settings are Ardupilot which offers the Arducopter firmware, and PX4 which offers Multicopter firmware. Both these firmwares are very extensive and cover numerous use cases. However, for our purposes we will only cover enabling autonomous flight through observing the mode of the autopilot. Both Ardupilot and PX4 use the concept of flight modes, where each mode operates and supports different levels or types of flight stabilisation and/or autonomous functions. Traditionally this is for pilots to change between different controller layouts for different applications. It's necessary to change to the correct mode for safe and controllable flight. The following table shows the most often used flight modes within Starling. Ardupilot Mode PX4 Mode Functionality stabilized manual Full manual control with RC sticks being sent directly to control roll, pitch, yaw and height PosHold position UAV uses onboard sensing to stay in place, RC sticks used to translate position loiter auto.hold Automatic mode where UAV stays in the same location until further instructions given. land auto.land Automatic mode which attempts to land the UAV Guided offboard Navigates to setpoints sent to it by ground control or companion computer As mentioned before, the base purpose of the firmware is to provide a given cascading PID controller for converting high level commands to motor thrusts. However both firmwares provide a plethora of other functionality from trajectory following, basic mission following, telemetry and communications and many others too. As a controller developer, it is also useful to understand the differences between the Ardupilot and PX4 controllers and what real-world impacts they have. In most of drone targeted applications we only require either position or velocity control which works fairly consistently between the two firmwares. In our own work, it has generally been noted that Ardupilot seems to be more suitable for outdoor flight, and PX4 for indoor flight. For this tutorial we will be developing a controller for indoor multi-vehicle flight and so we will assume the use of PX4. The biggest difference is actually in licensing where Ardupilot's license specifies that any developments must be contributed back, however PX4 is a bit more free allowing the forking and commercialisation without needing the announce or contribute (although this is not necessarily the best for the longevity of this open source project!). PX4 Software Stack \u00b6 PX4 consists of: PX4 Firmware - The core flight control software QGroundControl (QGC) - GUI for setup and mission planning MAVSDK (or other communication method) - API for developing drone applications Hardware-In-The-Loop (HITL) - Using real Pixhawk hardware for simulation or Software-In-The-Loop (SITL) for using simulated drones for testing. Autopilot communication \u00b6 Once in guided or offboard mode, the autopilot by default expects communications using the MAVLINK protocol . Traditionally this would have been used for a ground control station (GCS) to send commands to a UAV over a telemetry link. However, now it has also developed into a protocol for commanding the autopilot from an onboard companion computer over a USB or serial connection too. The MAVLink protocol is a set of preset commands which compatible firmwares understand and react to. In this tutorial we will primarily be observing the MAVlink interface, but we will not get into writing your own software with MAVlink yet. With the growing prevalence of ROS2, all of the major firmwares have attempted to provide direct communication using ROS2's underlying communication/middleware protocol of DDS (Data Distribution Service). Although we will not cover this in this module, this is what we currently use to communicate between a companion computer and the Pixhawk for drone work. Note however that even if DDS is enabled, autopilots will still send Mavlink messages down a Telemetry stream - often a dedicated low bandwidth communication channel for the monitoring of the drone's health and status. Ground Stations \u00b6 A key element of any drone operation is a competent and reliable ground station setup. The purpose of the ground station is to provide a reliable communication link with the drone and receive telemetry to be able to keep an eye on, and monitor the status of the drone. Often this is done via a second telemtry radio operating on a different frequency - in our case an SIK radio on 433MHz - with one end connected to the drone, and the other connected via USB to a laptop or computer (or handheld games console like the steam deck). There are a number of choices for a piece of software which runs on the ground station. In industry, you may see Alterion or other softwares which provide an interface. In the open source world we either use mission planner with Ardupilot or QGroundControl with PX4. In this tutorial we will trying to use QGroundControl (QGC). Tools like QGC are crucial for real-time flight monitoring, control, and mission planning . It allows users to: Monitor telemetry data (battery levels, GPS status, IMU readings, etc.). Send flight commands such as takeoff, landing, and mission execution. Configure drone parameters including PID tuning, sensor calibrations, and failsafe settings. Visualize the drone's location and trajectory on a map interface. Log and analyze flight data to debug issues or optimize performance. Ground stations are useful as they enable: Safety : Enables real-time monitoring, preventing potential failures. Ease of Use : Provides a user-friendly interface to interact with the drone. Flexibility : Supports different flight modes (manual, autonomous, guided, etc.). Data Logging : Essential for post-flight analysis, AI training, and debugging. Remote Control : Operate the drone without requiring direct physical interaction. Setting Up Pixhawk with PX4 and QGroundControl \u00b6 Now let's set up your Pixhawk with PX4, connect to it and do some initial calibrations with QGC. Setup Overview \u00b6 Install QGroundControl (QGC) ( Download ) Flash PX4 firmware onto Pixhawk via QGC Configure sensors and RC calibration Use QGC to set up some automated flights (Not today) Setup offboard control (for AI/robotics integration) Install QGroundControl \u00b6 Follow the following instructions for all platforms: https://docs.qgroundcontrol.com/master/en/qgc-user-guide/getting_started/quick_start.html Power up and connect the Pixhawk \u00b6 Tidily unbox the Pixhawk box in a way that you can put everything back later! For this tutorial, all you will need is: Pixhawk USB-C cable Plug the USB-C cable from the Pixhawk to a port in your laptop Now start QGroundControl In the top left it will hopefully automatically pick up the Pixhawk and you can start browsing the settings and seeing the live view. See https://docs.qgroundcontrol.com/master/en/qgc-user-guide/getting_started/quick_start.html for how to navigate and use QGC. Flashing the Pixhawk \u00b6 The first step for us is to flash the latest PX4 version onto the pixhawk. This can be done easily from within QGC. Follow the following instruction page: https://docs.qgroundcontrol.com/master/en/qgc-user-guide/setup_view/firmware.html Sensor Calibration \u00b6 The next step would be to perform sensor calibration in order to calibrate the onboard accelerometer and gyroscope. In the setup menu go to the sensors tab and follow the instructions. Of course in practice you would perform the calibration after having built the drone, but this will do for now! Continue Setup \u00b6 Scroll through the other setup steps, there are a number that we are skipping in this session, but in practice you would have to usually complete: Radio Setup : It's standard to have a radio connected with both a receiver connected to the pixhawk and a transmitter with the dedicated pilot. The radio is important in order to have a backup method of controlling the drone, as well enabling mode switching between various manual flight regimes and offboard mode. Important for safety too as having a emergency stop setup is highly recommended Safety Setup : All the different safety systems built in from return to home to parachutes if they're installed. Parameters : All of these setup screens essentially manipulate the value of various parameters under the hood. Have a scroll through the parameter list and you will quickly see that there are a lot of options for a variety of scenarios. Bear in mind that PX4 is highly multi-functional as it has been written to work on anything from fixed-wing aircraft to mini-submarines! You might want to try setting up the virtual joystick to enable easier testing! Joystick Next step \u00b6 So once the setup is complete, the last thing to have a look at is observing some of the MAVLink that's coming through Go have a look at the Analyse View: analyse view , and then the mavlink inspector However you will note that because we have a \"drone\" (just the Pixhawk sitting on your desk that wont fly) you wont really be able to do much with it. Traditionally we would use Software In The Loop testing to simulate a virtual Pixhawk for testing and development. However in this class we would like to try and use Hardware-In-The-Loop testing to \"fly\" your drone. Using PX4 in HITL (Hardware-In-The-Loop) Simulation \u00b6 For development and testing in industry, we often use Software-In-The-Loop (SITL) which allows us to test the functionality of one or more drones from our laptops without needing physical access to the real drones. To be specific the SITL literally runs the exact same firmware that would be running on the Pixhawk, and not some simulated version. Often a SITL is paired with a simulator of some description (by default gazebo) to provide physics input to simulate sensors and other external devices. However for the purpose of this workshop, we have decided to introduce Hardware-In-The-Loop (HITL) testing. This is often a later step of development testing when you have written new firmware, or simply want to make sure the physical PCBs and microcontrollers are functioning as expected. In HITL testing, a real Pixhawk is plugged into a computer, where the computer again uses a simulator to simulate physics and sensors which is passed back into the Pixhawk. Cruically this hopefully avoids computer setup issues for software while allowing real-time testing on actual hardware. To make this a bit different, and to hopefully avoid compatibility issues with other work, we have decided to recommend the use of JMavSim simulator instead of Gazebo for this session. JMavSim is another simulator specifically designed for fixed wing and multi-rotor platforms, but does not include the general physics capabilities that Gazebo includes. Because of this it is much lighter weight, and there have been works which run JMavSim onboard in the loop for model-predictive-control predictions. Read about it here (along with keyboard controls): https://github.com/PX4/jMAVSim Installing Dependencies \u00b6 Ensure you have QGC installed with joystick enabled For non-Ubuntu, please see the following instructions: https://docs.px4.io/main/en/dev_setup/dev_env.html Although Be Aware That It May Mess With Your Current Installation of Gazebo Options are: - Docker (and forwarding the volume representing the Pixhawk) - Following the cherry-picked Ubuntu instructions below Would not recommend a virtual machine since we are connecting real hardware up. (Unless you just want to try out the SITL) Ubuntu \u00b6 Git clone and install PX4 git clone --recursive https://github.com/PX4/PX4-Autopilot.git -b v1.15.4 cd PX4-Autopilot NOTE : You will need the --recursive to ensure all the dependencies are also pulled. If you forgot to and your builds are failing, in the root of the repository run git submodule update --init --recursive . JMavSim Install (Ubuntu Tested) \u00b6 # check ubuntu version # otherwise warn and point to docker? UBUNTU_RELEASE=\"`lsb_release -rs`\" if [[ \"${UBUNTU_RELEASE}\" == \"14.04\" ]]; then echo \"Ubuntu 14.04 is no longer supported\" exit 1 elif [[ \"${UBUNTU_RELEASE}\" == \"16.04\" ]]; then echo \"Ubuntu 16.04 is no longer supported\" exit 1 elif [[ \"${UBUNTU_RELEASE}\" == \"18.04\" ]]; then echo \"Ubuntu 18.04\" elif [[ \"${UBUNTU_RELEASE}\" == \"20.04\" ]]; then echo \"Ubuntu 20.04\" elif [[ \"${UBUNTU_RELEASE}\" == \"22.04\" ]]; then echo \"Ubuntu 22.04\" fi # General simulation dependencies sudo DEBIAN_FRONTEND=noninteractive apt-get -y --quiet --no-install-recommends install \\ bc \\ ; if [[ \"${UBUNTU_RELEASE}\" == \"18.04\" ]]; then java_version=11 elif [[ \"${UBUNTU_RELEASE}\" == \"20.04\" ]]; then java_version=13 elif [[ \"${UBUNTU_RELEASE}\" == \"22.04\" ]]; then java_version=11 else java_version=14 fi # Java (jmavsim) sudo DEBIAN_FRONTEND=noninteractive apt-get -y --quiet --no-install-recommends install \\ ant \\ openjdk-$java_version-jre \\ openjdk-$java_version-jdk \\ libvecmath-java \\ ; # Set Java 11 as default sudo update-alternatives --set java $(update-alternatives --list java | grep \"java-$java_version\") Steps to Run PX4 HITL: \u00b6 These instructions here are based on the following document: https://docs.px4.io/main/en/simulation/hitl.html They have been updated and adjusted Starting Pixhawk \u00b6 Using the USB cable, plug the pixhawk into your computer. Now start QGroundControl - if you are lucky after a few seconds, QGC should automatically detect your drone. If not, it could be anything from the cable you are using to your OS not assigning the serial port correctly. Configuring your Pixhawk/PX4 installation for HITL \u00b6 Within QGC we need to setup PX4 into HITL mode - this follows the above linked guide Open Setup (click the Q in the top left) -> Vehicle Setup -> Safety Enable HITL mode by selecting Enabled from the HITL Enabled List Restart the Pixhawk (Unplug and plug it back in again) Go to Vehicle Setup again and go to Airframe Click on the Simulation airframe and ensure that \"HIL Quadcopter X\" is selected Note that we do not have an external radio so we will not be able to perform these steps (instead we use the QGC joysticks). There may be an error that comes up because of this - you are mostly okay to ignore these warning/error messages. In order to use QGC Joystick, you will need to change some parameters. Go to Vehicle Setup -> Parameters. Search and change the following. COM_RC_IN_MODE to \"RC and Joystick with fallback\". This allows joystick input and disables RC input checks. NAV_RCL_ACT to \"Disarm\". This ensures that no RC failsafe actions interfere when not running HITL with a radio control. Running JMAVSim \u00b6 Ensure QGC is closed first Open a terminal and navigate to the PX4_Autopilot repo you cloned earlier # Set Lat Long of UCL East export PX4_HOME_LAT=51.537668693830824 export PX4_HOME_LON=-0.012029639288719024 export PX4_HOME_ALT=28.5 # Run JMavSim ./Tools/simulation/jmavsim/jmavsim_run.sh -q -s -d /dev/ttyACM0 -b 921600 -r 250 If you get an error make sure you have recursively git cloned INFO Replace the serial port name /dev/ttyACM0 as appropriate. On macOS this port would be /dev/tty.usbmodem1. On Windows (including Cygwin) it would be the COM1 or another port - check the connection in the Windows Device Manager. Once started, now separately (in a new terminal or otherwise) restart QGC. Tada! After waiting about a minute for the system to initialise itself (It will complain at you a bunch), QGC should show that the system is ready to fly. Tasks \u00b6 Takeoff the drone and manually fly it around the olympic park using the joysticks in the bottom of the screen See your trajectory in QGC Try some of the keyboard shortcuts of JMavSim to see your drone fly differently Note: If you crash your drone somehow, you may need to restart your flight controller and jmavsim When in midair, right click on a location and you will get some options to fly in a straight line to various locations. Land or return to the start location Have a play with the planning tools Once you have made a mission you will need to upload it (this may appear to fail) If you can't automatically start it, you may need to manually change the mode to mission . Imagine you were a drone inspection company, do you think this software is suitable for your role? What's this software good at? What's it missing? Summary \u00b6 In this tutorial, we introduced the following Pixhawk : The hardware autopilot platform PX4 : The open-source flight control firmware How to set up and use PX4 with Pixhawk PX4 Hardware In The Loop (HITL) Testing Creating Missions with QGC For more details, refer to the PX4 Developer Guide . This is all we'll cover in this course, as this content could fill a whole other module. In the optional session we will be trying to put some of this into practice on a real drone in the flight arena. Specifically connecting some of this up to ROS2 and Aerostack2 and seeing how this performs!","title":"Introducing Pixhawk and PX4"},{"location":"6_practical_pixhawk_PX4/#practical-6-pixhawk-and-px4-commercial-and-rd-autonomous-drone-systems","text":"Practical 6: Pixhawk and PX4, Commercial and R&D Autonomous Drone Systems Overview What is Pixhawk? Common Pixhawk Versions: How to use a Pixhawk Flight Control Firmware PX4 Software Stack Autopilot communication Ground Stations Setting Up Pixhawk with PX4 and QGroundControl Setup Overview Install QGroundControl Power up and connect the Pixhawk Flashing the Pixhawk Sensor Calibration Continue Setup Next step Using PX4 in HITL (Hardware-In-The-Loop) Simulation Installing Dependencies Ubuntu JMavSim Install (Ubuntu Tested) Steps to Run PX4 HITL: Starting Pixhawk Configuring your Pixhawk/PX4 installation for HITL Running JMAVSim Tasks Summary","title":"Practical 6: Pixhawk and PX4, Commercial and R&amp;D Autonomous Drone Systems"},{"location":"6_practical_pixhawk_PX4/#overview","text":"Pixhawk and PX4 are widely used in autonomous drone development, providing an open-source hardware and software ecosystem for UAV control. In this tutorial, we will introduce: Pixhawk : The hardware autopilot platform PX4 : The open-source flight control firmware How to set up and use PX4 with Pixhawk PX4 Hardware In The Loop (HITL) Testing Creating Missions with QGC This will help you transition from Crazyflies and Aerostack2 to more advanced UAV platforms.","title":"Overview"},{"location":"6_practical_pixhawk_PX4/#what-is-pixhawk","text":"Pixhawk is a family of open-source flight controllers developed for UAVs and robotics applications. It provides: High-performance autopilot capabilities Multiple sensor support (GPS, IMUs, barometers, etc.) Compatibility with PX4 and ArduPilot firmware Connectivity options like UART, I2C, CAN, MAVLink and DDS","title":"What is Pixhawk?"},{"location":"6_practical_pixhawk_PX4/#common-pixhawk-versions","text":"Pixhawk 4 : Standard for academic and research use Pixhawk 6X : More advanced processing capabilities Pixhawk 6C : Compact version with strong processing power Pixhawk Mini : Compact version for small UAVs","title":"Common Pixhawk Versions:"},{"location":"6_practical_pixhawk_PX4/#how-to-use-a-pixhawk","text":"This is the Pixhawk 6C you will be playing around with, see all of the ports for various inputs This following wiring diagram is from an older Pixhawk model, but should show the key elements of connecting up and powering a Pixhawk","title":"How to use a Pixhawk"},{"location":"6_practical_pixhawk_PX4/#flight-control-firmware","text":"There is no universal controller design of converting from user inputs to motor thrust. In the same way, there are numerous other functionalities that an autopilot can cover. These can range from running control loops for gimbals, cameras and other actuation, to high level mission following and safety features. These functionalities are bundled into specific autopilot firmware which each offer a slightly different set of features, as well as differing user interfaces each with their advantages and drawbacks. The two current most common autopilot firmware's in use in research settings are Ardupilot which offers the Arducopter firmware, and PX4 which offers Multicopter firmware. Both these firmwares are very extensive and cover numerous use cases. However, for our purposes we will only cover enabling autonomous flight through observing the mode of the autopilot. Both Ardupilot and PX4 use the concept of flight modes, where each mode operates and supports different levels or types of flight stabilisation and/or autonomous functions. Traditionally this is for pilots to change between different controller layouts for different applications. It's necessary to change to the correct mode for safe and controllable flight. The following table shows the most often used flight modes within Starling. Ardupilot Mode PX4 Mode Functionality stabilized manual Full manual control with RC sticks being sent directly to control roll, pitch, yaw and height PosHold position UAV uses onboard sensing to stay in place, RC sticks used to translate position loiter auto.hold Automatic mode where UAV stays in the same location until further instructions given. land auto.land Automatic mode which attempts to land the UAV Guided offboard Navigates to setpoints sent to it by ground control or companion computer As mentioned before, the base purpose of the firmware is to provide a given cascading PID controller for converting high level commands to motor thrusts. However both firmwares provide a plethora of other functionality from trajectory following, basic mission following, telemetry and communications and many others too. As a controller developer, it is also useful to understand the differences between the Ardupilot and PX4 controllers and what real-world impacts they have. In most of drone targeted applications we only require either position or velocity control which works fairly consistently between the two firmwares. In our own work, it has generally been noted that Ardupilot seems to be more suitable for outdoor flight, and PX4 for indoor flight. For this tutorial we will be developing a controller for indoor multi-vehicle flight and so we will assume the use of PX4. The biggest difference is actually in licensing where Ardupilot's license specifies that any developments must be contributed back, however PX4 is a bit more free allowing the forking and commercialisation without needing the announce or contribute (although this is not necessarily the best for the longevity of this open source project!).","title":"Flight Control Firmware"},{"location":"6_practical_pixhawk_PX4/#px4-software-stack","text":"PX4 consists of: PX4 Firmware - The core flight control software QGroundControl (QGC) - GUI for setup and mission planning MAVSDK (or other communication method) - API for developing drone applications Hardware-In-The-Loop (HITL) - Using real Pixhawk hardware for simulation or Software-In-The-Loop (SITL) for using simulated drones for testing.","title":"PX4 Software Stack"},{"location":"6_practical_pixhawk_PX4/#autopilot-communication","text":"Once in guided or offboard mode, the autopilot by default expects communications using the MAVLINK protocol . Traditionally this would have been used for a ground control station (GCS) to send commands to a UAV over a telemetry link. However, now it has also developed into a protocol for commanding the autopilot from an onboard companion computer over a USB or serial connection too. The MAVLink protocol is a set of preset commands which compatible firmwares understand and react to. In this tutorial we will primarily be observing the MAVlink interface, but we will not get into writing your own software with MAVlink yet. With the growing prevalence of ROS2, all of the major firmwares have attempted to provide direct communication using ROS2's underlying communication/middleware protocol of DDS (Data Distribution Service). Although we will not cover this in this module, this is what we currently use to communicate between a companion computer and the Pixhawk for drone work. Note however that even if DDS is enabled, autopilots will still send Mavlink messages down a Telemetry stream - often a dedicated low bandwidth communication channel for the monitoring of the drone's health and status.","title":"Autopilot communication"},{"location":"6_practical_pixhawk_PX4/#ground-stations","text":"A key element of any drone operation is a competent and reliable ground station setup. The purpose of the ground station is to provide a reliable communication link with the drone and receive telemetry to be able to keep an eye on, and monitor the status of the drone. Often this is done via a second telemtry radio operating on a different frequency - in our case an SIK radio on 433MHz - with one end connected to the drone, and the other connected via USB to a laptop or computer (or handheld games console like the steam deck). There are a number of choices for a piece of software which runs on the ground station. In industry, you may see Alterion or other softwares which provide an interface. In the open source world we either use mission planner with Ardupilot or QGroundControl with PX4. In this tutorial we will trying to use QGroundControl (QGC). Tools like QGC are crucial for real-time flight monitoring, control, and mission planning . It allows users to: Monitor telemetry data (battery levels, GPS status, IMU readings, etc.). Send flight commands such as takeoff, landing, and mission execution. Configure drone parameters including PID tuning, sensor calibrations, and failsafe settings. Visualize the drone's location and trajectory on a map interface. Log and analyze flight data to debug issues or optimize performance. Ground stations are useful as they enable: Safety : Enables real-time monitoring, preventing potential failures. Ease of Use : Provides a user-friendly interface to interact with the drone. Flexibility : Supports different flight modes (manual, autonomous, guided, etc.). Data Logging : Essential for post-flight analysis, AI training, and debugging. Remote Control : Operate the drone without requiring direct physical interaction.","title":"Ground Stations"},{"location":"6_practical_pixhawk_PX4/#setting-up-pixhawk-with-px4-and-qgroundcontrol","text":"Now let's set up your Pixhawk with PX4, connect to it and do some initial calibrations with QGC.","title":"Setting Up Pixhawk with PX4 and QGroundControl"},{"location":"6_practical_pixhawk_PX4/#setup-overview","text":"Install QGroundControl (QGC) ( Download ) Flash PX4 firmware onto Pixhawk via QGC Configure sensors and RC calibration Use QGC to set up some automated flights (Not today) Setup offboard control (for AI/robotics integration)","title":"Setup Overview"},{"location":"6_practical_pixhawk_PX4/#install-qgroundcontrol","text":"Follow the following instructions for all platforms: https://docs.qgroundcontrol.com/master/en/qgc-user-guide/getting_started/quick_start.html","title":"Install QGroundControl"},{"location":"6_practical_pixhawk_PX4/#power-up-and-connect-the-pixhawk","text":"Tidily unbox the Pixhawk box in a way that you can put everything back later! For this tutorial, all you will need is: Pixhawk USB-C cable Plug the USB-C cable from the Pixhawk to a port in your laptop Now start QGroundControl In the top left it will hopefully automatically pick up the Pixhawk and you can start browsing the settings and seeing the live view. See https://docs.qgroundcontrol.com/master/en/qgc-user-guide/getting_started/quick_start.html for how to navigate and use QGC.","title":"Power up and connect the Pixhawk"},{"location":"6_practical_pixhawk_PX4/#flashing-the-pixhawk","text":"The first step for us is to flash the latest PX4 version onto the pixhawk. This can be done easily from within QGC. Follow the following instruction page: https://docs.qgroundcontrol.com/master/en/qgc-user-guide/setup_view/firmware.html","title":"Flashing the Pixhawk"},{"location":"6_practical_pixhawk_PX4/#sensor-calibration","text":"The next step would be to perform sensor calibration in order to calibrate the onboard accelerometer and gyroscope. In the setup menu go to the sensors tab and follow the instructions. Of course in practice you would perform the calibration after having built the drone, but this will do for now!","title":"Sensor Calibration"},{"location":"6_practical_pixhawk_PX4/#continue-setup","text":"Scroll through the other setup steps, there are a number that we are skipping in this session, but in practice you would have to usually complete: Radio Setup : It's standard to have a radio connected with both a receiver connected to the pixhawk and a transmitter with the dedicated pilot. The radio is important in order to have a backup method of controlling the drone, as well enabling mode switching between various manual flight regimes and offboard mode. Important for safety too as having a emergency stop setup is highly recommended Safety Setup : All the different safety systems built in from return to home to parachutes if they're installed. Parameters : All of these setup screens essentially manipulate the value of various parameters under the hood. Have a scroll through the parameter list and you will quickly see that there are a lot of options for a variety of scenarios. Bear in mind that PX4 is highly multi-functional as it has been written to work on anything from fixed-wing aircraft to mini-submarines! You might want to try setting up the virtual joystick to enable easier testing! Joystick","title":"Continue Setup"},{"location":"6_practical_pixhawk_PX4/#next-step","text":"So once the setup is complete, the last thing to have a look at is observing some of the MAVLink that's coming through Go have a look at the Analyse View: analyse view , and then the mavlink inspector However you will note that because we have a \"drone\" (just the Pixhawk sitting on your desk that wont fly) you wont really be able to do much with it. Traditionally we would use Software In The Loop testing to simulate a virtual Pixhawk for testing and development. However in this class we would like to try and use Hardware-In-The-Loop testing to \"fly\" your drone.","title":"Next step"},{"location":"6_practical_pixhawk_PX4/#using-px4-in-hitl-hardware-in-the-loop-simulation","text":"For development and testing in industry, we often use Software-In-The-Loop (SITL) which allows us to test the functionality of one or more drones from our laptops without needing physical access to the real drones. To be specific the SITL literally runs the exact same firmware that would be running on the Pixhawk, and not some simulated version. Often a SITL is paired with a simulator of some description (by default gazebo) to provide physics input to simulate sensors and other external devices. However for the purpose of this workshop, we have decided to introduce Hardware-In-The-Loop (HITL) testing. This is often a later step of development testing when you have written new firmware, or simply want to make sure the physical PCBs and microcontrollers are functioning as expected. In HITL testing, a real Pixhawk is plugged into a computer, where the computer again uses a simulator to simulate physics and sensors which is passed back into the Pixhawk. Cruically this hopefully avoids computer setup issues for software while allowing real-time testing on actual hardware. To make this a bit different, and to hopefully avoid compatibility issues with other work, we have decided to recommend the use of JMavSim simulator instead of Gazebo for this session. JMavSim is another simulator specifically designed for fixed wing and multi-rotor platforms, but does not include the general physics capabilities that Gazebo includes. Because of this it is much lighter weight, and there have been works which run JMavSim onboard in the loop for model-predictive-control predictions. Read about it here (along with keyboard controls): https://github.com/PX4/jMAVSim","title":"Using PX4 in HITL (Hardware-In-The-Loop) Simulation"},{"location":"6_practical_pixhawk_PX4/#installing-dependencies","text":"Ensure you have QGC installed with joystick enabled For non-Ubuntu, please see the following instructions: https://docs.px4.io/main/en/dev_setup/dev_env.html Although Be Aware That It May Mess With Your Current Installation of Gazebo Options are: - Docker (and forwarding the volume representing the Pixhawk) - Following the cherry-picked Ubuntu instructions below Would not recommend a virtual machine since we are connecting real hardware up. (Unless you just want to try out the SITL)","title":"Installing Dependencies"},{"location":"6_practical_pixhawk_PX4/#ubuntu","text":"Git clone and install PX4 git clone --recursive https://github.com/PX4/PX4-Autopilot.git -b v1.15.4 cd PX4-Autopilot NOTE : You will need the --recursive to ensure all the dependencies are also pulled. If you forgot to and your builds are failing, in the root of the repository run git submodule update --init --recursive .","title":"Ubuntu"},{"location":"6_practical_pixhawk_PX4/#jmavsim-install-ubuntu-tested","text":"# check ubuntu version # otherwise warn and point to docker? UBUNTU_RELEASE=\"`lsb_release -rs`\" if [[ \"${UBUNTU_RELEASE}\" == \"14.04\" ]]; then echo \"Ubuntu 14.04 is no longer supported\" exit 1 elif [[ \"${UBUNTU_RELEASE}\" == \"16.04\" ]]; then echo \"Ubuntu 16.04 is no longer supported\" exit 1 elif [[ \"${UBUNTU_RELEASE}\" == \"18.04\" ]]; then echo \"Ubuntu 18.04\" elif [[ \"${UBUNTU_RELEASE}\" == \"20.04\" ]]; then echo \"Ubuntu 20.04\" elif [[ \"${UBUNTU_RELEASE}\" == \"22.04\" ]]; then echo \"Ubuntu 22.04\" fi # General simulation dependencies sudo DEBIAN_FRONTEND=noninteractive apt-get -y --quiet --no-install-recommends install \\ bc \\ ; if [[ \"${UBUNTU_RELEASE}\" == \"18.04\" ]]; then java_version=11 elif [[ \"${UBUNTU_RELEASE}\" == \"20.04\" ]]; then java_version=13 elif [[ \"${UBUNTU_RELEASE}\" == \"22.04\" ]]; then java_version=11 else java_version=14 fi # Java (jmavsim) sudo DEBIAN_FRONTEND=noninteractive apt-get -y --quiet --no-install-recommends install \\ ant \\ openjdk-$java_version-jre \\ openjdk-$java_version-jdk \\ libvecmath-java \\ ; # Set Java 11 as default sudo update-alternatives --set java $(update-alternatives --list java | grep \"java-$java_version\")","title":"JMavSim Install (Ubuntu Tested)"},{"location":"6_practical_pixhawk_PX4/#steps-to-run-px4-hitl","text":"These instructions here are based on the following document: https://docs.px4.io/main/en/simulation/hitl.html They have been updated and adjusted","title":"Steps to Run PX4 HITL:"},{"location":"6_practical_pixhawk_PX4/#starting-pixhawk","text":"Using the USB cable, plug the pixhawk into your computer. Now start QGroundControl - if you are lucky after a few seconds, QGC should automatically detect your drone. If not, it could be anything from the cable you are using to your OS not assigning the serial port correctly.","title":"Starting Pixhawk"},{"location":"6_practical_pixhawk_PX4/#configuring-your-pixhawkpx4-installation-for-hitl","text":"Within QGC we need to setup PX4 into HITL mode - this follows the above linked guide Open Setup (click the Q in the top left) -> Vehicle Setup -> Safety Enable HITL mode by selecting Enabled from the HITL Enabled List Restart the Pixhawk (Unplug and plug it back in again) Go to Vehicle Setup again and go to Airframe Click on the Simulation airframe and ensure that \"HIL Quadcopter X\" is selected Note that we do not have an external radio so we will not be able to perform these steps (instead we use the QGC joysticks). There may be an error that comes up because of this - you are mostly okay to ignore these warning/error messages. In order to use QGC Joystick, you will need to change some parameters. Go to Vehicle Setup -> Parameters. Search and change the following. COM_RC_IN_MODE to \"RC and Joystick with fallback\". This allows joystick input and disables RC input checks. NAV_RCL_ACT to \"Disarm\". This ensures that no RC failsafe actions interfere when not running HITL with a radio control.","title":"Configuring your Pixhawk/PX4 installation for HITL"},{"location":"6_practical_pixhawk_PX4/#running-jmavsim","text":"Ensure QGC is closed first Open a terminal and navigate to the PX4_Autopilot repo you cloned earlier # Set Lat Long of UCL East export PX4_HOME_LAT=51.537668693830824 export PX4_HOME_LON=-0.012029639288719024 export PX4_HOME_ALT=28.5 # Run JMavSim ./Tools/simulation/jmavsim/jmavsim_run.sh -q -s -d /dev/ttyACM0 -b 921600 -r 250 If you get an error make sure you have recursively git cloned INFO Replace the serial port name /dev/ttyACM0 as appropriate. On macOS this port would be /dev/tty.usbmodem1. On Windows (including Cygwin) it would be the COM1 or another port - check the connection in the Windows Device Manager. Once started, now separately (in a new terminal or otherwise) restart QGC. Tada! After waiting about a minute for the system to initialise itself (It will complain at you a bunch), QGC should show that the system is ready to fly.","title":"Running JMAVSim"},{"location":"6_practical_pixhawk_PX4/#tasks","text":"Takeoff the drone and manually fly it around the olympic park using the joysticks in the bottom of the screen See your trajectory in QGC Try some of the keyboard shortcuts of JMavSim to see your drone fly differently Note: If you crash your drone somehow, you may need to restart your flight controller and jmavsim When in midair, right click on a location and you will get some options to fly in a straight line to various locations. Land or return to the start location Have a play with the planning tools Once you have made a mission you will need to upload it (this may appear to fail) If you can't automatically start it, you may need to manually change the mode to mission . Imagine you were a drone inspection company, do you think this software is suitable for your role? What's this software good at? What's it missing?","title":"Tasks"},{"location":"6_practical_pixhawk_PX4/#summary","text":"In this tutorial, we introduced the following Pixhawk : The hardware autopilot platform PX4 : The open-source flight control firmware How to set up and use PX4 with Pixhawk PX4 Hardware In The Loop (HITL) Testing Creating Missions with QGC For more details, refer to the PX4 Developer Guide . This is all we'll cover in this course, as this content could fill a whole other module. In the optional session we will be trying to put some of this into practice on a real drone in the flight arena. Specifically connecting some of this up to ROS2 and Aerostack2 and seeing how this performs!","title":"Summary"},{"location":"7_challenge_multi_drone/","text":"CW2: Swarming and Multi-Drone Formation Flight Challenge \u00b6 Challenge \u00b6 This challenge revolves around swarming of drones and formation flight. We are asking you to investigate the difference in performance and application of decentralised swarm based approaches versus centralised approaches to performing formation flight with a group of 5 drones (minimum 3 if experiencing technical issues) in 4 different scenario stages. Swarming and Formation Flight \u00b6 Swarming refers to the collective behaviour of multiple agents (drones) operating together using local rules without a centralised controller. These behaviours emerge from interactions between individual drones and their environment. Swarm robotics takes inspiration from nature\u2014such as birds, fish, and insects\u2014to design scalable, flexible, and robust robotic systems. Swarm behaviours are often decentralised and self-organised, meaning that individual drones follow simple local rules that collectively result in a global pattern of behaviour. Common decentralised swarm control strategies: Boids Model (Flocking Behaviour) \u2013 Uses three simple rules: separation, alignment, and cohesion. Potential Fields \u2013 Assigns virtual attractive/repulsive forces to goals and obstacles to guide movement. Optimisation-based Swarms \u2013 Optimisation approach focusing on local decision-making given a set of constraints. Bio-Inspired Methods \u2013 Use of indirect coordination through share environmental cues such as pheromone-based navigation or genetic algorithms. Formation flight is a more structured approach to multi-agent coordination where drones maintain a specific geometric arrangement while moving. Unlike general swarming, formation flying often requires precise positioning and coordination. Common formation flight strategies: Centralised Approaches Leader-Follower \u2013 One drone acts as the leader dictating the trajectory while others maintain a relative position. This can be both centralised and de-centralised. For the latter, this introduces approaches where the swarm my automatically elect a new leader. Multi-Agent Path Planning (MAPF) \u2013 Global centralised planning approach computed for the whole route and optimising e.g. for collision-free paths. Virtual Structures \u2013 The entire formation is treated as a rigid body and controlled as one unit. Decentralised Approaches Boids with Formation Constraints \u2013 Similar to flocking but with additional formation control. Consensus-Based Control \u2013 Drones agree on formation changes based on local communication. Distributed Potential Fields \u2013 Drones use attraction/repulsion forces while maintaining formation. Swarming and formation flight have numerous real-world applications across various industries. In aerial surveillance and search-and-rescue, swarming drones can quickly cover large areas, scan for missing persons, or assess disaster zones without relying on a single point of failure. In logistics and delivery, drone swarms can efficiently transport packages in coordinated formations, optimizing airspace usage and reducing delivery times. In environmental monitoring, swarms of drones can track wildlife migrations, detect deforestation, or monitor air and water quality over vast regions. In entertainment and art, synchronized drone light shows use precise formation flight to create complex aerial displays, offering an innovative alternative to fireworks. These examples highlight the versatility of swarm robotics in enhancing efficiency, scalability, and adaptability in real-world operations. Your Challenge \u00b6 We have created a competition style course with 4 different scenario stages to complete one after another. For each of these stages 1 to 4, you need to consider the coordination of 5 drones using either a centrialised and decentrialised method of your choosing. The aim is to route 5 drones through each scenario stage. If you have technical challenges, an acceptable minimum number of drones is 3. In groups of 2, you will be investigating, developing and testing your algorithms in simulation. Stage 1: Changing Formations : Implementing the formation flight algorithms which have the ability to changing the formation periodically whilst maintaining a circular trajectory. Compare different formation shapes (Line, V-shape, Diamond, Circular Orbit, Grid, Staggered) Stage 2: Window Traversal : Using your formation flying methods attempt to maneouever your swarm of drones through two narrow windows slits. Consider how to split, rejoin, or compress the formation to pass through gaps. Stage 3: Forest Traversal : Using your formation flying methods attempt to maneouever your swarm of drones through a forest of trees. Your swarm should avoid collisions and maintain efficiency in movement. Stage 4: Dynamic Obstacles : Using your formation flying methods attempt to maneouever your swarm of drones through a set of dynamically moving obstacles. You may need adaptive formation control to respond to changes in real time. Hardware Challenge Event at HereEast 26th March \u00b6 In your groups, you will be given the opportunity to run a viable solution on real crazyflies on the 26th March and / or 2nd April at UCL HereEast - this is an optional non-assessed task. I encourage as many groups as possible to join the event. We will be maintaining a leaderboard and points will be awarded based on success rate, reconfigurablity and time taken. A sign up sheet will be provided. As an incentive to run your solution on hardware at UCL HereEast, groups have the chance to complete scenario stages 2 and or 3 depending on logistics and the number of crashes we have on the day. A small, low value prize will be provided to the winning solution. The event will be recorded. Regardless of whether you are competing or not, I encourage students to attend the sessions to support your fellow colleagues. Challenge Environment \u00b6 Just as with the previous courseworks and gazebo aruco challenges, this will also be within aerostack2 and ros2. At this point you should feel more familiar with the systems and how they all work. To enable this challenge, we have created a new repository specifically for this challenge. The repository is here: challenge_multi_drone https://github.com/UCL-MSC-RAI-COMP0240/challenge_multi_drone Installation \u00b6 As before, follow the [README.md] in the challenge repository for installation instructions. NOTE that we have made a patch for aerostack2 and the crazyflie interface and you will therefore have to pull our versions of these two libraries and build them from source yourself. Running the environment \u00b6 The README.md goes into full details of each command. Many things are the same as your previous courseworks. However a key difference is that we are now running multiple drones. As you can imagine this is much more taxing on your machines, and some approaches to running this coursework may no longer be appropriate. Like previous you will need two terminals, both in the root of the repository: In terminal 1 run the following to launch the drone simulation of scenario1, and you will see gazebo popup. ./launch_as2.bash -s scenarios/scenario1.yaml And in a second terminal, you can run the ground station, which will spin up the visualisation software rviz2 as well as provide a prompt for you to run your mission (Though you can run your python mission from everywhere, provided you source /<your workspace>/install/setup.bash first) ./launch_ground_station.bash # or if you want to play around with teleoperating the drone ./launch_ground_station.bash -t Remember : If you want to close the simulation down, just run the stop.bash script from the repository root in any terminal or tmux window. Bulding your solution \u00b6 We have released a scenario1.yaml which describes a sample environment which contains the 4 stages mentioned within this challenge. Have a read of the scenarios file to see some of the parameters you will need to manage. TODO : More scenarios will be released shortly for testing. Recommended Tasks \u00b6 Understand the Problem and Setup : Familiarise yourself with the coursework brief Carefully read the challenge description, objectives, and assessment criteria. Understand the differences between centralised formation flight and decentralised swarm control. Set up the simulation environment Clone the challenge_multi_drone repository from GitHub. Follow the README instructions to install dependencies and patches for Aerostack2 and Crazyflie interfaces. Run a basic test to ensure Gazebo and RViz2 are launching correctly. The aim is to route 5 drones through each scenario stage. If you have technical challenges, an acceptable minimum number of drones is 3. Analyse the provided scenario1.yaml file Understand the environment parameters (waypoints, obstacles, formation constraints). Identify key parameters you may need to tune for different stages. Try not to hard core your solutions, read the initial parameters from the yaml file. Implementing Baseline Formation Flight Strategies : Stage 1: Implement basic formation flight Choose and implement 1 centrailised approach e.g. leader-Follower model, global multi-agent planning or virtual structure Choose and implement 1 decentralised approach e.g. boids-based, consensus-based control, potential fields Compare different formation shapes (Line, V-shape, Diamond, Circular Orbit, Grid, Staggered). Test formation transitions Implement periodic formation shape changes while maintaining a circular trajectory. Ensure smooth transitions between formations. Developing Advanced Adaptive Formation Control Stage 2: Implement Window Traversal strategies Design methods to split, rejoin, or compress the formation to pass through narrow gaps. Compare centralised and decentralised methods for managing formation changes. Stage 3: Implement Forest Traversal strategies Introduce collision avoidance mechanisms while maintaining formation integrity. Test how different formation control methods handle navigation through obstacles. Stage 4: Implement Dynamic Obstacle Avoidance Introduce adaptive reconfiguration to respond to moving obstacles in real-time. Optimise decision-making for obstacle avoidance without breaking formation integrity. Performance Evaluation and Optimisation Compare performance of centralised vs decentralised approaches for each of the stages 1-4. Evaluate formation stability, reconfigurability, and efficiency in each scenario. Measure performance metrics such as completion time, success rate, and communication overhead. Optimise computational efficiency Reduce latency and improve real-time response. Tune formation parameters and control gains for smoother transitions. Document findings for coursework submission Provide an analysis comparing both approaches, their strengths, and limitations. Include performance graphs, data plots, and insights from the simulation trials. Discuss real-world applications and trade-offs in different industry scenarios. Suggested Group Task Assignments \u00b6 To ensure a fair workload distribution, each group member should be responsible for complementary tasks while collaborating on integration, testing, and final documentation. Our suggested task division strategy is to split by the development of centralised and decentralised algorithms, allowing flexibility based on skillsets and interests. E.g.: Student 1 (Centralised Control) Implement 1 algorithm to solve the stages e.g. Leader-Follower model, Global Multi-agent Planning or Virtual Structure Test formation transitions using predefined trajectory control. Evaluate control performance for formation maintenance and obstacle avoidance. Student 2 (Decentralised Swarm Methods) Implement 1 algorithm to solve the stages e.g. Boids Model, Potential Fields, and Consensus-Based Control. Develop an adaptive swarm behaviour that responds to environmental constraints. Fine-tune local interaction rules to optimise coordination and robustness. Regardless of the distribution of tasks, groups then need to jointly come together and share their algorithms: - Compare centralised versus decentralised performance across the different scenarios - Ensure solutions work in each scenario - Prepare a report with performance metrics, trade-off analysis and conclusions. Coursework Submission \u00b6 The deadline for this coursework is 23rd April 2025, 16.00. Submissions will uploaded into Moodle using the relevant Coursework 2 submission links. You will need to submit 3 links on moodle items: Coursework 2 report: This report should be structured to clearly explain the problem, methodology, implementation, results and conclusions. 4min - Video Presentation: Narrative, demonstration and summary of results Code submission Assessment Breakdown and Structure (Deadline 23rd April 16.00) \u00b6 Marking will be based on your submitted report, video and code only. Report Structure (total page count 10 - ~6000 words limit, 60% of total grade) Introduction (10%) Brief Overview: Explain the purpose of the coursework and significance of drone swarming. Problem Statement: What are the challenges in multi-UAV formation flight? Objectives: Define the goal of the report (e.g., evaluating centralised vs. decentralised approaches). Methodology (30%) Formation Flight Strategies: Centralised Approach: Implement one method (e.g., Leader-Follower, Virtual Structures, or Multi-Agent Path Planning). Decentralised Approach: Implement one method (e.g., Boids Model, Potential Fields, or Consensus-Based Control). Swarm Coordination Mechanisms: How do the UAVs maintain formation and avoid collisions? How does formation reconfiguration (splitting/rejoining) work? Implementation Process: Algorithm Design: Explain key design choices. Simulation Environment: Describe ROS2, Aerostack2, and scenario1.yaml configuration. Tuning of Parameters: Explain how you adjusted parameters to optimise performance. Experimental Results & Performance Evaluation (25%) (~3-4 Pages) Comparative Analysis: Stage 1 - Formation Changes: Success rate of formation transitions. Time taken for different formation types (e.g., Line, V-shape, Diamond). Time taken for completion. Stage 2 - Window Traversal: Success rate (%) of drones passing through gaps, number of collisions. Time taken to rejoin formations. Time taken for completion. Stage 3 - Forest Traversal: Success rate, number of collisions. Efficiency of obstacle avoidance while maintaining formation. Time taken for completion. Stage 4 - Dynamic Obstacles: Adaptive formation behaviour against moving obstacles. Success rate, number of collisions. Time taken for completion. Discussion (20%) Key Insights: Strengths and weaknesses of centralised vs. decentralised approaches. How formation stability, obstacle handling, and adaptability differ between methods. Computational efficiency and real-time response. Challenges & Mitigations: Issues encountered in implementation, debugging, or parameter tuning. Solutions applied to overcome performance bottlenecks. Future Improvements: Ideas for improving UAV swarm coordination. Possible hybrid approaches that combine centralised and decentralised control. Conclusion (10%) Summary of Findings: Key takeaways from experiments. Final Performance Insights: Which method worked best? Under what conditions? Real-World Applications: Discuss how these methods can be used in disaster response, logistics, or surveillance. Contribution Statement (5%) Summary of your individual contributions to the report and video submission. **Video Submission (40% of total grade) ** Length: 5mins Format: MP4, max 500mb Required Elements: Formation Flight Demonstration \u2013 Show formation transitions in the simulator. Obstacle Traversal \u2013 Showcase Window and Forest Traversal. Side-by-Side Comparison \u2013 Compare Centralised vs. Decentralised approaches. Performance Metrics Overlay \u2013 Display success rates and efficiency stats. Brief Narration/Annotations \u2013 Explain the observed UAV behaviours. Conclusion/Summary of key results. Code Submission Zip your packages to upload","title":"Multi-Drone Swarming Formation Flight Challenge"},{"location":"7_challenge_multi_drone/#cw2-swarming-and-multi-drone-formation-flight-challenge","text":"","title":"CW2: Swarming and Multi-Drone Formation Flight Challenge"},{"location":"7_challenge_multi_drone/#challenge","text":"This challenge revolves around swarming of drones and formation flight. We are asking you to investigate the difference in performance and application of decentralised swarm based approaches versus centralised approaches to performing formation flight with a group of 5 drones (minimum 3 if experiencing technical issues) in 4 different scenario stages.","title":"Challenge"},{"location":"7_challenge_multi_drone/#swarming-and-formation-flight","text":"Swarming refers to the collective behaviour of multiple agents (drones) operating together using local rules without a centralised controller. These behaviours emerge from interactions between individual drones and their environment. Swarm robotics takes inspiration from nature\u2014such as birds, fish, and insects\u2014to design scalable, flexible, and robust robotic systems. Swarm behaviours are often decentralised and self-organised, meaning that individual drones follow simple local rules that collectively result in a global pattern of behaviour. Common decentralised swarm control strategies: Boids Model (Flocking Behaviour) \u2013 Uses three simple rules: separation, alignment, and cohesion. Potential Fields \u2013 Assigns virtual attractive/repulsive forces to goals and obstacles to guide movement. Optimisation-based Swarms \u2013 Optimisation approach focusing on local decision-making given a set of constraints. Bio-Inspired Methods \u2013 Use of indirect coordination through share environmental cues such as pheromone-based navigation or genetic algorithms. Formation flight is a more structured approach to multi-agent coordination where drones maintain a specific geometric arrangement while moving. Unlike general swarming, formation flying often requires precise positioning and coordination. Common formation flight strategies: Centralised Approaches Leader-Follower \u2013 One drone acts as the leader dictating the trajectory while others maintain a relative position. This can be both centralised and de-centralised. For the latter, this introduces approaches where the swarm my automatically elect a new leader. Multi-Agent Path Planning (MAPF) \u2013 Global centralised planning approach computed for the whole route and optimising e.g. for collision-free paths. Virtual Structures \u2013 The entire formation is treated as a rigid body and controlled as one unit. Decentralised Approaches Boids with Formation Constraints \u2013 Similar to flocking but with additional formation control. Consensus-Based Control \u2013 Drones agree on formation changes based on local communication. Distributed Potential Fields \u2013 Drones use attraction/repulsion forces while maintaining formation. Swarming and formation flight have numerous real-world applications across various industries. In aerial surveillance and search-and-rescue, swarming drones can quickly cover large areas, scan for missing persons, or assess disaster zones without relying on a single point of failure. In logistics and delivery, drone swarms can efficiently transport packages in coordinated formations, optimizing airspace usage and reducing delivery times. In environmental monitoring, swarms of drones can track wildlife migrations, detect deforestation, or monitor air and water quality over vast regions. In entertainment and art, synchronized drone light shows use precise formation flight to create complex aerial displays, offering an innovative alternative to fireworks. These examples highlight the versatility of swarm robotics in enhancing efficiency, scalability, and adaptability in real-world operations.","title":"Swarming and Formation Flight"},{"location":"7_challenge_multi_drone/#your-challenge","text":"We have created a competition style course with 4 different scenario stages to complete one after another. For each of these stages 1 to 4, you need to consider the coordination of 5 drones using either a centrialised and decentrialised method of your choosing. The aim is to route 5 drones through each scenario stage. If you have technical challenges, an acceptable minimum number of drones is 3. In groups of 2, you will be investigating, developing and testing your algorithms in simulation. Stage 1: Changing Formations : Implementing the formation flight algorithms which have the ability to changing the formation periodically whilst maintaining a circular trajectory. Compare different formation shapes (Line, V-shape, Diamond, Circular Orbit, Grid, Staggered) Stage 2: Window Traversal : Using your formation flying methods attempt to maneouever your swarm of drones through two narrow windows slits. Consider how to split, rejoin, or compress the formation to pass through gaps. Stage 3: Forest Traversal : Using your formation flying methods attempt to maneouever your swarm of drones through a forest of trees. Your swarm should avoid collisions and maintain efficiency in movement. Stage 4: Dynamic Obstacles : Using your formation flying methods attempt to maneouever your swarm of drones through a set of dynamically moving obstacles. You may need adaptive formation control to respond to changes in real time.","title":"Your Challenge"},{"location":"7_challenge_multi_drone/#hardware-challenge-event-at-hereeast-26th-march","text":"In your groups, you will be given the opportunity to run a viable solution on real crazyflies on the 26th March and / or 2nd April at UCL HereEast - this is an optional non-assessed task. I encourage as many groups as possible to join the event. We will be maintaining a leaderboard and points will be awarded based on success rate, reconfigurablity and time taken. A sign up sheet will be provided. As an incentive to run your solution on hardware at UCL HereEast, groups have the chance to complete scenario stages 2 and or 3 depending on logistics and the number of crashes we have on the day. A small, low value prize will be provided to the winning solution. The event will be recorded. Regardless of whether you are competing or not, I encourage students to attend the sessions to support your fellow colleagues.","title":"Hardware Challenge Event at HereEast 26th March"},{"location":"7_challenge_multi_drone/#challenge-environment","text":"Just as with the previous courseworks and gazebo aruco challenges, this will also be within aerostack2 and ros2. At this point you should feel more familiar with the systems and how they all work. To enable this challenge, we have created a new repository specifically for this challenge. The repository is here: challenge_multi_drone https://github.com/UCL-MSC-RAI-COMP0240/challenge_multi_drone","title":"Challenge Environment"},{"location":"7_challenge_multi_drone/#installation","text":"As before, follow the [README.md] in the challenge repository for installation instructions. NOTE that we have made a patch for aerostack2 and the crazyflie interface and you will therefore have to pull our versions of these two libraries and build them from source yourself.","title":"Installation"},{"location":"7_challenge_multi_drone/#running-the-environment","text":"The README.md goes into full details of each command. Many things are the same as your previous courseworks. However a key difference is that we are now running multiple drones. As you can imagine this is much more taxing on your machines, and some approaches to running this coursework may no longer be appropriate. Like previous you will need two terminals, both in the root of the repository: In terminal 1 run the following to launch the drone simulation of scenario1, and you will see gazebo popup. ./launch_as2.bash -s scenarios/scenario1.yaml And in a second terminal, you can run the ground station, which will spin up the visualisation software rviz2 as well as provide a prompt for you to run your mission (Though you can run your python mission from everywhere, provided you source /<your workspace>/install/setup.bash first) ./launch_ground_station.bash # or if you want to play around with teleoperating the drone ./launch_ground_station.bash -t Remember : If you want to close the simulation down, just run the stop.bash script from the repository root in any terminal or tmux window.","title":"Running the environment"},{"location":"7_challenge_multi_drone/#bulding-your-solution","text":"We have released a scenario1.yaml which describes a sample environment which contains the 4 stages mentioned within this challenge. Have a read of the scenarios file to see some of the parameters you will need to manage. TODO : More scenarios will be released shortly for testing.","title":"Bulding your solution"},{"location":"7_challenge_multi_drone/#recommended-tasks","text":"Understand the Problem and Setup : Familiarise yourself with the coursework brief Carefully read the challenge description, objectives, and assessment criteria. Understand the differences between centralised formation flight and decentralised swarm control. Set up the simulation environment Clone the challenge_multi_drone repository from GitHub. Follow the README instructions to install dependencies and patches for Aerostack2 and Crazyflie interfaces. Run a basic test to ensure Gazebo and RViz2 are launching correctly. The aim is to route 5 drones through each scenario stage. If you have technical challenges, an acceptable minimum number of drones is 3. Analyse the provided scenario1.yaml file Understand the environment parameters (waypoints, obstacles, formation constraints). Identify key parameters you may need to tune for different stages. Try not to hard core your solutions, read the initial parameters from the yaml file. Implementing Baseline Formation Flight Strategies : Stage 1: Implement basic formation flight Choose and implement 1 centrailised approach e.g. leader-Follower model, global multi-agent planning or virtual structure Choose and implement 1 decentralised approach e.g. boids-based, consensus-based control, potential fields Compare different formation shapes (Line, V-shape, Diamond, Circular Orbit, Grid, Staggered). Test formation transitions Implement periodic formation shape changes while maintaining a circular trajectory. Ensure smooth transitions between formations. Developing Advanced Adaptive Formation Control Stage 2: Implement Window Traversal strategies Design methods to split, rejoin, or compress the formation to pass through narrow gaps. Compare centralised and decentralised methods for managing formation changes. Stage 3: Implement Forest Traversal strategies Introduce collision avoidance mechanisms while maintaining formation integrity. Test how different formation control methods handle navigation through obstacles. Stage 4: Implement Dynamic Obstacle Avoidance Introduce adaptive reconfiguration to respond to moving obstacles in real-time. Optimise decision-making for obstacle avoidance without breaking formation integrity. Performance Evaluation and Optimisation Compare performance of centralised vs decentralised approaches for each of the stages 1-4. Evaluate formation stability, reconfigurability, and efficiency in each scenario. Measure performance metrics such as completion time, success rate, and communication overhead. Optimise computational efficiency Reduce latency and improve real-time response. Tune formation parameters and control gains for smoother transitions. Document findings for coursework submission Provide an analysis comparing both approaches, their strengths, and limitations. Include performance graphs, data plots, and insights from the simulation trials. Discuss real-world applications and trade-offs in different industry scenarios.","title":"Recommended Tasks"},{"location":"7_challenge_multi_drone/#suggested-group-task-assignments","text":"To ensure a fair workload distribution, each group member should be responsible for complementary tasks while collaborating on integration, testing, and final documentation. Our suggested task division strategy is to split by the development of centralised and decentralised algorithms, allowing flexibility based on skillsets and interests. E.g.: Student 1 (Centralised Control) Implement 1 algorithm to solve the stages e.g. Leader-Follower model, Global Multi-agent Planning or Virtual Structure Test formation transitions using predefined trajectory control. Evaluate control performance for formation maintenance and obstacle avoidance. Student 2 (Decentralised Swarm Methods) Implement 1 algorithm to solve the stages e.g. Boids Model, Potential Fields, and Consensus-Based Control. Develop an adaptive swarm behaviour that responds to environmental constraints. Fine-tune local interaction rules to optimise coordination and robustness. Regardless of the distribution of tasks, groups then need to jointly come together and share their algorithms: - Compare centralised versus decentralised performance across the different scenarios - Ensure solutions work in each scenario - Prepare a report with performance metrics, trade-off analysis and conclusions.","title":"Suggested Group Task Assignments"},{"location":"7_challenge_multi_drone/#coursework-submission","text":"The deadline for this coursework is 23rd April 2025, 16.00. Submissions will uploaded into Moodle using the relevant Coursework 2 submission links. You will need to submit 3 links on moodle items: Coursework 2 report: This report should be structured to clearly explain the problem, methodology, implementation, results and conclusions. 4min - Video Presentation: Narrative, demonstration and summary of results Code submission","title":"Coursework Submission"},{"location":"7_challenge_multi_drone/#assessment-breakdown-and-structure-deadline-23rd-april-1600","text":"Marking will be based on your submitted report, video and code only. Report Structure (total page count 10 - ~6000 words limit, 60% of total grade) Introduction (10%) Brief Overview: Explain the purpose of the coursework and significance of drone swarming. Problem Statement: What are the challenges in multi-UAV formation flight? Objectives: Define the goal of the report (e.g., evaluating centralised vs. decentralised approaches). Methodology (30%) Formation Flight Strategies: Centralised Approach: Implement one method (e.g., Leader-Follower, Virtual Structures, or Multi-Agent Path Planning). Decentralised Approach: Implement one method (e.g., Boids Model, Potential Fields, or Consensus-Based Control). Swarm Coordination Mechanisms: How do the UAVs maintain formation and avoid collisions? How does formation reconfiguration (splitting/rejoining) work? Implementation Process: Algorithm Design: Explain key design choices. Simulation Environment: Describe ROS2, Aerostack2, and scenario1.yaml configuration. Tuning of Parameters: Explain how you adjusted parameters to optimise performance. Experimental Results & Performance Evaluation (25%) (~3-4 Pages) Comparative Analysis: Stage 1 - Formation Changes: Success rate of formation transitions. Time taken for different formation types (e.g., Line, V-shape, Diamond). Time taken for completion. Stage 2 - Window Traversal: Success rate (%) of drones passing through gaps, number of collisions. Time taken to rejoin formations. Time taken for completion. Stage 3 - Forest Traversal: Success rate, number of collisions. Efficiency of obstacle avoidance while maintaining formation. Time taken for completion. Stage 4 - Dynamic Obstacles: Adaptive formation behaviour against moving obstacles. Success rate, number of collisions. Time taken for completion. Discussion (20%) Key Insights: Strengths and weaknesses of centralised vs. decentralised approaches. How formation stability, obstacle handling, and adaptability differ between methods. Computational efficiency and real-time response. Challenges & Mitigations: Issues encountered in implementation, debugging, or parameter tuning. Solutions applied to overcome performance bottlenecks. Future Improvements: Ideas for improving UAV swarm coordination. Possible hybrid approaches that combine centralised and decentralised control. Conclusion (10%) Summary of Findings: Key takeaways from experiments. Final Performance Insights: Which method worked best? Under what conditions? Real-World Applications: Discuss how these methods can be used in disaster response, logistics, or surveillance. Contribution Statement (5%) Summary of your individual contributions to the report and video submission. **Video Submission (40% of total grade) ** Length: 5mins Format: MP4, max 500mb Required Elements: Formation Flight Demonstration \u2013 Show formation transitions in the simulator. Obstacle Traversal \u2013 Showcase Window and Forest Traversal. Side-by-Side Comparison \u2013 Compare Centralised vs. Decentralised approaches. Performance Metrics Overlay \u2013 Display success rates and efficiency stats. Brief Narration/Annotations \u2013 Explain the observed UAV behaviours. Conclusion/Summary of key results. Code Submission Zip your packages to upload","title":"Assessment Breakdown and Structure (Deadline 23rd April 16.00)"}]}